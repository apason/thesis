%% History:
%% December 2020 Veli Mäkinen removed obsolete options related to 40 cr theses
%% May 2019 Tomi Männistö, Antti-Pekka Tuovinen proofreading; 30 vs. 40 cr theses, etc.
%% May 2019 Tomi Männistö changed from babelbib to bibtex; Abstract page (and other pages as well) reformatting.
%% January–May 2019 several issues fixed by Niko Mäkitalo; long fields in abstract
%% March 2018 template file extended by Lea Kutvonen to exploit HYthesisML.cls.
%% Feb2018 This template file for the use of HYgraduML.cls was  modified by Veli Mäkinen from HY_fysiikka_LuKtemplate.tex
%% authored by Roope Halonen ja Tomi Vainio in 2017.
%% Some text is also inherited from engl_malli.tex versions by Kutvonen, Erkiö, Mäkelä, Verkamo, Kurhila, and
%% Nykänen, to accompany tktltiki.cls (by Puolakka 2002).


%% Follow comments to support use.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% STEP 1: Choose options for MSc / BSc / seminar layout and your bibliographic style
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%  Language: 
%%      finnish, swedish, or english
%%  Pagination (use twoside by default)  
%%      oneside or twoside,
%%  Study programme / kind of report
%%      csm  = Master's thesis in Computer Science Master's Programme;
%%      tkt = Bachelor's thesis in Computer Science Bachelor's Programme;
%%      seminar = seminar report
%%  For Master's thesis choose your line or track:
%%      (30 cr thesis, 2020 onwards, Master's Programme in Computer Science = csm)
%%      software-track-2020 = Software study track
%%      algorithms-track-2020 = Algorithms study track
%%      networking-track-2020 = Networking study track
%%
%%      (30 cr thesis, Master's Programme in Computer Science = csm)
%%      sw-track-2018 = Software Systems study track
%%      alko-track-2018 = Algorithms study track
%%      nodes-track-2018 = Networking and Services study track
%%
%%      (30 cr thesis, Master's Programme in Computer Science = csm)
%%      sw-line-2017 =  Software systems subprogramme
%%      alko-line-2017 = Algorithms, Data Analytics and Machine Learning subprogramme
%%      bio-line-2017 = Algorithmic Bioinformatics subprogramme
%%      nodes-line-2017 = Networking and Services subprogramme
%%

\documentclass[english,twoside,censored,csm,algorithms-track-2020]{HYthesisML}


% In theses, open new chapters only at right page.
% For other types of documents, may ask "openany" in document.
\PassOptionsToClass{openright,twoside,a4paper}{report}
%\PassOptionsToClass{openany,twoside,a4paper}{report}

\usepackage{csquotes}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% REFERENCES
%% Some notes on bibliography usage and options:
%% natbib -> you can use, e.g., \citep{} or \parencite{} for (Einstein, 1905); with APA \cite -> Einstein, 1905 without ()
%% maxcitenames=2 -> only 2 author names in text citations, if more -> et al. is used
%% maxbibnames=99 as no great need to suppress the biliography list in a thesis
%% for more information see biblatex package documentation, e.g., from https://ctan.org/pkg/biblatex 

%% Reference style: select one 
%% for APA = Harvard style = authoryear -> (Einstein, 1905) use:
\usepackage[style=authoryear,bibstyle=authoryear,backend=biber,natbib=true,maxnames=99,maxcitenames=2,giveninits=true,uniquename=init]{biblatex}
%% for numeric = Vancouver style -> [1] use:
%\usepackage[style=numeric,bibstyle=numeric,backend=biber,natbib=true,maxbibnames=99,giveninits=true,uniquename=init]{biblatex}
%% for alpahbetic -> [Ein05] use:
%\usepackage[style=alphabetic,bibstyle=alphabetic,backend=biber,natbib=true,maxbibnames=99,giveninits=true,uniquename=init]{biblatex}
%

\addbibresource{bibliography.bib}
% in case you want the final delimiter between authors & -> (Einstein & Zweistein, 1905) 
% \renewcommand{\finalnamedelim}{ \& }
% List the authors in the Bibilipgraphy as Lastname F, Familyname G,
\DeclareNameAlias{sortname}{family-given}
% remove the punctuation between author names in Bibliography 
%\renewcommand{\revsdnamepunct}{ }


%% Block of definitions for fonts and packages for picture management.
%% In some systems, the figure packages may not be happy together.
%% Choose the ones you need.

%\usepackage[utf8]{inputenc} % For UTF8 support, in some systems. Use UTF8 when saving your file.

\usepackage{enumerate}
\usepackage{lmodern}         % Font package, again in some systems.
\usepackage{textcomp}        % Package for special symbols
\usepackage[pdftex]{color, graphicx} % For pdf output and jpg/png graphics
\usepackage{epsfig}
\usepackage{subfigure}
\usepackage[pdftex, plainpages=false]{hyperref} % For hyperlinks and pdf metadata
\usepackage{fancyhdr}        % For nicer page headers
\usepackage{tikz}            % For making vector graphics (hard to learn but powerful)
\usepackage{wrapfig}        % For nice text-wrapping figures (use at own discretion)
% Tikz stuff for apas graphs
\usetikzlibrary{positioning,arrows,shapes,automata}
\tikzstyle{gnode} = [circle,minimum size=1cm,fill=blue!20,draw]
\tikzstyle{line} = [line width=0.5,draw]
\tikzstyle{shaded} = [color=blue!30,line width=2mm,draw]
\tikzstyle{shadedf} = [color=blue!25,line width=1.7mm,draw]
\tikzstyle{tnode} = [minimum size=0.67cm]
\tikzstyle{enode} = [minimum size=0.01cm]
\tikzstyle{dotted} = [dashed, line width=0.4, draw]

\usepackage{amsmath, amssymb, amsthm, amsfonts, hyperref} % For better math

\usepackage{algpseudocode} % for pseudocode: provides algorithmic environment
\usepackage{algorithm}     % for pseudocode: enables captioning

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[chapter]
\theoremstyle{definition}
\newtheorem{definition}[equation]{Definition}
\makeatletter
\let\c@equation\c@figure
\makeatother
\newtheorem{lemma}[theorem]{Lemma}


\usepackage[pdftex]{graphicx}
\usepackage{subfigure}
\usepackage{float}


\singlespacing               %line spacing options; normally use single

\fussy
%\sloppy                      % sloppy and fussy commands can be used to avoid overlong text lines
% if you want to see which lines are too long or have too little stuff, comment out the following lines
% \overfullrule=1mm
% to see more info in the detailed log about under/overfull boxes...
% \showboxbreadth=50 
% \showboxdepth=50



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% STEP 2:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Set up personal information for the title page and the abstract form.
%% Replace parameters with your information.
\title{Implementation and benchmarking of Ukkonen 1990 -algorithm}

% TM: Contributors to template editors now listed in the beginning of the file in comments
\author{Arttu Kilpinen}
\date{\today}



% Set supervisors and examiners, use the titles according to the thesis language
% Prof. 
% Dr. or in Finnish toht. or tri or FT, TkT, Ph.D. or in Swedish... 
\supervisors{Assoc Prof.~Simon Puglisi}
\examiners{Prof.~Dunno yet}


\keywords{Implementation, Shortest Common Superstring}
\additionalinformation{\translate{\track}}

%% For seminar reports:
%%\additionalinformation{Name of the seminar}

%% Replace classification terms with the ones that match your work. ACM
%% ACM Digital library provides a taxonomy and a tool for classification
%% in computer science. Use 1-3 paths, and use right arrows between the
%% about three levels in the path; each path requires a new line.

\classification{\protect{\ \\
\  Theory of Computation $\rightarrow$ Design and Analysis of Algorithms  $\rightarrow$ Data Structures Design and Analysis $\rightarrow$ Data Compression  \\
\  Theory of Computation $\rightarrow$ Design and Analysis of Algorithms  $\rightarrow$ Data Structures Design and Analysis $\rightarrow$ Pattern Matching \\
\  Theory of Computation $\rightarrow$ Design and Analysis of Algorithms  $\rightarrow$ Data Structures Design and Analysis $\rightarrow$ Sorting and Searching
}}

%% if you want to quote someone special. You can comment this line out and there will be nothing on the document.
%\quoting{Bachelor's degrees make pretty good placemats if you get them laminated.}{Jeph Jacques}


%% OPTIONAL STEP: Set up properties and metadata for the pdf file that pdfLaTeX makes.
%% Your name, work title, and keywords are recommended.
\hypersetup{
    unicode=true,           % to show non-Latin characters in Acrobat’s bookmarks
    pdftoolbar=true,        % show Acrobat’s toolbar?
    pdfmenubar=true,        % show Acrobat’s menu?
    pdffitwindow=false,     % window fit to page when opened
    pdfstartview={FitH},    % fits the width of the page to the window
    pdftitle={},            % title
    pdfauthor={},           % author
    pdfsubject={},          % subject of the document
    pdfcreator={},          % creator of the document
    pdfproducer={pdfLaTeX}, % producer of the document
    pdfkeywords={something} {something else}, % list of keywords for
    pdfnewwindow=true,      % links in new window
    colorlinks=true,        % false: boxed links; true: colored links
    linkcolor=black,        % color of internal links
    citecolor=black,        % color of links to bibliography
    filecolor=magenta,      % color of file links
    urlcolor=cyan           % color of external links
}

%%-----------------------------------------------------------------------------------

\begin{document}

% Generate title page.
\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% STEP 3:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Write your abstract to be positioned here.
%% You can make several abstract pages (if you want it in different languages),
%% but you should also then redefine some of the above parameters in the proper
%% language as well, in between the abstract definitions.

\begin{abstract}

  Abstract here. Last thing to write

\end{abstract}

% Place ToC
\newpage
\mytableofcontents
\mainmatter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% STEP 4: Write the thesis.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Your actual text starts here. You shouldn't mess with the code above the line except
%% to change the parameters. Removing the abstract and ToC commands will mess up stuff.
%%
%% You may wish to include material to avoid browsing the definitions
%% above. Command \include{file} includes the file of name file.tex.
%% As a side effect, subsequent inclusions may force a page break.

% BSc instructions
%\include{bsc_finnish_contents}
%\include{bsc_english_contents}
% MSc instructions
%\include{msc_finnish_contents}
%\include{msc_english_contents}


Puglisin kommentteja:
- Experiments section saattaa paisua aika paljon. Voisi mahdollisesti jakaa useaan kappaleeseen. Esim implementation kohtaa voisi laittaa myös ukkonen kappaleeseen.

Intro ja Conclusions luonnollisella kielellä. Muualla teknistä kamaa.

%% ei kappalejakoa introon. sectionit pois.
\chapter{Introduction}~\label{chp-intro}
  vitusti viitteitä ja pelkkää LUONNOLLISTA kieltä. Ei esim määritelmiä!

  \section{Motivation}

  - scs yleisesti datan pakkaamiseen
  - molekyylibiologia sekvensointi vaikeaa pitkälle pätkälle
    - molekyylin pilkkominen random paloiksi, sekvensointi ja superstringin ottaminen.
      saadaan approksimaatio alkuperäisestä pitkästä sekvenssistä
    - toimii käutännössä hyvin \citep{Peltola83}
  

  introssa motivaatiossa rlz mainittu mutta myöhemmin teknisesti
  
  alanko or someone mentionet thet it would be interesting to see an implementation

  \section{Related Work}

  O(nm) based algorithm based on KnuthMorrisPratt \citep{Knuth77} \citep{Tarhio88}
  perustuu kaikkien overlap parien laskemiseen kmp:llä.

  hae jostain tietokannasta approx scs algoja !

  related workista oma kappale jos muita vertailuja kuin alanko ja norri. muuten ehkä introon.
  
  \section{Structure of the Thesis}

%%
\chapter {Shortest Common Superstring (preli)}


preliminaries including syntax

what is an approximation algorithm

Maininta aakkostosta?

Tätä voi ehkä jakaa sectioneihin?

The algorithm described in this thesis \\[1in]



In this chapter we define the concepts used in the thesis. These include the concepts related to strings as well as the concept of approximation algorithm
and state automata. The definiton of linear time complexity is also recalled. This chapter contains the explanations in natural languages following a
a corresponding formal definitions. First we define the concepts related to strings followed by ...  \\[1in]



Regardless of the alphabet a string is simply a finite sequence of symbols. The length of the string equals the number of symbols it contains. 

\begin{definition}[\textsc{String}]~\label{def-string}
  
  For an alphabet $\Sigma$ a string $S$ is a finite sequence of symbols  ${s_1\cdots s_n}$ where each symbol $s_i \in \Sigma$.
  The length of the string $S$ denoted by $|S| = n$ is the number of symbols in the string.
  An empty string denoted by $\epsilon$ contains zero symbols.
  
\end{definition}

If a string $S$ contains another string $S'$ we say that the string $S'$ is a substring of a string $S$ and string $S$ is a superstring of $S'$.
By containing a string we mean that there is a sequence of symbols in $S$ that spells out $S'$

\begin{definition}[\textsc{Substring and Superstring}]~\label{def-sub-super}
  Let there be two strings $S$ and $S'$.
  $S'$ is a substring of $S$ if and only if $S$ can be written as $RS'T$ where $R$ and $T$ are strings. $R$ and $T$ may be empty.
  If $R$ and $T$ are both empty then $S = S'$ which is a substring of itself.
  An empty string is trivially a substring of every other string.\\

  If $S'$ if a substring of $S$ then $S$ is a superstring of $S'$
\end{definition}

If a string contains (I.e. is a superstring) a set of strings, it is said to be a common superstring of the set.

\begin{definition}[\textsc{Common superstring}]~\label{def-cs}

  Let $R = \{S1,...,Sn\}$ be a set of strings and $T$ be a string.
  $T$ is a common superstring of $R$ if and only if $T$ is a superstring for each $Si$ such that $i\in 1,...,n$.
  
\end{definition}

Trivially a common superstring of a set of strings can be constructed by catenating all of the strings of the set together.
This kind of string has a length that equals the lengths of all the substrings added together.

In many cases there exists one or more common superstrings that are shorter than the common superstring produced by this naive methot.

(if there is no overlaps then the naive method yelds the csc ... ref ??)

If a common superstring is a shortest possible it is said to be a shortest common superstring. Finding a shortest common superstring is
an \textsc{NP}-hard problem. 

\begin{definition}[\textsc{Shortest Common Superstring}]~\label{def-scs}

  Let string $T$ be a common superstring of a set of strings $K$.
  $T$ is a shortest common superstring of $K$ if and only if there does not exist a common superstring of $K$ which is shorter than $T$.
  
\end{definition}

% Tarhio88
Since the decision version of this problem is NP-complete... \citep{Garey79}

-Lemma: Jos x on y:n substring s.t. y kuuluu R niin scs(R) == scs(Rux)
todistus ???

definition: reduced set of strings = joukko stringejä joissa mikään string ei ole toisen substring.

definition: overlap note this is not symmetric!
maximal overlap.


%%
\chapter{Ukkonen's Algorithm}

In this chapter we describe the Ukkonen's linear time approximation algorithm for
\textsc{Shortest Common Superstring} problem in detail. First we discuss about the
Aho-Corasic (AC for short) machine as it is used in the algorithm for finding the overlaps
between the set of input keywords. This chapter contains four pseudo code blocks which
essentially forms the algorithm as whole. The algorithms represented here are slightly changed
and contains few corrections from the form they were originally presented. Each such
correction is mentioned and ...

\listofalgorithms

\section{The Greedy Heuristic}

%% Introtaan overlap graph
%% demotaan että tästä selvästi saadaan superstringi
%% nyt pitää vielä näyttää että tällä tavalla (overlap graphista saatu) lyhyin superstringi on se the scs!
%%   tämän todistamiseksi tarvitaan
%%   - näytetään että decomposition on sama
%%   - parittaiset decompositionit on maksimaalisia
  
%% Note to Simon: The purpose of this chapter is to introduce the greedy heuristic using strings. Then
%% we show that the analogy between the SCS and longest Hampath problem and define the heuristics using hampath. The point of proving the problems are equivalent is that Tarhio and Ukkonen used it to prove the
%% correctness and the bound (approx scs >= 1/2 scs). 
%% I think this whole section is poorly written and it is hard to follow. Maybe there is something missing
%% and it is not even correct. Any ideas how to improve the reaability and emphasise the important things?

Let there be a set of strings $R=\{s_1,...,s_m\}$. A naive method for constructing a common superstring
is to concatenate all the strings in the set. For the set $R$ the resulting superstring
is $s_1s_2\cdots s_m$.
The following greedy heuristic can be used to the construction of the approximation of the shortest
common superstring.

\begin{enumerate}
\item Examine the set and remove the two strings $s_i$ and $s_j$ that have the longest overlap among all pairs of strings $(s_i, s_j)$ s.t. $i\neq j$. Note that the longest overlap may be an empty string. If there are multiple pairs of strings with the longest overlap, the decision can be made arbitrarily.
\item Merge the strings together such that they are maximally overlapped to form a shortest common superstring of the set $\{s_i, s_j\}$.
  \item Add the new string back to the set $R$ and repeat until there is only one string left.
\end{enumerate}

It turns out that the \textsc{Shortest Common Superstring} problem is analogyous with a special case of
the \textsc{Longest Hamiltonian Path} problem, that is, when the set of strings is converted into
a weighted complete digraph as follows. The graph $G_R=(V_R,E_R)$ is constructed from the set of strings $R$.
Let there be a vertex $v_i\in V_R$ for each string $s_i\in R$. Moreover, we define the start node and
the end node $\{v_{start},v_{end}\}\in V_R$. Each vertex $v_i$ have a directed weighted edge to each
vertex
$v_j$ such that $i\neq j$. The start node has an edge to each $v_i$ and each $v_i$ has an edge to the
end node. The weight $(v_{start}, v_i) = 0$ and the weight $(v_i, v_{end}) = 0$ for all $i=1,...,m$.
The weight of every other edge $(v_i, v_j)$ is the length of the maximum overlap between $s_i$ and $s_j$.

%-intro hamiltonian pathiin!
%-special case of shortest hampath in weighted directed graph.
%- graafin nodet keywordeja SEKÄ alku ja loppunode
%- graafin kaarten painot maksimaalisia overlappeja

Since the subgraph consisting of vertices $\{v_i\}, i=1...m$ is a complete graph, there certainly exists a
Hamiltonian path(s). Taken by any
of the Hamiltonian paths $H$ from the graph $G_R$, we can construct a common superstring of $R$ by
overlapping the strings representing the nodes in the same order they appear on the path in the
following way.

Lets take the strings in the same order as their corresponding nodes appear in the path $H$. Put the
strings above each other such that they are maximally overlapped. The projection $p(H)$ which describes
the strings that are maximally overlapped (i.e. the overlaps are not duplicated) clearly results as a
superstring of $R$. Depending on the Hamiltonian path $H$ the overlaps in the projetion $p(H)$ have
different lengths hence the projection changes depending on the path $H$. 

\begin{figure}
\centering
\begin{tikzpicture}
  
  \node[enode] (H1) at (0,0) {};
  \node[enode] (H2) at (14,0) {};
  \path [|-|] (H1) edge node[below] {p(H)} (H2);

  \node[enode, above = 0.5cm of H1] (T1a) {};
  \node[enode, above right = 0.5cm and 3.3cm of H1] (T1b) {};
  \path[|-|] (T1a) edge (T1b);
  \node[enode, right = 0cm of T1b] (T1txt) {$t_1$};

  \node[enode, above right = 0.5cm and 1.2cm of T1a] (T2a) {};
  \node[enode, right = 2.6cm of T2a] (T2b) {};
  \path[|-|] (T2a) edge (T2b);
  \node[enode, right = 0cm of T2b] (T2txt) {$t_2$};
  
  \node[enode, above right = 0.5cm and 2cm of T2a] (T3a) {};
  \node[enode, right = 2.6cm of T3a] (T3b) {};
  \path[|-|] (T3a) edge (T3b);
  \node[enode, right = 0cm of T3b] (T3txt) {$t_3$};

  \node[tnode, rotate = 40, above = 0.2cm of T3b] (dots1) {...};

  \node[enode, above right = 1cm and 2.2cm of T3a] (Tia) {};
  \node[enode, right = 2.6cm of Tia] (Tib) {};
  \path[|-|] (Tia) edge (Tib);
  \node[enode, right = 0cm of Tib] (Titxt) {$t_i$};

  \node[enode, above right = 0.5cm and 1.2cm of Tia] (Ti1a) {};
  \node[enode, right = 2.6cm of Ti1a] (Ti1b) {};
  \path[|-|] (Ti1a) edge (Ti1b);
  \node[enode, right = 0cm of Ti1b] (Titxt) {$t_{i+1}$};

  \node[enode, above = 0.1cm of Tia] (zc) {};
  \node[enode, below = 0.1cm of Ti1a] (za) {};
  \node[enode, above = 0.1cm of Tib] (zb) {};

  \path[|-|, shorten >= -0.3cm] (zc) edge node[right = -0.1cm,fill=white,align=center] {$u_i$} (za);
  \path[|-|] (za) edge node[fill=white,align=center] {$v_i$} (zb);

  \node[tnode, rotate = 35, above = 0.2cm of Ti1b] (dots2) {...};

  \node[enode, above right = 1cm and 2.2cm of Ti1a] (Tma) {};
  \node[enode, right = 3.455cm of Tma] (Tmb) {};
  \path[|-|] (Tma) edge (Tmb);
  \node[enode, right = 0cm of Tmb] (Titxt) {$t_m$};  
  
\end{tikzpicture}
\caption{The projection $p(H)$ of the set of strings $\{s_1,...,s_m\}$} \label{fig-projection}
\end{figure}

% INTORO, VÄITE, PERUSTELU, KONKLUSIO

% PAPERISSA TÄÄ KOKO PASKA ON REDUCED. eli käydään ensin reduced läpi ja tehdään tämä sille.. ?

The figure \ref{fig-projection} shows an example of the projection constructed from a
Hamiltonian path $H$ in an overlap graph. The corresponding set of keywords contains of
strings $s_1$ to $s_m$. The strings $t_1$ to $t_m$ are the same strings in the order they appear on $H$.
We can represent the string $t_i, i=1...m-1$ as a (prefix,suffix) pair
$(u_i,v_i)$ where $v_i$ is the suffix of $t_i$ that is also a prefix of $t_{i+1}$ such that the
overlap between $t_i$ and $t_{i+1}$ is maximal. $u_i$ is the prefix of $t_i$ whose length is $|t_i|-|v_i|$
i.e. $u_i$ is the part of $t_i$ that can not be overlapped with $t_{i+1}$. Using this partitioning we can
write the projection $p(H)$ as $u_1u_2\cdots u_{m-1}t_m$.

Since the maximal overlaps $v_i,i\in1,...,m-1$ appears in the projection only once, the length of the
projection is $(|s_1|+...+|s_m|) - (|v_1|+...+|v_{m-1}|)$. The length of the maximal overlaps
are equivalent
with the weights of the overlap graph $G_R$ hence $v_1+...+v_{m-1} = |H|$. As the size of $|H|$ grows, the size
of the projection gets shorter.

%That means the shortest common superstring of $R$ corresponds with the longest Hamiltonian path in $G_R$.
% ^ This must be proven!

With this analogy the size of $H$ is the compression of the common superstring
and the length of $p(H)$ is the length of the common superstring.

We now know that the Hamiltonian path in the overlap graph can be used to form a common super string
of the set of keywords. Next we conclude that also the shortest common superstring of $R$ has a
corresponding Hamiltonian path. To prove this we introduce the following lemmas.


\begin{lemma}[]~\label{lem-composition}
The compositions of $p(H)$ and the shortest common superstring $z$ are equivalent.
\end{lemma}
\begin{proof}
  The lemma can bee proven by construction. Let there be a set of strings $R=\{r_1,r_2,...,r_m\}$ and
  the shortest common superstring $z$ for $R$. The indices of the strings $r_i$ to $r_m$ are in the same
  order they appear in $z$. Let $y_i$ be the longest suffix of $r_i$ such that it is also a prefix
  of $r_{i+1}$. I.e there is an overlap $y_i$ between $r_i$ and $R_{i+1}$. Additionally, let us
  define $x_i$ to be the prefix of $r_i$ that does not overlap with $y_i$. By this construction we
  see that the composition of any SCS for any set of strings is equivalent with the composition
  of the superstring acquired via the projection from the Hamiltonian path.

  Hence, there exists a
  Hamiltonian path and the corresponding projection for any SCS.
\end{proof}

By lemma \ref{lem-composition} there exists a decomposition of the SCS $z$ that corresponds a
Hamiltonian path in an overlap graph. This proves the following theorem.


  %% niin oikea scs on tämän verkon lyhin hampathi. ahne heuristiikka kuitenkin antaa usein jonkin
  %% muun ratkaisun.
  %% koska projektion p(H) sekä common superstringin konstruointi on analogisesti ekvivalenttia niin lyhin
  %% projektio tarkoittaa lyhintä common superstringiä.


\begin{theorem} [] ~\label{lem-similarity-between-SCS-Hamp}

  The projection $p(H)$ for the longest Hamiltonian path in the overlap graph is equivalent with
  the shortest common superstring for $R$.

\end{theorem}

The following defines a conceptually equivalent greedy heuristic for the SCS problem in the terms of
a Hamiltonian path in the overlap graph.

To  construct a Hamiltonian path select an edge $e$ from the set $E_R$ such that
\begin{enumerate}
\item $e$ has the largest weight. If there are multiple such edges, the decision can be made arbitrarily.
\item $e$ is not selected before
\item With already selected edges $e$ can be used to form a Hamiltonian cycle.
\end{enumerate}
The constraint 3 indicates the following things
\begin{enumerate}[i]
\item The selection does not form a cycle.
\item The earlier selected edges can not have the same start node with $e$.
\item The earlier selected edges can not have the same end node with $e$.
\end{enumerate}

%  ongelma ahneessa heuristiikassa on että voi tehdä hyvän valinnan joka pakottaa myöhemmin huonoihin valintoihin.

%  CONCEPTUALLY the ukkonen algo does just that so the proofs apply.

%ennen tätä maininta approksimaatioalgosta. koska np niin käytetään approksimaatiota ja tätä heuristiikkaa

%-määrittele kompressio

%-scsn kompressio on vähintään puolet optimaalisesta \citep{Tarhio88}

\begin{theorem}[]~\label{theorem-heuristic-bound}
  Let $H'$ be the approximate longest Hamiltonian path in the overlap graph created using
  the greedy heuristic and let $H$ be the the actual longest Hamiltonian path. Now,
  
  $(|H'|)\leq \frac{1}{2}(|H|)$
  
\end{theorem}
\begin{proof}
  \citep{Tarhio88} proved the theorem.
\end{proof}

In other words the compression achieved by the greedy heuristic is at least half of the optimal
compression. 

%  approx factor (ei välttämättä epsilon mutta jotkut boundit on olemassa ainakin kompressiolle)



\section{AC-Machine}

%  - mitä tää paska oli alunperin: Laajennettu kmp
%  - äärellinen automaatti poikkeuksin (failure)
%  - pattern matching machine

% Ei yhden rivin kappaleita!  
  
  Aho-Corasic machine is a finite state machine like data structure that was developed to be used
  in a pattern matching program to find occurrences of the given keywords in a text string. \citep{Aho75}
  The input of such pattern matching program contains a finite set of strings called keywords and an
  arbitrary string called a text string. The output of the program is a list of locations of every
  occurrence of every keyword in a text. The AC-machine data structure and the associated pattern
  matching program was described in \citep{Aho75}.

  todo: similarity with kmp ?

  % Tulee fiilis etten tiedä mistä puhun. Kerro vähän enemmän tähän kappaleeseen  
  As the string matching part of \citep{Aho75} is not used in \citep{Ukkonen90} nor it is relevant in
  this thesis, we only discuss the AC-machine as a data structure and omit the pattern matching
  functionality of the original author.

  Formally AC-machine is defined\footnote{\citep{Aho75} also defines an output function.
  This is needed only when the AC-machine is used in a pattern matching program.} by the goto function
  $g : \mathbb{N} \times \Sigma \rightarrow \mathbb{N}, g(s_s,c) = s_d$ and the failure function
  $f : \mathbb{N} \rightarrow \mathbb{N}, f(s_s) = s_d$


  When the AC-machine is used in a text search program,
  the goto function describes the state transition from start state $s_s$ to the destination state
  $s_d$ with the current input symbol $c$ of the state automaton. If there is no such goto transition
  the failure function $f$ is queried and the failure transition from the start state $s_s$ to
  the destination state $s_d$ occurs. The alphabet $\Sigma$ contains every symbol in the set of
  keywords encoded by the AC-machine.

  To construct the AC-machine for the set of keywords the \textsc{FSA} like goto function is first
  created. The goto function is then used to construct the failure function which completes the
  construction of the AC-machine. The goto function is similar with a child function of a trie.
  I.e it describes a rooted tree that stores a set of keywords such that edges are labeled with the
  symbols of the alphabet and the edge label is different between each sibling nodes. We say that a node
  spells out a string that is formed by concatenating the edge labels from root to the node.
  The failure function describes chain of state transitions in a case when the goto function is not defined
  for the current input symbol. The failure function is queried as long as the goto function is not defined.
  In the AC-machine the goto function is defined for every symbol in the alphabet. Therefore the
  failure function does not need to be defined for the root node. Other nodes, whether the
  goto function is fully defined or not always have a defined failure function.
  The algorithms \ref{ac-goto} and \ref{ac-fail} are used as a complete definitons of the goto and
  failure functions. 

  The figure \ref{fig-ac} describes an example of the AC-machine constructed from the set of
  keywords $K=\{$"baa", "baba" , "abab" , "aab"$\}$. The solid lines describes the goto function
  and the dashed lines describes the failure function. For example $g(8, a) = $b and $f(4)=11$.

  Can produce "aababaa" (scs with compression 14-7=7) or "baababab" with compression 14-8 = 6.


  \begin{figure}[h]
  \centering
  \begin{tikzpicture}
%   [scale=.8]

    \node[gnode] (1) at (0,0) {$1$};
    
    \node[gnode, above right = 1cm and 2.277cm of 1] (2) {$2$};
    \node[gnode, right = 2cm of 2] (3) {$3$};
    \node[gnode, above right = 1cm and 2.277cm of 3] (4) {$4$};
    
    \node[gnode, right = 2cm of 3] (5) {$5$};
    \node[gnode, right = 2cm of 5] (6) {$6$};

    \node[gnode, right =  2cm of 1] (7) {$7$};
    \node[gnode, right =  2cm of 7] (8) {$8$};
    \node[gnode, right =  2cm of 8] (9) {$9$};
    \node[gnode, right =  2cm of 9] (10) {$10$};
    
    \node[gnode, below right = 1cm and 2.277cm of 7] (11) {$11$};
    \node[gnode, right = 2cm of 11] (12) {$12$};

    \node[tnode, above left = 2.725cm and 2.277cm of 1] (v1) {};
    \node[tnode, below left = 1cm and 2.277 of 1] (v2) {};
    
    \path[->]
    (1) edge[loop,in=110,out=190,distance=3cm] node[below right,align=center] {$\neg \{a,b\}$} (1)
    
    (1) edge[bend left=15] node[above] {b} (2)
    (2) edge node[above] {a} (3)
    (3) edge node[above] {a} (4)

    (3) edge node[above] {b} (5)
    (5) edge node[above] {a} (6)

    (1) edge[bend right=15] node[above] {a} (7)
    (7) edge node[above] {b} (8)
    (8) edge node[above] {a} (9)
    (9) edge node[above] {b} (10)

    (7) edge[bend right=15] node[above] {a} (11)
    (11) edge node[above] {b} (12);

    \path [->,dotted]

    (2) edge[bend left=15] (1)
    (7) edge [bend right=15] (1)
    
    (3) edge (7)
    (8) edge (2)
    (11) edge[bend right=15] (7)
    
    %%(4) edge (v1) edge (v2) edge (11)
    (5) edge (8)
    (9) edge (3)
    (12) edge (8)
    
    (6) edge (9)
    (10) edge (5);

    \path[->,dotted,rounded corners, shorten <= 0.5cm, shorten >= 0.5cm] plot coordinates {(4) (v1) (v2) (11)};
    % repaint these nodes
    \node[gnode, above right = 1cm and 2.277cm of 3] (4) {$4$};
    \node[gnode, below right = 1cm and 2.277cm of 7] (11) {$11$};
    

  \end{tikzpicture}
  \caption{An example of an AC-machine. TODO: bigger arrow heads.} \label{fig-ac}
\end{figure}

  \begin{algorithm}[h!]
    
    \caption{Construction of the goto function} \label{ac-goto}
    % TODO: Kirjoita omin sanoin input, output, method.
    \hspace*{\algorithmicindent} \textbf{Input:} Set of keywords $K = \{s_1,s_2...,s_k\}$.\\
    \hspace*{\algorithmicindent} \textbf{Output:} Goto function $g$\\
    \hspace*{\algorithmicindent} \textbf{Method:} The index of the root node is 1. The procedure \textit{insert($s$)} inserts into the goto graph a path that spells out $s$ or does nothing if there already exists such path.
    
      \begin{algorithmic}[1]

        \Function{calculateGotoFunction}{$K = \{s_1,s_2...,s_k\}$}
          \State $newstate\gets 1$
          \For{$i \gets 1$ until $k$}
            \State $INSERT(s_i)$
          \EndFor
          \For{all $a$ s.t. $g(1,a)$ is not defined }
            \State $g(1,a)\gets 1$
          \EndFor
        \EndFunction

      \item[]

        \Function{insert}{$s = (a_0,a_1,...,a_{m-1})$}
          \State $state\gets 1$
          \State $j\gets 0$
          \While{$g(state, a_j)$ is defined}
            \State $state\gets g(state,a_j)$
            \State $j\gets j+1$
          \EndWhile
          \For{$p\gets j$ until $m-1$}
            \State $newstate\gets newstate+1$
            \State $g(state,a_p)\gets newstate$
            \State $state\gets newstate$
          \EndFor
        \EndFunction
      \end{algorithmic}
      
  \end{algorithm}

  Algorithm 1 creates a goto function. It begins by creating the root node in line 2. The
  meaning of this is to allocate memory for the goto function so the data $g(1, \sigma)$ can be
  saved. The algorithm continues by looping over the set of keywords and inserting them to the
  goto function in lines 3 to 5. At the end of the main function in lines 7 to 8
  the goto function is defined for all $g(1,\sigma)$ that are undefined creating a loop within
  the root node. This ensures that the failure function of the root node is never used.

  The procedure \textsc{INSERT} inserts a single keyword to the goto function. The idea in the lines
  13 to 16 is to skip the prefix of $s$ that already exists in the trie. The variable $j$ initialized
  in the line 12 keeps track of this prefix. After the first symbol not already represented in the
  trie is found the new branch is created and the remaining suffix of $s$ is inserted symbol by symbol
  at lines 17-20.

  Lets simulate this algorithm with a set of keywords $K=\{$"baa", "baba" , "abab" , "aab"$\}$.
  algorithm starts from line 2 where the root node is created. The figure \ref{fig-ac-step1}
  presents this state. Since there are four keywords the the first for loop is executed four times.
  In the first round the \textit{insert} function is called for the first keyword "baa".
  In the \textit{insert} function the first while loop is not executed since there is no yet
  outgoing edges from the root node. The for loop is executed for every symbol and the three
  nodes (2,3,4) are added to the goto function with edge labels 'b', 'a', and 'a'.
  Figure \ref{fig-ac-step2} presents this state of the construction.


  \begin{figure}[h]
  \centering
  \begin{tikzpicture}
%   [scale=.8]

    \node[gnode] (1) at (0,0) {$1$};
    
  \end{tikzpicture}
  \caption{A root node. TODO: bigger arrow heads.} \label{fig-ac-step1}
  \end{figure}

  \begin{figure}[h]
  \centering
  \begin{tikzpicture}
%   [scale=.8]

    \node[gnode] (1) at (0,0) {$1$};
    
    \node[gnode, right = 2cm of 1] (2) {$2$};
    \node[gnode, right = 2cm of 2] (3) {$3$};
    \node[gnode, right = 2cm of 3] (4) {$4$};
    
    \path[->]
    (1) edge node[above] {b} (2)
    (2) edge node[above] {a} (3)
    (3) edge node[above] {a} (4);

  \end{tikzpicture}
  \caption{first key added. TODO: bigger arrow heads.} \label{fig-ac-step2}
\end{figure}

  With the second round of the for loop we add the keyword "baba". In this case the \textit{insert}
  function executes the while loop two times since the common prefix of "baa" and "baba" has a length
  2. At this point the goto graph is branched and the remaining symbols 'b' and 'a' are inserted. The
  figure \ref{fig-ac-step3} corresponds with this state.

  \begin{figure}[h]
  \centering
  \begin{tikzpicture}
%   [scale=.8]

    \node[gnode] (1) at (0,0) {$1$};
    
    \node[gnode, right = 2cm of 1] (2) {$2$};
    \node[gnode, right = 2cm of 2] (3) {$3$};
    \node[gnode, above right = 1cm and 2.277cm of 3] (4) {$4$};
    
    \node[gnode, right = 2cm of 3] (5) {$5$};
    \node[gnode, right = 2cm of 5] (6) {$6$};

    \path[->]
    (1) edge node[above] {b} (2)
    (2) edge node[above] {a} (3)
    (3) edge node[above] {a} (4)

    (3) edge node[above] {b} (5)
    (5) edge node[above] {a} (6);

  \end{tikzpicture}
  \caption{second key added. TODO: bigger arrow heads.} \label{fig-ac-step3}
\end{figure}

  In the same way, the rest of the keywords are added. For keyword "baba" the the insertion
  happens in the same way as with the first keyword, that is the while loop in the \textit{insert}
  function is not executed since there is no goto transition from the root with a symbol 'a'.
  For the last keyword the branching of the goto graph happens again
  similarly with the second keyword. At this point the goto function is represents a trie of the
  input keywords. After the first for loop in the main function is ecexuted 
  the second loop in lines 6 to 8 creates an edge from root to root for every symbol in the
  alphabet such that $g(1,\sigma)$ is not defined. This completes the algorithm \ref{ac-goto}.
  The resulting goto graph is described in the figure \ref{fig-ac-goto}

  \begin{figure}[h]
  \centering
  \begin{tikzpicture}
%   [scale=.8]

    \node[gnode] (1) at (0,0) {$1$};
    
    \node[gnode, above right = 1cm and 2.277cm of 1] (2) {$2$};
    \node[gnode, right = 2cm of 2] (3) {$3$};
    \node[gnode, above right = 1cm and 2.277cm of 3] (4) {$4$};
    
    \node[gnode, right = 2cm of 3] (5) {$5$};
    \node[gnode, right = 2cm of 5] (6) {$6$};

    \node[gnode, right =  2cm of 1] (7) {$7$};
    \node[gnode, right =  2cm of 7] (8) {$8$};
    \node[gnode, right =  2cm of 8] (9) {$9$};
    \node[gnode, right =  2cm of 9] (10) {$10$};
    
    \node[gnode, below right = 1cm and 2.277cm of 7] (11) {$11$};
    \node[gnode, right = 2cm of 11] (12) {$12$};

    \path[->]
    (1) edge[loop,in=110,out=190,distance=3cm] node[below right,align=center] {$\neg \{a,b\}$} (1)
    
    (1) edge node[above] {b} (2)
    (2) edge node[above] {a} (3)
    (3) edge node[above] {a} (4)

    (3) edge node[above] {b} (5)
    (5) edge node[above] {a} (6)

    (1) edge node[above] {a} (7)
    (7) edge node[above] {b} (8)
    (8) edge node[above] {a} (9)
    (9) edge node[above] {b} (10)

    (7) edge node[above] {a} (11)
    (11) edge node[above] {b} (12);

  \end{tikzpicture}
  \caption{The complete goto graph. TODO: bigger arrow heads.} \label{fig-ac-goto}
\end{figure}

  Next we describe the construction of the failure function in more detail. The failure function
  for a state is calculated using the failure function of the parent of that state thus the computation
  starts from the root to the nodes of depth 1 to the nodes of depth 2 and so on. By depth of a node
  we mean the distance (number of edges) from the root to that node. That clearly equals with the
  index of a symbol in the string that the node spells out. E.g. the depth of the node that spells
  out "baa" is 3.

  Since the failure function is not defined for the root node (depth 0), the calculation is started from
  depth 1. We define $f(s)=1$ for all states $s$ with depth = 1. Assuming the failure function is
  calculated for all nodes of depth d-1, then the failure function of a node s is calculated with the
  following method.
  
  \begin{figure}[h]
  \begin{enumerate}
  \item Let $p$ be the parent of the node $s$ such that $g(p,\sigma) = s$.
  \item Set $state = f(p)$.
  \item Set $state = f(state)$ until $g(state,\sigma)$ is defined. Since $g(1,\sigma)$ is defined
    for all symbols that state is always defined.
  \item Set $f(s) = state$
  \end{enumerate}
   \caption{Listing of the failure calculation} \label{failure-calculation}    
  \end{figure}

  Let us continue with the example from the goto function construction. By the definiton $f(1)$ is not
  defined. For the states of depth 1 ($s\in\{2,7\}$) the failure function is defined $f(s)=1$. The
  figure \ref{fig-ac-step5} represents this state.

  \begin{figure}[h]
  \centering
  \begin{tikzpicture}
%   [scale=.8]

    \node[gnode] (1) at (0,0) {$1$};
    
    \node[gnode, above right = 1cm and 2.277cm of 1] (2) {$2$};
    \node[gnode, right = 2cm of 2] (3) {$3$};
    \node[gnode, above right = 1cm and 2.277cm of 3] (4) {$4$};
    
    \node[gnode, right = 2cm of 3] (5) {$5$};
    \node[gnode, right = 2cm of 5] (6) {$6$};

    \node[gnode, right =  2cm of 1] (7) {$7$};
    \node[gnode, right =  2cm of 7] (8) {$8$};
    \node[gnode, right =  2cm of 8] (9) {$9$};
    \node[gnode, right =  2cm of 9] (10) {$10$};
    
    \node[gnode, below right = 1cm and 2.277cm of 7] (11) {$11$};
    \node[gnode, right = 2cm of 11] (12) {$12$};

    \path[->]
    (1) edge[loop,in=110,out=190,distance=3cm] node[below right,align=center] {$\neg \{a,b\}$} (1)
    
    (1) edge[bend left=15] node[above] {b} (2)
    (2) edge node[above] {a} (3)
    (3) edge node[above] {a} (4)

    (3) edge node[above] {b} (5)
    (5) edge node[above] {a} (6)

    (1) edge[bend right=15] node[above] {a} (7)
    (7) edge node[above] {b} (8)
    (8) edge node[above] {a} (9)
    (9) edge node[above] {b} (10)

    (7) edge node[above] {a} (11)
    (11) edge node[above] {b} (12);

    \path [->,dotted]

    (2) edge[bend left=15] (1)
    (7) edge [bend right=15] (1);
    
  \end{tikzpicture}
  \caption{The failure function construction when $f(s)$ is defined for all states $s$ with depth <= 1. TODO: bigger arrow heads.} \label{fig-ac-step5}
\end{figure}

  Next we calculate the failure function for the states 3, 8 and 11 that is the states with depth 2.
  For state 3 we take the parent state 2 and follow the the failure function to the root node. Since
  $g(1,$a$) = 7$ we define $f(3)=7$. For the states 8 and 11 the precedure is similar. $f(8)$ gets
  the value 2 and $f(11)=7$. The procedure is iterated until the failure function is defined for every
  node. The figure \ref{fig-ac} describes the final result. Because the AC-machine only contains these
  two functions, the figure \ref{fig-ac} also presents the fully constructed AC-machine.

    \begin{figure}
  \centering
  \begin{tikzpicture}
%   [scale=.8]

    \node[gnode] (1) at (0,0) {$1$};
    
    \node[gnode, above right = 1cm and 2.277cm of 1] (2) {$2$};
    \node[gnode, right = 2cm of 2] (3) {$3$};
    \node[gnode, above right = 1cm and 2.277cm of 3] (4) {$4$};
    
    \node[gnode, right = 2cm of 3] (5) {$5$};
    \node[gnode, right = 2cm of 5] (6) {$6$};

    \node[gnode, right =  2cm of 1] (7) {$7$};
    \node[gnode, right =  2cm of 7] (8) {$8$};
    \node[gnode, right =  2cm of 8] (9) {$9$};
    \node[gnode, right =  2cm of 9] (10) {$10$};
    
    \node[gnode, below right = 1cm and 2.277cm of 7] (11) {$11$};
    \node[gnode, right = 2cm of 11] (12) {$12$};

    \node[tnode, above left = 2.725cm and 2.277cm of 1] (v1) {};
    \node[tnode, below left = 1cm and 2.277 of 1] (v2) {};
    
    \path[->]
    (1) edge[loop,in=110,out=190,distance=3cm] node[below right,align=center] {$\neg \{a,b\}$} (1)
    
    (1) edge[bend left=15] node[above] {b} (2)
    (2) edge node[above] {a} (3)
    (3) edge node[above] {a} (4)

    (3) edge node[above] {b} (5)
    (5) edge node[above] {a} (6)

    (1) edge[bend right=15] node[above] {a} (7)
    (7) edge node[above] {b} (8)
    (8) edge node[above] {a} (9)
    (9) edge node[above] {b} (10)

    (7) edge[bend right=15] node[above] {a} (11)
    (11) edge node[above] {b} (12);

    \path [->,dotted]

    (2) edge[bend left=15] (1)
    (7) edge [bend right=15] (1)
    
    (3) edge (7)
    (8) edge (2)
    (11) edge[bend right=15] (7)
    
    %%(4) edge (v1) edge (v2) edge (11)
    (5) edge (8)
    (9) edge (3)
    (12) edge (8)
    
    (6) edge (9)
    (10) edge (5);

    \path[->,dotted,rounded corners, shorten <= 0.5cm, shorten >= 0.5cm] plot coordinates {(4) (v1) (v2) (11)};
    % repaint these nodes
    \node[gnode, above right = 1cm and 2.277cm of 3] (4) {$4$};
    \node[gnode, below right = 1cm and 2.277cm of 7] (11) {$11$};
    

  \end{tikzpicture}
  \caption{Fully constructed AC-machine for the keywords $R$. TODO: bigger arrow heads.} \label{fig-ac}
\end{figure}


    In the previous example the value for the failure function was always found in a single iteration
    of the step 3 of the above definition. Next we go through the pseudocode for the failure function
    calculation in algorithm \ref{ac-fail}.


  \begin{algorithm}[h!]

    \caption{Construction of the failure function} \label{ac-fail}
    \hspace*{\algorithmicindent} \textbf{Input:} Goto function $g$ from algorithm \ref{ac-goto}\\
    \hspace*{\algorithmicindent} \textbf{Output:} Failure function $f$

    \begin{algorithmic}[1]
      \Function{calculateFailureFunction}{$g : \mathbb{N} \rightarrow \mathbb{N}$}
        \State $queue\gets \textit{empty}$
        \For {each $a$ s.t. $g(1,a) = s \neq 0$}
          \State $queue\gets queue \cup \{s\}$
          \State $f(s)\gets 1$
        \EndFor
        \While{$queue \neq empty$}\\
          \hspace*{\algorithmicindent}let $r$ be the next state in $queue$
          \State $queue\gets queue \backslash \{r\}$
          \For{each $a$ s.t. $g(r,a) = s$ is defined}
            \State $queue\gets queue \cup \{s\}$
            \State $state\gets f(r)$
            \While{$g(state,a)$ is undefined}
              \State $state\gets f(state)$
            \EndWhile
            \State $f(s)\gets g(state,a)$              
          \EndFor
        \EndWhile
          
      \EndFunction

    \end{algorithmic}
  \end{algorithm}


  Since the failure values must be calculated from lowest depth nodes to the highest depth nodes
  we go through the goto path using a breadth-first search. This is provided by the FIFO that is
  appended in every non-leaf node. I.e. the children of the currently processed node are put into the
  queue and the next node to be processed is popped. The queue is initialized to be empty in the first
  line of the algorithm. The for loop in lines 3 to 6 adds all the children of the root to the queue.
  Because the root node is special as it contains links to itself the additional constraint
  in line 3 is added to prevent the infinite recursion. In the same for loop we define the
  $f(s)=1$ for all nodes with depth 1. The while loop from line 7 to 18 implements the instructions
  in the listing \ref{failure-calculation} Since the AC-machine does not save parent information
  this is done a bit differently, although the result is exactly the same. In line 8 and 9 we pop a
  node from the queue. At this point the failure function for that node is already defined. Then we
  process each child of the node separately in lines 10 to 17 by 1) adding the child to the queue
  in line 11  2) travelling the failure function of the parent as long as we get a node that have goto
  transition with the same symbol that appears between the parent and the currently processed child
  in lines 12 to 15 and  3) set the failure function for the child in line 16. The execution of the
  function halts when the BFS of the  AC-machine is ready and the queue is finally empty.


  For now, as the AC-machine is fully defined we focus on the important property it can be used as
  the lemma \ref{lem-overlap} suggests.


\begin{lemma}[]~\label{lem-overlap}
  Let there be a state $p$ and state $q, p\neq q$ in an AC-machine that are represented by strings
  $s$ and $t$ respectively. The failure value $f(p) = q$ if and only if $t$ is the longest proper suffix
  of $s$ that is also a prefix of some keyword.
\end{lemma}
\begin{proof}
  %% This can be proven by induction. The induction hypothesis trivially holds for the base case when
  %% depth of $p$ is 1. That is when $q$ is the root node represented by the empty string and $|s|=1$.
  %% For the induction step we assume that the hypothesis holds for all states with depth $\leq$ d
  %% $> 1$.
  Trust me bro.
\end{proof}  

The lemma \ref{lem-overlap} is used later on in the description of the SCS algotithm. Next we look
over the time complexities of the algorithms \ref{ac-fail} and \ref{ac-goto}. Lets first investigate
the time complexity of the insert and read operations of the functions $g$ and $f$. The failure
function maps each state (except the root) to some other state. It is most effective and practical
to implement this as an array which stores the value $f(s)=d$ such that the array index $s$ has
a value $d$. Therefore the both inserting and reading to/from the failure function is performed
in constant time. The goto function maps a pair of values to a single value.
In the same way, if we use a two dimensional table which stores
a $|\Sigma|$ sized array for each state in the AC-machine, the read and write operations of $g$ would
perform in a constant time. This of course results in a very high memory usage since the table would be
sparsely used especially with large alphabets and long keywords. The goto function can also
be implemented in many other ways. One solution to the low memory efficient is to save a balanced
binary tree for each state in the AC-machine. With binary trees the insert and read operations are
performed in logarithmic time.

The main function of the algorithm \ref{af-goto} consists of two for loops.
The first loop calls a procedure once for each keyword of the input. The function calls are
executed in a constant time so the time complexity of the first for loop is linear with respect
to the number of input keywords. The second for loop starting in line 6 is used to search all
children of the root node. Since the root node has only $\Sigma$ goto links the loop is executed at most
$\Sigma$ times. Here $\Sigma$ refers to the size of the alphabet. In the worst case every symbol in
every input keywords is different. In that case $|\Sigma| = n$, where $n$ is te sum of the input symbols.
Therefore the second for loop is executed at most $n$ times resulting the time complexity of the main
function to be linear with respect to the size of the input.

Lets next examine the insert function. The first two lines 11 and 12 can clearly be executed in constant
time. The first while loop is used to find the symbol of the string that is not yet represented
in the goto graph. Let $s=vu$ such that $v$ is the part of $s$ that is already represented in the
goto graph. The index variable $j$ is used so it contains the value of the first symbol of $u$ after
the while loop is executed. The for loop continues from $j$ so the lines 18 to 20 are executed
only for $u$. In the other words the \textit{insert} function behaves in a way that every symbol of
the input keyword is processed either in one of the two loops but not both. Since the time complexities
of the loop bodies depend on the implementation of $g$, the the total time complexity of the function
is $O(n)*O(1) = O(n)$ or $O(n)*O(log(n)) = O(n\cdot log(n))$ when the goto function is implemented
in linear or logarithmic time respectively.

The former discussion resultsleads us with the following theorem
\begin{theorem}[]~\label{thm-linear-goto}
The goto function for the AC-machine can be constructed in linear time.
\end{theorem}

Lets examine the construction of the failure function \ref{ac-fail}. In the discussion of theorem
\ref{thm-linear-goto} we reasoned that finding all children of the state can be done in
linear time. In the failure calculation we have the same kind of instruction in the for loop
in line 10. Since this loop is executed for every state of the machine this kind of approach leads
us to $O(n|\Sigma|)$ time complexity for finding every child of every state. Instead of scanning
the goto function for each symbol in the alphabet we can use an auxiliary data structure to
explicitely save the children of each state. One possibility is to use a linked list ($O(1)$ time
insertions) and save the information of the children while creating the goto function. This adds
$O(1)$ time instruction to the for loop in the \textit{insert} function. The body of this for loop
is executed $O(n) times so the total time complexity is not changed. This allows us to get the
children of a state in linear time with respect to the number of the children (and not the number of
symbols in the alphabet). Since every state
has at most one parent the the body of the for loop is executed $O(n)$ times total neverthless it
is nested with another while loop (line 7). Thus the wile loop in line 7 as well as the for loop in
line 10  executed once for every state that is $O(n)$ times. The lines 11 and 12 are clearly constant
time operations and the time complexity of line 16 depends on the goto function implementation and
can also be a constant time operation. Lastly, lets examine the inner while loop on line 13.
The idea is to travel the failure path until a state is found such that the goto function
for that state is defined with the same symbol as the goto transition is defined for state we are
defining the failure function for. According to the definition of the failure function the state
$y=f(x)$ has a depth at most one less than the depth of the state $x$. This means that the while
loop is always executed at most d times where d is the depth of the state \textit{state}. 

\begin{theorem}[]~\label{thm-linear-failure}
The failure function for the AC-machine can be constructed in linear time.
\end{theorem}

  note that this works for reduced or nonreduced set.

\section{The approximation algorithm}

The acrual algorithm that uses the ac machine here

  
  %% HUOM!!! Kirjoitetaan pseude alkuun sellaisena kuin se on alkuperäisissä papereissa.
  %% Omat indeksöinnit voivat poiketa ja tämä korjataan sitten kun pseudo kirjotietaan
  %% Vastaamaan omaa koodia!
  
%Tänne bugikorjaukset sähköpostista joka lähetetty 30.4

  \begin{algorithm}[h]

    %% OMIN SANOIN
    \caption{Ukkonen90 algorithm 1, preprocessing} \label{ukk-pre}
    \hspace*{\algorithmicindent} \textbf{Input:} Set $R=\{x_1,...,x_m\}$ of strings, and the goto function $g$ and the failure function $f$ for $R$.\\
    \hspace*{\algorithmicindent} \textbf{Output:} Depth $d(s)$, list $L(s)$ and link $b(s)$ for each state $s$ of the AC machine; Pointer $B$ to the first state in the $b$-link chain; state $F(i)$ representing string $x_i$ for each $x_i$ with the exception that if $x_i$ is a substring of another string in $R$ then $F(i) = 0$.\\
    \hspace*{\algorithmicindent} \textbf{Notation:} operator $\cdot$ denotes list catenation. An inverse of $F$ is represented by $E$: if $F(i) = s$ then $E(s) = i$; initially $E(s) = 0$ for every state s.\\
    
    \begin{algorithmic}[1]
      \Function{calculateAuxiliaryFunctions}{$g : \mathbb{N} \rightarrow \mathbb{N}$, $f : \mathbb{N} \rightarrow \mathbb{N}$}
        \For {$i = 1,...,m$}
          \hspace*{\algorithmicindent} let $x_i = a_1,...,a_k$
          \State $s\gets 0$
          \For {$j = 1,...,k$}
            \State $s\gets g(s,a_j)$
            \State $L(s)\gets L(s) \cdot \{j\}$
            \If {$j = k$}
              \State $F(i)\gets s$
              \State $E(s)\gets i$
              \If {$s$ is not a leaf of the AC machine}
                \State $F(i)\gets 0)$
              \EndIf
            \EndIf
          \EndFor
        \EndFor
        \State $queue\gets 0$
        \State $d(0)\gets 0$
        \State $B\gets 0$
        \While{$queue\neq empty$}
          \hspace*{\algorithmicindent} let $r$ be the next state in queue
          \State $queue\gets queue \backslash \{r\}$
          \For {each $s$ s.t. $g(r,a) = s$ for some $a$}
            \State $queue\gets queue\cdot s$
            \State $d(s)\gets d(r)+1$
            \State $b(s)\gets B$
            \State $F(E(f(s)))\gets 0$
          \EndFor
        \EndWhile
      \EndFunction
        
    \end{algorithmic}
  \end{algorithm}

  \begin{algorithm}[h]
    %% TODO: omin sanoin input, output.
    \caption{Ukkonen90 Algorithm 2 Construction of H} \label{ukk-h}
    \hspace*{\algorithmicindent} \textbf{Input:} Augmented AC machine for $R$ as constructed by ref\\ %ref?
    \hspace*{\algorithmicindent} \textbf{Output:} A Hamiltonian path $H$ in the overlap graph of reduced $R$. A common superstring for $R$ can then be constructed by forming $p(H)$.\\

    \begin{algorithmic}[1]
      \Function{createPath}{Outputs of the previous functions, Set of keywords $R = \{k_1,...,k_m\}$}
        \For {$j = 1,...,m$}
          \If {$F(j)\neq 0$}
            \State $P(f(F(j)))\gets P(f(F(j)))\cdot {j}$
            \State $FIRST(j)\gets j$
            \State $LAST(j)\gets j$
          \Else
            \State $forbidden(j)\gets true$
          \EndIf
        \EndFor  
        \State $s\gets b(B)$
        \While {$s\neq 0$}
          \If {$P(s)$ is not empty}
            \For {each $j$ in $L(s)$ s.t. $forbidden(j) = false$}
              \State $i\gets$ the first element of $P(s)$
              \If {$FIRST(i) = j)$}
                \If {$P(s)$ has only one element}
                  \hspace*{\algorithmicindent} \textbf{goto} \textit{next}
                \Else
                  \State $i\gets$ the second element of $P(s)$
                \EndIf
              \EndIf  
            \State $H\gets H\cdot \{(x_i,x_j)\}$
            \State $forbidden(j)\gets true$
            \State $FIRST(LAST(j))\gets FIRST(i)$
            \State $LAST(FIRST(i))\gets LAST(j)$
            \hspace*{\algorithmicindent} \textit{next:}
            \EndFor
          \State $P(f(s))\gets P(f(s))\cdot P(s)$
          \EndIf
          \State $s\gets b(s)$
        \EndWhile
      \EndFunction
    \end{algorithmic}
  \end{algorithm}
  


  \section{correctness}


%%
\chapter{Relative Lempel-Ziv}

  Lempel-Ziv dictionary construction.

  subsections?

%%
\chapter{experiments}

  \section{Implementation}

  \section{Benchmark Data}
  HW + instances

  \section{Results}

  \section{Discussion}

%
\chapter{Conclusions}
  

  \begin{enumerate}
  \item \citep{Aho75} describes the Aho-Corasic machine for the first time. It gives the pseudocode to creation and search.
  \item alanko dissertation, no bibtex yet. Discusses some things related to this topic.
  \item \citep{Alanko17} describes approx scs algorithm for compact space.
  \item statistics.pdf desccribes the dataset pizzachili.
  \item \citep{Ukkonen90} is the most important reference in this thesis. Describes the main scs algorithm.
  \item \citep{Tarhio88} Describes the same algorithm as ukkonen 90 but not in linear time.
  \end{enumerate}











%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\cleardoublepage                          %fixes the position of bibliography in bookmarks
\phantomsection
\addcontentsline{toc}{chapter}{\bibname}  % This lines adds the bibliography to the ToC
\printbibliography

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\backmatter

%
% TARVIINKO APPENDICES??
%
%

\begin{appendices}

%\input{instructions_english}
%% \input{instructions_finnish}

%% \appendix{Sample Appendix\label{appendix:model}}
%% usually starts on its own page, with the name and number of the appendix at the top. 
%% The appendices here are just models of the table of contents and the presentation. Each appendix
%% Each appendix is paginated separately.

%% In addition to complementing the main document, each appendix is also its own, independent entity.
%% This means that an appendix cannot be just an image or a piece of programming, but the appendix must explain its contents and meaning.

\end{appendices}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
