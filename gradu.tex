%% History:
%% December 2020 Veli Mäkinen removed obsolete options related to 40 cr theses
%% May 2019 Tomi Männistö, Antti-Pekka Tuovinen proofreading; 30 vs. 40 cr theses, etc.
%% May 2019 Tomi Männistö changed from babelbib to bibtex; Abstract page (and other pages as well) reformatting.
%% January–May 2019 several issues fixed by Niko Mäkitalo; long fields in abstract
%% March 2018 template file extended by Lea Kutvonen to exploit HYthesisML.cls.
%% Feb2018 This template file for the use of HYgraduML.cls was  modified by Veli Mäkinen from HY_fysiikka_LuKtemplate.tex
%% authored by Roope Halonen ja Tomi Vainio in 2017.
%% Some text is also inherited from engl_malli.tex versions by Kutvonen, Erkiö, Mäkelä, Verkamo, Kurhila, and
%% Nykänen, to accompany tktltiki.cls (by Puolakka 2002).


%% Follow comments to support use.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% STEP 1: Choose options for MSc / BSc / seminar layout and your bibliographic style
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%  Language: 
%%      finnish, swedish, or english
%%  Pagination (use twoside by default)  
%%      oneside or twoside,
%%  Study programme / kind of report
%%      csm  = Master's thesis in Computer Science Master's Programme;
%%      tkt = Bachelor's thesis in Computer Science Bachelor's Programme;
%%      seminar = seminar report
%%  For Master's thesis choose your line or track:
%%      (30 cr thesis, 2020 onwards, Master's Programme in Computer Science = csm)
%%      software-track-2020 = Software study track
%%      algorithms-track-2020 = Algorithms study track
%%      networking-track-2020 = Networking study track
%%
%%      (30 cr thesis, Master's Programme in Computer Science = csm)
%%      sw-track-2018 = Software Systems study track
%%      alko-track-2018 = Algorithms study track
%%      nodes-track-2018 = Networking and Services study track
%%
%%      (30 cr thesis, Master's Programme in Computer Science = csm)
%%      sw-line-2017 =  Software systems subprogramme
%%      alko-line-2017 = Algorithms, Data Analytics and Machine Learning subprogramme
%%      bio-line-2017 = Algorithmic Bioinformatics subprogramme
%%      nodes-line-2017 = Networking and Services subprogramme
%%

\documentclass[english,twoside,censored,csm,algorithms-track-2020]{HYthesisML}


% In theses, open new chapters only at right page.
% For other types of documents, may ask "openany" in document.
\PassOptionsToClass{openright,twoside,a4paper}{report}
%\PassOptionsToClass{openany,twoside,a4paper}{report}

\usepackage{csquotes}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% REFERENCES
%% Some notes on bibliography usage and options:
%% natbib -> you can use, e.g., \citep{} or \parencite{} for (Einstein, 1905); with APA \cite -> Einstein, 1905 without ()
%% maxcitenames=2 -> only 2 author names in text citations, if more -> et al. is used
%% maxbibnames=99 as no great need to suppress the biliography list in a thesis
%% for more information see biblatex package documentation, e.g., from https://ctan.org/pkg/biblatex 

%% Reference style: select one 
%% for APA = Harvard style = authoryear -> (Einstein, 1905) use:
\usepackage[style=authoryear,bibstyle=authoryear,backend=biber,natbib=true,maxnames=99,maxcitenames=2,giveninits=true,uniquename=init]{biblatex}
%% for numeric = Vancouver style -> [1] use:
%\usepackage[style=numeric,bibstyle=numeric,backend=biber,natbib=true,maxbibnames=99,giveninits=true,uniquename=init]{biblatex}
%% for alpahbetic -> [Ein05] use:
%\usepackage[style=alphabetic,bibstyle=alphabetic,backend=biber,natbib=true,maxbibnames=99,giveninits=true,uniquename=init]{biblatex}
%

\addbibresource{bibliography.bib}
% in case you want the final delimiter between authors & -> (Einstein & Zweistein, 1905) 
% \renewcommand{\finalnamedelim}{ \& }
% List the authors in the Bibilipgraphy as Lastname F, Familyname G,
\DeclareNameAlias{sortname}{family-given}
% remove the punctuation between author names in Bibliography 
%\renewcommand{\revsdnamepunct}{ }


%% Block of definitions for fonts and packages for picture management.
%% In some systems, the figure packages may not be happy together.
%% Choose the ones you need.

%\usepackage[utf8]{inputenc} % For UTF8 support, in some systems. Use UTF8 when saving your file.

\usepackage{enumerate}
\usepackage{lmodern}         % Font package, again in some systems.
\usepackage{textcomp}        % Package for special symbols
\usepackage[pdftex]{color, graphicx} % For pdf output and jpg/png graphics
\usepackage{epsfig}
\usepackage{subfigure}
\usepackage[pdftex, plainpages=false]{hyperref} % For hyperlinks and pdf metadata
\usepackage{fancyhdr}        % For nicer page headers
\usepackage{tikz}            % For making vector graphics (hard to learn but powerful)
\usepackage{wrapfig}        % For nice text-wrapping figures (use at own discretion)
% Tikz stuff for apas graphs
\usetikzlibrary{positioning,arrows,shapes,automata}
\tikzstyle{gnode} = [circle,minimum size=1cm,fill=blue!20,draw]
\tikzstyle{line} = [line width=0.5,draw]
\tikzstyle{shaded} = [color=blue!30,line width=2mm,draw]
\tikzstyle{shadedf} = [color=blue!25,line width=1.7mm,draw]
\tikzstyle{tnode} = [minimum size=0.67cm]
\tikzstyle{enode} = [minimum size=0.01cm]
\tikzstyle{dotted} = [dashed, line width=0.4, draw]

\usepackage{amsmath, amssymb, amsthm, amsfonts, hyperref} % For better math

\usepackage{algpseudocode} % for pseudocode: provides algorithmic environment
\usepackage{algorithm}     % for pseudocode: enables captioning


\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[chapter]
\theoremstyle{definition}
\newtheorem{definition}[equation]{Definition}
\makeatletter
\let\c@equation\c@figure
\makeatother
\newtheorem{lemma}[theorem]{Lemma}


\usepackage[pdftex]{graphicx}
\usepackage{subfigure}
\usepackage{float}


\singlespacing               %line spacing options; normally use single

\fussy
%\sloppy                      % sloppy and fussy commands can be used to avoid overlong text lines
% if you want to see which lines are too long or have too little stuff, comment out the following lines
% \overfullrule=1mm
% to see more info in the detailed log about under/overfull boxes...
% \showboxbreadth=50 
% \showboxdepth=50



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% STEP 2:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Set up personal information for the title page and the abstract form.
%% Replace parameters with your information.
\title{Implementation and benchmarking of Ukkonen 1990 -algorithm}

% TM: Contributors to template editors now listed in the beginning of the file in comments
\author{Arttu Kilpinen}
\date{\today}



% Set supervisors and examiners, use the titles according to the thesis language
% Prof. 
% Dr. or in Finnish toht. or tri or FT, TkT, Ph.D. or in Swedish... 
\supervisors{Assoc Prof.~Simon Puglisi}
\examiners{Prof.~Dunno yet}


\keywords{Implementation, Shortest Common Superstring}
\additionalinformation{\translate{\track}}

%% For seminar reports:
%%\additionalinformation{Name of the seminar}

%% Replace classification terms with the ones that match your work. ACM
%% ACM Digital library provides a taxonomy and a tool for classification
%% in computer science. Use 1-3 paths, and use right arrows between the
%% about three levels in the path; each path requires a new line.

\classification{\protect{\ \\
\  Theory of Computation $\rightarrow$ Design and Analysis of Algorithms  $\rightarrow$ Data Structures Design and Analysis $\rightarrow$ Data Compression  \\
\  Theory of Computation $\rightarrow$ Design and Analysis of Algorithms  $\rightarrow$ Data Structures Design and Analysis $\rightarrow$ Pattern Matching \\
\  Theory of Computation $\rightarrow$ Design and Analysis of Algorithms  $\rightarrow$ Data Structures Design and Analysis $\rightarrow$ Sorting and Searching
}}

%% if you want to quote someone special. You can comment this line out and there will be nothing on the document.
%\quoting{Bachelor's degrees make pretty good placemats if you get them laminated.}{Jeph Jacques}


%% OPTIONAL STEP: Set up properties and metadata for the pdf file that pdfLaTeX makes.
%% Your name, work title, and keywords are recommended.
\hypersetup{
    unicode=true,           % to show non-Latin characters in Acrobat’s bookmarks
    pdftoolbar=true,        % show Acrobat’s toolbar?
    pdfmenubar=true,        % show Acrobat’s menu?
    pdffitwindow=false,     % window fit to page when opened
    pdfstartview={FitH},    % fits the width of the page to the window
    pdftitle={},            % title
    pdfauthor={},           % author
    pdfsubject={},          % subject of the document
    pdfcreator={},          % creator of the document
    pdfproducer={pdfLaTeX}, % producer of the document
    pdfkeywords={something} {something else}, % list of keywords for
    pdfnewwindow=true,      % links in new window
    colorlinks=true,        % false: boxed links; true: colored links
    linkcolor=black,        % color of internal links
    citecolor=black,        % color of links to bibliography
    filecolor=magenta,      % color of file links
    urlcolor=cyan           % color of external links
}

%%-----------------------------------------------------------------------------------

\begin{document}

% Generate title page.
\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% STEP 3:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Write your abstract to be positioned here.
%% You can make several abstract pages (if you want it in different languages),
%% but you should also then redefine some of the above parameters in the proper
%% language as well, in between the abstract definitions.

\begin{abstract}

  Abstract here. Last thing to write

\end{abstract}

% Place ToC
\newpage
\mytableofcontents
\mainmatter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% STEP 4: Write the thesis.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Your actual text starts here. You shouldn't mess with the code above the line except
%% to change the parameters. Removing the abstract and ToC commands will mess up stuff.
%%
%% You may wish to include material to avoid browsing the definitions
%% above. Command \include{file} includes the file of name file.tex.
%% As a side effect, subsequent inclusions may force a page break.

% BSc instructions
%\include{bsc_finnish_contents}
%\include{bsc_english_contents}
% MSc instructions
%\include{msc_finnish_contents}
%\include{msc_english_contents}


%% Puglisin kommentteja:
%% - Experiments section saattaa paisua aika paljon. Voisi mahdollisesti jakaa useaan kappaleeseen. Esim implementation kohtaa voisi laittaa myös ukkonen kappaleeseen.

%% Intro ja Conclusions luonnollisella kielellä. Muualla teknistä kamaa.

%% ei kappalejakoa introon. sectionit pois.
\chapter{Introduction}~\label{chp-intro}
%  vitusti viitteitä ja pelkkää LUONNOLLISTA kieltä. Ei esim määritelmiä!

\section{Motivation}
  %% - scs yleisesti datan pakkaamiseen
  %% - molekyylibiologia sekvensointi vaikeaa pitkälle pätkälle
  %%   - molekyylin pilkkominen random paloiksi, sekvensointi ja superstringin ottaminen.
  %%     saadaan approksimaatio alkuperäisestä pitkästä sekvenssistä
  %%   - toimii käutännössä hyvin \citep{Peltola83}
  

  %% introssa motivaatiossa rlz mainittu mutta myöhemmin teknisesti
  
  %% alanko or someone mentionet thet it would be interesting to see an implementation

  \section{Related Work}

  %% O(nm) based algorithm based on KnuthMorrisPratt \citep{Knuth77} \citep{Tarhio88}
  %% perustuu kaikkien overlap parien laskemiseen kmp:llä.

  %% hae jostain tietokannasta approx scs algoja !

  %% related workista oma kappale jos muita vertailuja kuin alanko ja norri. muuten ehkä introon.
  
  \section{Structure of the Thesis}

%%
\chapter {Shortest Common Superstring (preli)}


%% preliminaries including syntax

%% what is an approximation algorithm

%% Maininta aakkostosta?

%% Tätä voi ehkä jakaa sectioneihin?

%% The algorithm described in this thesis \\[1in]


TODO:
In this chapter we define the concepts used in the thesis. These include the concepts related to
strings as well as the concept of approximation algorithms and state automata. The definition of
linear time complexity is also recalled. This chapter contains the explanations in natural
languages following a corresponding formal definitions. First we define the concepts related
to strings followed by ...  \\[1in]



Regardless of the alphabet a string is simply a finite sequence of symbols. The length of the string equals the number of symbols it contains. 

\begin{definition}[\textsc{String}]~\label{def-string}
  
  For an alphabet $\Sigma$ a string $S$ is a finite sequence of symbols  ${s_1\cdots s_n}$ where each symbol $s_i \in \Sigma$.
  The length of the string $S$ denoted by $|S| = n$ is the number of symbols in the string.
  An empty string denoted by $\epsilon$ contains zero symbols.
  
\end{definition}

If a string $S$ contains another string $S'$ we say that the string $S'$ is a substring of a string $S$ and string $S$ is a superstring of $S'$.
By containing a string we mean that there is a sequence of symbols in $S$ that spells out $S'$

\begin{definition}[\textsc{Substring and Superstring}]~\label{def-sub-super}
  Let there be two strings $S$ and $S'$.
  $S'$ is a substring of $S$ if and only if $S$ can be written as $RS'T$ where $R$ and $T$ are strings. $R$ and $T$ may be empty.
  If $R$ and $T$ are both empty then $S = S'$ which is a substring of itself.
  An empty string is trivially a substring of every other string.\\

  If $S'$ if a substring of $S$ then $S$ is a superstring of $S'$
\end{definition}

If a string contains (I.e. is a superstring) a set of strings, it is said to be a common superstring of the set.

\begin{definition}[\textsc{Common superstring}]~\label{def-cs}

  Let $R = \{S1,...,Sn\}$ be a set of strings and $T$ be a string.
  $T$ is a common superstring of $R$ if and only if $T$ is a superstring for each $Si$ such that $i\in 1,...,n$.
  
\end{definition}

Trivially a common superstring of a set of strings can be constructed by catenating all of the strings of the set together.
This kind of string has a length that equals the lengths of all the substrings added together.

In many cases there exists one or more common superstrings that are shorter than the common superstring produced by this naive methot.

(if there is no overlaps then the naive method yields the csc ... ref ??)

If a common superstring is a shortest possible it is said to be a shortest common superstring. Finding a shortest common superstring is
an \textsc{NP}-hard problem. 

\begin{definition}[\textsc{Shortest Common Superstring}]~\label{def-scs}

  Let string $T$ be a common superstring of a set of strings $K$.
  $T$ is a shortest common superstring of $K$ if and only if there does not exist a common superstring of $K$ which is shorter than $T$.
  
\end{definition}

% Tarhio88
%Since the decision version of this problem is NP-complete... \citep{Garey79}


%-Lemma: Jos x on y:n substring s.t. y kuuluu R niin scs(R) == scs(Rux)
%todistus ???

%definition: reduced set of strings = joukko stringejä joissa mikään string ei ole toisen substring.

%definition: overlap note this is not symmetric!
%maximal overlap.


%%
\chapter{Ukkonen's Algorithm}

In this chapter we describe Ukkonen's linear time approximation algorithm for
\textsc{Shortest Common Superstring} problem in detail. First we define the heuristic that the
algorithm uses in terms of strings. Then we show that the problem can be reduced to the problem
of finding a longest Hamiltonian path in a specific overlap graph that encodes the same SCS problem.
After the problems are proven to be equivalent we define the same heuristic in terms of graph theory.
The equivalency of the two approaches are necessary since the bounds of the achieved compression
is proven using the \textsc{Longest Hamiltonian Path} problem.

Further we define the Aho-Corasick (AC for short) machine as it is used in the algorithm for finding
the maximal overlaps between the set of input keywords. The time complexity of the construction of
the AC-machine is also discussed. We prove the correctness of the property of the AC-machine to find
the maximal pairwise overlaps between every input keyword.

This chapter also contains four pseudo code blocks which essentially forms the algorithm as whole.
The algorithms represented here are slightly changed and contain few corrections from the form they
were originally presented...... to be continued...

\listofalgorithms

\section{The Greedy Heuristic}

%% Introtaan overlap graph
%% demotaan että tästä selvästi saadaan superstringi
%% nyt pitää vielä näyttää että tällä tavalla (overlap graphista saatu) lyhyin superstringi on se the scs!
%%   tämän todistamiseksi tarvitaan
%%   - näytetään että decomposition on sama
%%   - parittaiset decompositionit on maksimaalisia
  
%% Note to Simon: The purpose of this chapter is to introduce the greedy heuristic using strings. Then
%% we show that the analogy between the SCS and longest Hampath problem and define the heuristics using hampath. The point of proving the problems are equivalent is that Tarhio and Ukkonen used it to prove the
%% correctness and the bound (approx scs >= 1/2 scs). 
%% I think this whole section is poorly written and it is hard to follow. Maybe there is something missing
%% and it is not even correct. Any ideas how to improve the reaability and emphasise the important things?

Let there be a set of strings $R=\{s_1,...,s_m\}$. A naive method for constructing a common superstring
is to concatenate all the strings in the set. For the set $R$ the resulting superstring
is $s_1s_2\cdots s_m$.
The following greedy heuristic can be used to the construction of the approximation of the shortest
common superstring.

\begin{enumerate}
\item Examine the set and remove the two strings $s_i$ and $s_j$ that have the longest overlap among all pairs of strings $(s_i, s_j)$ s.t. $i\neq j$. Note that the longest overlap may be an empty string. If there are multiple pairs of strings with the longest overlap, the decision can be made arbitrarily.
\item Merge the strings together such that they are maximally overlapped to form a shortest common superstring of the set $\{s_i, s_j\}$.
  \item Add the new string back to the set $R$ and repeat until there is only one string left.
\end{enumerate}

It turns out that the \textsc{Shortest Common Superstring} problem is analogous with a special case of
the \textsc{Longest Hamiltonian Path} problem, that is, when the set of strings is converted into
a weighted complete digraph as follows. The graph $G_R=(V_R,E_R)$ is constructed from the set of strings $R$.
Let there be a vertex $v_i\in V_R$ for each string $s_i\in R$. Moreover, we define the start node and
the end node $\{v_{start},v_{end}\}\in V_R$. Each vertex $v_i$ has a directed weighted edge to each
vertex
$v_j$ such that $i\neq j$. The start node has an edge to each $v_i$ and each $v_i$ has an edge to the
end node. The weight $(v_{start}, v_i) = 0$ and the weight $(v_i, v_{end}) = 0$ for all $i=1,...,m$.
The weight of every other edge $(v_i, v_j)$ is the length of the maximum overlap between $s_i$ and $s_j$.

%-intro hamiltonian pathiin!
%-special case of shortest hampath in weighted directed graph.
%- graafin nodet keywordeja SEKÄ alku ja loppunode
%- graafin kaarten painot maksimaalisia overlappeja

Since the subgraph consisting of vertices $\{v_i\}, i=1...m$ is a complete graph, there certainly exists a
Hamiltonian path(s). Taken by any
of the Hamiltonian paths $H$ from the graph $G_R$, we can construct a common superstring of $R$ by
overlapping the strings representing the nodes in the same order they appear on the path in the
following way.

Let us take the strings in the same order as their corresponding nodes appear in the path $H$. Put the
strings above each other such that they are maximally overlapped. The projection $p(H)$ which describes
the strings that are maximally overlapped (i.e. the overlaps are not duplicated) clearly results as a
common superstring of $R$. Depending on the Hamiltonian path $H$ the overlaps in the
projection $p(H)$ have different lengths hence the projection changes depending on the path $H$. 

\begin{figure}
\centering
\begin{tikzpicture}
  
  \node[enode] (H1) at (0,0) {};
  \node[enode] (H2) at (14,0) {};
  \path [|-|] (H1) edge node[below] {p(H)} (H2);

  \node[enode, above = 0.5cm of H1] (T1a) {};
  \node[enode, above right = 0.5cm and 3.3cm of H1] (T1b) {};
  \path[|-|] (T1a) edge (T1b);
  \node[enode, right = 0cm of T1b] (T1txt) {$t_1$};

  \node[enode, above right = 0.5cm and 1.2cm of T1a] (T2a) {};
  \node[enode, right = 2.6cm of T2a] (T2b) {};
  \path[|-|] (T2a) edge (T2b);
  \node[enode, right = 0cm of T2b] (T2txt) {$t_2$};
  
  \node[enode, above right = 0.5cm and 2cm of T2a] (T3a) {};
  \node[enode, right = 2.6cm of T3a] (T3b) {};
  \path[|-|] (T3a) edge (T3b);
  \node[enode, right = 0cm of T3b] (T3txt) {$t_3$};

  \node[tnode, rotate = 40, above = 0.2cm of T3b] (dots1) {...};

  \node[enode, above right = 1cm and 2.2cm of T3a] (Tia) {};
  \node[enode, right = 2.6cm of Tia] (Tib) {};
  \path[|-|] (Tia) edge (Tib);
  \node[enode, right = 0cm of Tib] (Titxt) {$t_i$};

  \node[enode, above right = 0.5cm and 1.2cm of Tia] (Ti1a) {};
  \node[enode, right = 2.6cm of Ti1a] (Ti1b) {};
  \path[|-|] (Ti1a) edge (Ti1b);
  \node[enode, right = 0cm of Ti1b] (Titxt) {$t_{i+1}$};

  \node[enode, above = 0.1cm of Tia] (zc) {};
  \node[enode, below = 0.1cm of Ti1a] (za) {};
  \node[enode, above = 0.1cm of Tib] (zb) {};

  \path[|-|, shorten >= -0.3cm] (zc) edge node[right = -0.1cm,fill=white,align=center] {$u_i$} (za);
  \path[|-|] (za) edge node[fill=white,align=center] {$v_i$} (zb);

  \node[tnode, rotate = 35, above = 0.2cm of Ti1b] (dots2) {...};

  \node[enode, above right = 1cm and 2.2cm of Ti1a] (Tma) {};
  \node[enode, right = 3.455cm of Tma] (Tmb) {};
  \path[|-|] (Tma) edge (Tmb);
  \node[enode, right = 0cm of Tmb] (Titxt) {$t_m$};  
  
\end{tikzpicture}
\caption{The projection $p(H)$ of the set of strings $\{s_1,...,s_m\}$} \label{fig-projection}
\end{figure}

% INTORO, VÄITE, PERUSTELU, KONKLUSIO

% PAPERISSA TÄÄ KOKO PASKA ON REDUCED. eli käydään ensin reduced läpi ja tehdään tämä sille.. ?

The figure \ref{fig-projection} shows an example of the projection constructed from a
Hamiltonian path $H$ in an overlap graph. The corresponding set of keywords consists of
strings $s_1$ to $s_m$. The strings $t_1$ to $t_m$ are the same strings in the order they appear on $H$.
We can represent the string $t_i, i=1...m-1$ as a (prefix,suffix) pair
$(u_i,v_i)$ where $v_i$ is the suffix of $t_i$ that is also a prefix of $t_{i+1}$ such that the
overlap between $t_i$ and $t_{i+1}$ is maximal. $u_i$ is the prefix of $t_i$ whose length is $|t_i|-|v_i|$
i.e. $u_i$ is the part of $t_i$ that can not be overlapped with $t_{i+1}$. Using this partitioning we can
write the projection $p(H)$ as $u_1u_2\cdots u_{m-1}t_m$.

Since the maximal overlaps $v_i,i\in1,...,m-1$ appears in the projection only once, the length of the
projection is $(|s_1|+...+|s_m|) - (|v_1|+...+|v_{m-1}|)$. The length of the maximal overlaps
are equivalent
with the weights of the overlap graph $G_R$ hence $v_1+...+v_{m-1} = |H|$. As the size of $|H|$ grows, the size
of the projection gets shorter.

%That means the shortest common superstring of $R$ corresponds with the longest Hamiltonian path in $G_R$.
% ^ This must be proven!

With this analogy the size of $H$ is the compression of the common superstring
and the length of $p(H)$ is the length of the common superstring.

We now know that the Hamiltonian path in the overlap graph can be used to form a common super string
of the set of keywords. Next we conclude that also the shortest common superstring of $R$ has a
corresponding Hamiltonian path. To prove this we introduce the following lemma.


\begin{lemma}[]~\label{lem-composition}
The compositions of $p(H)$ and the shortest common superstring $z$ are equivalent.
\end{lemma}
\begin{proof}
  The lemma can be proven by construction. Let there be a set of strings $R=\{r_1,r_2,...,r_m\}$ and
  the shortest common superstring $z$ for $R$. The indices of the strings $r_i$ to $r_m$ are in the same
  order they appear in $z$. Let $y_i$ be the longest suffix of $r_i$ such that it is also a prefix
  of $r_{i+1}$. I.e there is an overlap $y_i$ between $r_i$ and $R_{i+1}$. Additionally, let us
  define $x_i$ to be the prefix of $r_i$ that does not overlap with $y_i$. By this construction we
  see that the composition of any SCS for any set of strings is equivalent with the composition
  of the superstring acquired via the projection from the Hamiltonian path.

  Hence, there exists a
  Hamiltonian path and the corresponding projection for any SCS.
\end{proof}

By lemma \ref{lem-composition} there exists a decomposition of the SCS $z$ that corresponds a
Hamiltonian path in an overlap graph. This proves the following theorem.


  %% niin oikea scs on tämän verkon lyhin hampathi. ahne heuristiikka kuitenkin antaa usein jonkin
  %% muun ratkaisun.
  %% koska projektion p(H) sekä common superstringin konstruointi on analogisesti ekvivalenttia niin lyhin
  %% projektio tarkoittaa lyhintä common superstringiä.


\begin{theorem} [] ~\label{lem-similarity-between-SCS-Hamp}

  The projection $p(H)$ for the longest Hamiltonian path in the overlap graph is equivalent with
  the shortest common superstring for $R$.

\end{theorem}

The following defines a conceptually equivalent greedy heuristic for the SCS problem in the terms of
a Hamiltonian path in the overlap graph.

To  construct a Hamiltonian path select an edge $e$ from the set $E_R$ such that
\begin{enumerate}
\item $e$ has the largest weight. If there are multiple such edges, the decision can be made arbitrarily.
\item $e$ is not selected before
\item With already selected edges $e$ can be used to form a Hamiltonian path.
\end{enumerate}
The constraint 3 indicates the following:
\begin{enumerate}[i]
\item The selection does not form a cycle.
\item The earlier selected edges can not have the same start node with $e$.
\item The earlier selected edges can not have the same end node with $e$.
\end{enumerate}

%  ongelma ahneessa heuristiikassa on että voi tehdä hyvän valinnan joka pakottaa myöhemmin huonoihin valintoihin.

%  CONCEPTUALLY the ukkonen algo does just that so the proofs apply.

%ennen tätä maininta approksimaatioalgosta. koska np niin käytetään approksimaatiota ja tätä heuristiikkaa

%-määrittele kompressio

%-scsn kompressio on vähintään puolet optimaalisesta \citep{Tarhio88}

\begin{theorem}[]~\label{theorem-heuristic-bound}
  Let $H'$ be the approximate longest Hamiltonian path in the overlap graph created using
  the greedy heuristic and let $H$ be the the actual longest Hamiltonian path. Now,
  
  $|H'|\geq \frac{1}{2}|H|$
  
\end{theorem}
\begin{proof}
  \citep{Tarhio88} proved the theorem.
\end{proof}

In other words the compression achieved by the greedy heuristic is at least half of the optimal
compression. 

%  approx factor (ei välttämättä epsilon mutta jotkut boundit on olemassa ainakin kompressiolle)



\section{AC-Machine}

%  - mitä tää paska oli alunperin: Laajennettu kmp
%  - äärellinen automaatti poikkeuksin (failure)
%  - pattern matching machine

% Ei yhden rivin kappaleita!  
  
  Aho-Corasick machine is a finite state automaton like data structure that was developed to be used
  in a pattern matching program to find occurrences of the given keywords in a text string. \citep{Aho75}
  The input of such pattern matching program contains a finite set of strings called keywords and an
  arbitrary string called a text string. The output of the program is a list of locations of every
  occurrence of every keyword in a text. The AC-machine data structure and the associated pattern
  matching program were described in \citep{Aho75}.

  todo: similarity with kmp ?

  % Tulee fiilis etten tiedä mistä puhun. Kerro vähän enemmän tähän kappaleeseen  
  As the string searching part of \citep{Aho75} is not used in \citep{Ukkonen90} nor it is relevant in
  this thesis, we only discuss the AC-machine as a data structure and omit the string searching
  functionality of the original author.

  Formally AC-machine is defined\footnote{\citep{Aho75} also defined an output function.
  This is needed only when the AC-machine is used for searching strings.} by the goto function
  $g : \mathbb{N} \times \Sigma \rightarrow \mathbb{N}, g(s_s,c) = s_d$ and the failure function
  $f : \mathbb{N} \rightarrow \mathbb{N}, f(s_s) = s_d$


  When the AC-machine is used in a text search program,
  the goto function describes the state transition from start state $s_s$ to the destination state
  $s_d$ with the current input symbol $c$ of the state automaton. If there is no such goto transition
  the failure function $f$ is queried and the failure transition from the start state $s_s$ to
  the destination state $s_d$ occurs. The alphabet $\Sigma$ contains every symbol in the set of
  keywords encoded by the AC-machine.

  To construct the AC-machine for the set of keywords the \textsc{FSA} like goto function is first
  created. The goto function is then used to construct the failure function which completes the
  construction of the AC-machine. The goto function is similar with a child function of a trie.
  I.e it describes a rooted tree that stores a set of keywords such that edges are labeled with the
  symbols of the alphabet and the edge label is different between each sibling nodes. We say that a node
  spells out a string that is formed by concatenating the edge labels from root to the node.
  The failure function describes chain of state transitions in a case when the goto function is not defined
  for the current input symbol. The failure function is queried as long as the goto function is not defined.
  In the AC-machine the goto function is defined for every symbol in the alphabet. Therefore the
  failure function does not need to be defined for the root node. Other nodes, whether the
  goto function is fully defined or not always have a defined failure function value.
  The algorithms \ref{ac-goto} and \ref{ac-fail} are used as a complete definitons of the goto and
  failure functions. 

  The figure \ref{fig-ac} describes an example of the AC-machine constructed from the set of
  keywords $K=\{$"baa", "baba" , "abab" , "aab"$\}$. The solid lines describes the goto function
  and the dashed lines describes the failure function. For example $g(8, a) = $b and $f(4)=11$.

%  Can produce "aababaa" (scs with compression 14-7=7) or "baababab" with compression 14-8 = 6.

  \begin{algorithm}[h!]
    
    \caption{Construction of the goto function} \label{ac-goto}
    % TODO: Kirjoita omin sanoin input, output, method.
    \hspace*{\algorithmicindent} \textbf{Input:} Set of keywords $K = \{s_1,s_2...,s_k\}$.\\
    \hspace*{\algorithmicindent} \textbf{Output:} Goto function $g$ that is initially undefined for all states\\
    \hspace*{\algorithmicindent} \textbf{Method:} The index of the root node is 1. The procedure \textit{insert($s$)} inserts into the goto graph a path that spells out $s$ or does nothing if there already exists such path.
    
      \begin{algorithmic}[1]

        \Function{calculateGotoFunction}{$K = \{s_1,s_2...,s_k\}$}
          \State $newstate\gets 1$
          \For{$i \gets 1$ until $k$}
            \State $INSERT(s_i)$
          \EndFor
          \For{all $a$ s.t. $g(1,a)$ is not defined }
            \State $g(1,a)\gets 1$
          \EndFor
        \EndFunction

      \item[]

        \Function{insert}{$s = (a_0,a_1,...,a_{m-1})$}
          \State $state\gets 1$
          \State $j\gets 0$
          \While{$g(state, a_j)$ is defined}
            \State $state\gets g(state,a_j)$
            \State $j\gets j+1$
          \EndWhile
          \For{$p\gets j$ until $m-1$}
            \State $newstate\gets newstate+1$
            \State $g(state,a_p)\gets newstate$
            \State $state\gets newstate$
          \EndFor
        \EndFunction
      \end{algorithmic}
      
  \end{algorithm}

  Algorithm 1 creates a goto function. It begins by creating the root node in line 2.
  The algorithm continues by looping over the set of keywords and inserting them to the
  goto function in lines 3 to 5. At the end of the main function in lines 7 to 8
  the goto function is defined for all $g(1,\sigma)$ that were undefined creating a loop within
  the root node. This ensures that the failure function of the root node is never used when the
  AC-machine is used for searching strings.

  The procedure \textsc{INSERT} inserts a single keyword to the goto graph. The idea in the lines
  13 to 16 is to skip the prefix of $s$ that already exists in the trie. The variable $j$ initialized
  in the line 12 keeps track of this prefix. After the first symbol not already represented in the
  trie is found the new branch is created and the remaining suffix of $s$ is inserted symbol by symbol
  at lines 17-20.

  Let us simulate this algorithm with a set of keywords $K=\{$"baa", "baba" , "abab" , "aab"$\}$.
  algorithm starts from line 2 where the root node is created. The figure \ref{fig-ac-step1}
  presents this state. Since there are four keywords the the first for loop is executed four times.
  In the first round the \textit{insert} function is called for the first keyword "baa".
  In the \textit{insert} function the first while loop is not executed since there is no yet
  outgoing edges from the root node. The for loop is executed for every symbol and the three
  nodes (2,3,4) are added to the goto graph with edge labels 'b', 'a', and 'a'.
  Figure \ref{fig-ac-step2} presents this state of the construction.


  \begin{figure}[h]
  \centering
  \begin{tikzpicture}
%   [scale=.8]

    \node[gnode] (1) at (0,0) {$1$};
    
  \end{tikzpicture}
  \caption{A root node. TODO: bigger arrow heads.} \label{fig-ac-step1}
  \end{figure}

  \begin{figure}[h]
  \centering
  \begin{tikzpicture}
%   [scale=.8]

    \node[gnode] (1) at (0,0) {$1$};
    
    \node[gnode, right = 2cm of 1] (2) {$2$};
    \node[gnode, right = 2cm of 2] (3) {$3$};
    \node[gnode, right = 2cm of 3] (4) {$4$};
    
    \path[->]
    (1) edge node[above] {b} (2)
    (2) edge node[above] {a} (3)
    (3) edge node[above] {a} (4);

  \end{tikzpicture}
  \caption{first key added. TODO: bigger arrow heads.} \label{fig-ac-step2}
\end{figure}

  With the second round of the for loop we add the keyword "baba". In this case the \textit{insert}
  function executes the while loop two times since the common prefix of "baa" and "baba" has a length
  2. At this point the goto graph is branched and the remaining symbols 'b' and 'a' are inserted. The
  figure \ref{fig-ac-step3} corresponds with this state.

  \begin{figure}[h]
  \centering
  \begin{tikzpicture}
%   [scale=.8]

    \node[gnode] (1) at (0,0) {$1$};
    
    \node[gnode, right = 2cm of 1] (2) {$2$};
    \node[gnode, right = 2cm of 2] (3) {$3$};
    \node[gnode, above right = 1cm and 2.277cm of 3] (4) {$4$};
    
    \node[gnode, right = 2cm of 3] (5) {$5$};
    \node[gnode, right = 2cm of 5] (6) {$6$};

    \path[->]
    (1) edge node[above] {b} (2)
    (2) edge node[above] {a} (3)
    (3) edge node[above] {a} (4)

    (3) edge node[above] {b} (5)
    (5) edge node[above] {a} (6);

  \end{tikzpicture}
  \caption{second key added. TODO: bigger arrow heads.} \label{fig-ac-step3}
\end{figure}

  In the same way, the rest of the keywords are added. For keyword "baba" the the insertion
  happens in the same way as with the first keyword, that is the while loop in the \textit{insert}
  function is not executed since there is no goto transition from the root with a symbol 'a'.
  For the last keyword the branching of the goto graph happens again
  similarly with the second keyword. At this point the goto function is a trie of the
  input keywords. After the first for loop in the main function is ecexuted 
  the second loop in lines 6 to 8 creates an edge from root to root for every symbol in the
  alphabet such that $g(1,\sigma)$ is not defined. With this example, we already have an edge from the
  root for both of the symbols in the binary alphabet, hence no edges are created in this step.
  However, the corresponding edge is drawn to the image for clarity. This completes the
  algorithm \ref{ac-goto}.
  The resulting goto graph is described in the figure \ref{fig-ac-goto}

  \begin{figure}[h]
  \centering
  \begin{tikzpicture}
%   [scale=.8]

    \node[gnode] (1) at (0,0) {$1$};
    
    \node[gnode, above right = 1cm and 2.277cm of 1] (2) {$2$};
    \node[gnode, right = 2cm of 2] (3) {$3$};
    \node[gnode, above right = 1cm and 2.277cm of 3] (4) {$4$};
    
    \node[gnode, right = 2cm of 3] (5) {$5$};
    \node[gnode, right = 2cm of 5] (6) {$6$};

    \node[gnode, right =  2cm of 1] (7) {$7$};
    \node[gnode, right =  2cm of 7] (8) {$8$};
    \node[gnode, right =  2cm of 8] (9) {$9$};
    \node[gnode, right =  2cm of 9] (10) {$10$};
    
    \node[gnode, below right = 1cm and 2.277cm of 7] (11) {$11$};
    \node[gnode, right = 2cm of 11] (12) {$12$};

    \path[->]
    (1) edge[loop,in=110,out=190,distance=3cm] node[below right,align=center] {$\neg \{a,b\}$} (1)
    
    (1) edge node[above] {b} (2)
    (2) edge node[above] {a} (3)
    (3) edge node[above] {a} (4)

    (3) edge node[above] {b} (5)
    (5) edge node[above] {a} (6)

    (1) edge node[above] {a} (7)
    (7) edge node[above] {b} (8)
    (8) edge node[above] {a} (9)
    (9) edge node[above] {b} (10)

    (7) edge node[above] {a} (11)
    (11) edge node[above] {b} (12);

  \end{tikzpicture}
  \caption{The complete goto graph. TODO: bigger arrow heads.} \label{fig-ac-goto}
\end{figure}

  Let us define a depth function for the AC-machine. At this point depth is merely a concept of the
  machine but later on the corresponding data structure is also created in algorithm \ref{ukk-pre}.
  In an AC-machin the depth of a state $p$, denoted by $d(p)$, is a depth of the node (number of
  edges from root to that node) that corresponds
  the state in the goto graph. That is equivalent with the length of the string that represents the
  node. i.e if a node $p$ in the goto graph is represented by the string $s$, then $d(p) = s$.
  
  Next we describe the construction of the failure function in more detail. The failure function
  for a state is calculated using the failure function of the parent of that state thus the computation
  starts from the root to the nodes of depth 1 to the nodes of depth 2 and so on. That clearly
  equals with the index of a symbol in the string that the node spells out. E.g. the depth of
  the node that spells out "baa" is 3.

  Since the failure function is not defined for the root node (depth 0), the calculation is started from
  depth 1. We define $f(s)=1$ for all states $s$ with depth = 1. Assuming the failure function is
  calculated for all nodes of depth d-1, then the failure function of a node s is defined with the
  following instructions.
  
  \begin{figure}[h]
  \begin{enumerate}
  \item Let $p$ be the parent of the node $s$ such that $g(p,\sigma) = s$.
  \item Set $state = f(p)$.
  \item Set $state = f(state)$ until $g(state,\sigma)$ is defined. Since $g(1,\sigma)$ is defined
    for all symbols that state is always defined.
  \item Set $f(s) = state$
  \end{enumerate}
   \caption{Listing of the failure calculation} \label{failure-calculation}    
  \end{figure}

  Let us continue with the example from the goto function construction. By the definiton $f(1)$ is not
  defined. For the states of depth 1 ($s\in\{2,7\}$) the failure function is defined $f(s)=1$. The
  figure \ref{fig-ac-step5} represents this state.

  \begin{figure}[h]
  \centering
  \begin{tikzpicture}
%   [scale=.8]

    \node[gnode] (1) at (0,0) {$1$};
    
    \node[gnode, above right = 1cm and 2.277cm of 1] (2) {$2$};
    \node[gnode, right = 2cm of 2] (3) {$3$};
    \node[gnode, above right = 1cm and 2.277cm of 3] (4) {$4$};
    
    \node[gnode, right = 2cm of 3] (5) {$5$};
    \node[gnode, right = 2cm of 5] (6) {$6$};

    \node[gnode, right =  2cm of 1] (7) {$7$};
    \node[gnode, right =  2cm of 7] (8) {$8$};
    \node[gnode, right =  2cm of 8] (9) {$9$};
    \node[gnode, right =  2cm of 9] (10) {$10$};
    
    \node[gnode, below right = 1cm and 2.277cm of 7] (11) {$11$};
    \node[gnode, right = 2cm of 11] (12) {$12$};

    \path[->]
    (1) edge[loop,in=110,out=190,distance=3cm] node[below right,align=center] {$\neg \{a,b\}$} (1)
    
    (1) edge[bend left=15] node[above] {b} (2)
    (2) edge node[above] {a} (3)
    (3) edge node[above] {a} (4)

    (3) edge node[above] {b} (5)
    (5) edge node[above] {a} (6)

    (1) edge[bend right=15] node[above] {a} (7)
    (7) edge node[above] {b} (8)
    (8) edge node[above] {a} (9)
    (9) edge node[above] {b} (10)

    (7) edge node[above] {a} (11)
    (11) edge node[above] {b} (12);

    \path [->,dotted]

    (2) edge[bend left=15] (1)
    (7) edge [bend right=15] (1);
    
  \end{tikzpicture}
  \caption{The failure function construction when $f(s)$ is defined for all states $s$ with depth <= 1. TODO: bigger arrow heads.} \label{fig-ac-step5}
\end{figure}

  Next we calculate the failure function for the states 3, 8 and 11 that is the states with depth 2.
  For state 3 we take the parent state 2 and follow the the failure function to the root node. Since
  $g(1,$a$) = 7$ we define $f(3)=7$. For the states 8 and 11 the procedure is similar. $f(8)$ gets
  the value 2 and $f(11)=7$. The procedure is iterated until the failure function is defined for every
  node. The figure \ref{fig-ac} describes the final result. Because the AC-machine only contains these
  two functions, the figure \ref{fig-ac} also presents the fully constructed AC-machine.

    \begin{figure}
  \centering
  \begin{tikzpicture}
%   [scale=.8]

    \node[gnode] (1) at (0,0) {$1$};
    
    \node[gnode, above right = 1cm and 2.277cm of 1] (2) {$2$};
    \node[gnode, right = 2cm of 2] (3) {$3$};
    \node[gnode, above right = 1cm and 2.277cm of 3] (4) {$4$};
    
    \node[gnode, right = 2cm of 3] (5) {$5$};
    \node[gnode, right = 2cm of 5] (6) {$6$};

    \node[gnode, right =  2cm of 1] (7) {$7$};
    \node[gnode, right =  2cm of 7] (8) {$8$};
    \node[gnode, right =  2cm of 8] (9) {$9$};
    \node[gnode, right =  2cm of 9] (10) {$10$};
    
    \node[gnode, below right = 1cm and 2.277cm of 7] (11) {$11$};
    \node[gnode, right = 2cm of 11] (12) {$12$};

    \node[tnode, above left = 2.725cm and 2.277cm of 1] (v1) {};
    \node[tnode, below left = 1cm and 2.277 of 1] (v2) {};
    
    \path[->]
    (1) edge[loop,in=110,out=190,distance=3cm] node[below right,align=center] {$\neg \{a,b\}$} (1)
    
    (1) edge[bend left=15] node[above] {b} (2)
    (2) edge node[above] {a} (3)
    (3) edge node[above] {a} (4)

    (3) edge node[above] {b} (5)
    (5) edge node[above] {a} (6)

    (1) edge[bend right=15] node[above] {a} (7)
    (7) edge node[above] {b} (8)
    (8) edge node[above] {a} (9)
    (9) edge node[above] {b} (10)

    (7) edge[bend right=15] node[above] {a} (11)
    (11) edge node[above] {b} (12);

    \path [->,dotted]

    (2) edge[bend left=15] (1)
    (7) edge [bend right=15] (1)
    
    (3) edge (7)
    (8) edge (2)
    (11) edge[bend right=15] (7)
    
    %%(4) edge (v1) edge (v2) edge (11)
    (5) edge (8)
    (9) edge (3)
    (12) edge (8)
    
    (6) edge (9)
    (10) edge (5);

    \path[->,dotted,rounded corners, shorten <= 0.5cm, shorten >= 0.5cm] plot coordinates {(4) (v1) (v2) (11)};
    % repaint these nodes
    \node[gnode, above right = 1cm and 2.277cm of 3] (4) {$4$};
    \node[gnode, below right = 1cm and 2.277cm of 7] (11) {$11$};
    

  \end{tikzpicture}
  \caption{Fully constructed AC-machine for the keywords $R=\{$"baa", "baba", "abab", "aab"$\}$. TODO: bigger arrow heads.} \label{fig-ac}
\end{figure}


    In the previous example the value for the failure function was always found in a single iteration
    of the step 3 of the above definition. Next we go through the pseudocode for the failure function
    calculation in algorithm \ref{ac-fail}.


  \begin{algorithm}[h!]

    \caption{Construction of the failure function} \label{ac-fail}
    \hspace*{\algorithmicindent} \textbf{Input:} Goto function $g$ from algorithm \ref{ac-goto}\\
    \hspace*{\algorithmicindent} \textbf{Output:} Failure function $f$

    \begin{algorithmic}[1]
      \Function{calculateFailureFunction}{$g : \mathbb{N} \rightarrow \mathbb{N}$}
        \State $queue\gets \textit{empty}$
        \For {each $a$ s.t. $g(1,a) = s \neq 1$}
          \State $queue\gets queue \cup \{s\}$
          \State $f(s)\gets 1$
        \EndFor
        \While{$queue \neq empty$}\\
          \hspace*{\algorithmicindent}let $r$ be the next state in $queue$
          \State $queue\gets queue \backslash \{r\}$
          \For{each $a$ s.t. $g(r,a) = s$ is defined}
            \State $queue\gets queue \cup \{s\}$
            \State $state\gets f(r)$
            \While{$g(state,a)$ is undefined}
              \State $state\gets f(state)$
            \EndWhile
            \State $f(s)\gets g(state,a)$              
          \EndFor
        \EndWhile
          
      \EndFunction

    \end{algorithmic}
  \end{algorithm}


  Since the failure values must be calculated from lowest depth nodes to the highest depth nodes
  we go through the goto path using a breadth-first search. This is provided by the FIFO that is
  appended in every non-leaf node. I.e. the children of the currently processed node are put into the
  queue and the next node to be processed is popped. The queue is initialized to be empty in the first
  line of the algorithm. The for loop in lines 3 to 6 adds all the children of the root to the queue.
  Because the root node is special as it contains links to itself the additional constraint
  in line 3 is added to prevent the infinite looping. In the same for loop we define the
  $f(s)=1$ for all nodes with depth 1. The while loop from line 7 to 18 implements the instructions
  in the listing \ref{failure-calculation} Since the AC-machine does not save parent information
  this is done a bit differently, although the result is exactly the same. In line 8 and 9 we pop a
  node from the queue. At this point the failure function for that node is already defined. Then we
  process each child of the node separately in lines 10 to 17 by 1) adding the child to the queue
  in line 11  2) travelling the failure function of the parent as long as we get a node that have goto
  transition with the same symbol that appears between the parent and the currently processed child
  in lines 12 to 15 and  3) set the failure function for the child in line 16. The execution of the
  function halts when the BFS of the  AC-machine is ready and the queue is finally empty.


  For now, as the AC-machine is fully defined we focus on the important property it can be used as
  the lemma \ref{lem-overlap} suggests.


\begin{lemma}[]~\label{lem-overlap}
  Let there be a state $p$ and state $q, p\neq q$ in an AC-machine that are represented by strings
  $s$ and $t$ respectively. The failure value $f(p) = q$ if and only if $t$ is the longest proper suffix
  of $s$ that is also a prefix of some keyword.
\end{lemma}
\begin{proof}
  Let us have an induction hypothesis that the lemma is true for all states whose depth is smaller
  than some depth $d > 1$. The hypothesis trivially holds for the base case when
  depth of the state is 1. That is when $f(p) = q$ is the root node represented by the empty
  string and the string represented by the state $p$ has a length 1.
  
  Let us assume there is a state $p$ of depth $d$ that is represented by a string
  $s=x_1x_2\cdots x_d$. Let $r$ be the parent state of $p$. Therefore $r$ is represented by a
  string $x_1x_2\cdots x_{d-1}$. Let $r_1,\cdots ,r_n$ be a sequence of states such that:

  \begin{enumerate}
  \item $r_1 = f(r)$
  \item $r_i = f(r_{i-1})$ for all $ 1 \leq i < n$
  \item $g(r_i, x_d)$ is undefined for all $i < n$
  \item $g(r_n, x_d) = q$
  \end{enumerate}

  Since the algorithm \ref{ac-fail} encodes the above rules in lines 13 to 16 the state $q$ is defined
  for $f(p)$. Let the strings $u_1,...u_n$ represent the states $r_1,...,r_n$ respectively.
  According to the induction hypothesis $u_1$ is a longest proper suffix of $x_1x_2\cdots x_{d-1}$
  that is also a prefix of some keyword. $u_2$ is a longest proper suffix of $u_1$ that is also
  a prefix of some keyword and so on. Therefore $u_n$ is a longest proper suffix of $s$ such that
  $u_nx_d$ is also a prefix of some keyword. Since the algorithm \ref{ac-fail} sets $f(p) = q$,
  the lemma is proven.
  
\end{proof}  

The lemma \ref{lem-overlap} is used later on in the description of Ukkonen's algotithm. Next we look
over the time complexities of the algorithms \ref{ac-goto} and \ref{ac-fail}. First we shall investigate
the time complexity of the insert and read operations of the functions $g$ and $f$. The failure
function maps each state (except the root) to some other state. It is most effective and practical
to implement this as an array which stores the value $f(s)=d$ such that the array index $s$ has
a value $d$. Therefore the both inserting and reading to/from the failure function is performed
in constant time. The goto function maps a pair of values to a single value.
In the same way, if we use a two dimensional array which stores
a $|\Sigma|$ sized row for each state in the AC-machine, the read and write operations of $g$ would
perform in a constant time. This of course results in a very high memory usage since the table would be
sparsely used especially with large alphabets and long keywords. The goto function can also
be implemented in many other ways. One solution which improves the memory efficiency is to save
a balanced binary tree for each state in the AC-machine. With binary trees the insert and read
operations are performed in logarithmic time.

The main function of the algorithm \ref{ac-goto} consists of two for loops.
The first loop calls a procedure once for each keyword of the input. The function calls are
executed in a constant time so the time complexity of the first for loop is linear with respect
to the number of input keywords. The second for loop starting in line 6 is used to search all
children of the root node. Since the root node has only $\Sigma$ goto links the loop is executed at most
$\Sigma$ times. Here $\Sigma$ refers to the size of the alphabet. In the worst case every symbol in
every input keywords is different. In that case $|\Sigma| = n$, where $n$ is te sum of the input symbols.
Therefore the second for loop is executed at most $n$ times resulting the time complexity of the main
function to be linear with respect to the size of the input.

Let us next examine the insert function. The first two lines 11 and 12 can clearly be executed in constant
time. The first while loop is used to find the symbol of the string that is not yet represented
in the goto graph. Let $s=vu$ such that $v$ is the part of $s$ that is already represented in the
goto graph. The index variable $j$ is used so it contains the value of the first symbol of $u$ after
the while loop is executed. The for loop continues from $j$ so the lines 18 to 20 are executed
only for $u$. In the other words the \textit{insert} function behaves in a way that every symbol of
the input keyword is processed either in one of the two loops but not both. Since the time complexities
of the loop bodies depend on the implementation of $g$, the the total time complexity of the function
is $O(n)\cdot O(1) = O(n)$ or $O(n)\cdot O(log(n)) = O(n\cdot log(n))$ when the goto function
can be used in linear or logarithmic time respectively.

The former discussion leads us with the following theorem
\begin{theorem}[]~\label{thm-linear-goto}
The goto function for the AC-machine can be constructed in linear time.
\end{theorem}

Let us examine the algorithm \ref{ac-fail}. In the discussion of theorem
\ref{thm-linear-goto} we reasoned that finding all children of the state can be done in
linear time. In the failure calculation we have the same kind of instruction in the for loop
in line 10. Since this loop is executed for every state of the machine this kind of approach leads
us to $O(n|\Sigma|)$ time complexity for finding every child of every state. Instead of scanning
the goto function for each symbol in the alphabet we can use an auxiliary data structure to
explicitely save the children of each state. One possibility is to use a linked list ($O(1)$ time
insertions) and save the information of the children while creating the goto function. This adds
$O(1)$ time instruction to the for loop in the \textit{insert} function. The body of this for loop
is executed $O(n)$ times so the total time complexity is not changed. This allows us to get the
children of a state in linear time with respect to the number of the children (and not the number of
symbols in the alphabet). Since every state
has at most one parent the body of the for loop is executed $O(n)$ times total neverthless it
is nested with another while loop (line 7). Thus the while loop in line 7 as well as the for loop in
line 10 are executed once for every state that is $O(n)$ times. The lines 11 and 12 are clearly constant
time operations and the time complexity of line 16 depends on the goto function implementation and
can also be a constant time operation. Lastly, let us examine the inner while loop on line 13.
The idea is to travel the failure path until a state is found such that the goto function
for that state is defined with the same symbol as the goto transition is defined for state we are
defining the failure function for. According to the definition of the failure function the state
$y=f(x)$ has a depth at most one less than the depth of the state $x$. This means that the while
loop is always executed at most d times where d is the depth of the state \textit{state}.
... Got stuck here..? How to prove that construction of $f$ is in linear time?

\begin{theorem}[]~\label{thm-linear-failure}
The failure function for the AC-machine can be constructed in linear time.
\end{theorem}

%  note that this works for reduced or nonreduced set.

\section{The approximation algorithm}

In this section we discuss the algorithms that use the AC-machine to find the pairwise overlaps
of a set of input keywords to implement the previously defined greedy heuristic. Conceptually,
we create an overlap graph of the keywords and select the edges according the rules of
figure \ref{failure-calculation}. In practice, the computation works in two steps. First we
preprocess the AC-machine and create the needed auxiliary data structures in algorithm \ref{ukk-pre}
followed by the algorithm \ref{ukk-h} which implements the greedy selection of the edges in the
overlap graph.

By lemma \ref{lem-overlap}, we can find the longest proper suffix of any state that is also
a prefix on some keyword by following the failure path of the state. In particular let the start
state $p$ itself be represented by some keyword (i.e. $p$ is a leaf). Now the failure function
defines the state $q$ that is represented by the longest proper suffix of $p$ that is also a prefix
of every keyword that represents the state(s) $r_i$ such that $q$ is an ancestor of $r_i$. 
Let us denote this kind of descendant relationship with a function $L(p)$ and call it a set of
supporters; $L(p)$ is a set of indices of the keyword set $R$ such that states represented by
$r_i\in R, i\in L(p)$ are descendants of the state $p$. In other words, if a key word index
$j$ is in $L(p)$, then the state represented by $r_j$ is a descendant of the state $p$ and
the maximal overlap between $p$ and $r_j$ has a length $d(q)$. For example, the set $L(3) = 1,2$
for the example AC-machine in figure \ref{fig-ac}.


Since the prefix relation is transitive we can iterate over the failure function to find every
pairwise overlap between $p$ and the keywords. The algorithm \ref{ukk-h} does exactly this.
The following lemma concludes this property.

%% nyt näytetään miten aiemmin mainittua heuristiikkaa pisimmälle hampathille voidaan käyttää
%% yhdessä ac-koneen kanssa muodostamaan scs approksimaatio

%% edellisestä lemmasta f(x) = y iff y suffix of x ...
%% seuraa se että iteroimalla jonkin keywordin failure funktiota löydetään kaikki suffixit
%% jotka ovat toisten prefiksejä. viite seruaavaan lemmaan..

%% Määritellään supporters set L(s) jolle pätee että kaikki keyt jotka vastaavat lehtiä
%% siten että s on niiden edeltäjä, kuuluvat tähän settiin. annetaan esimerkki goto graphissä.
%% määritellään d (tämän vois määritellä jo aiemmin) ja annetaan esimerkki.

%% L:n ja d:n avulla määritellään seuraava lemma

\begin{lemma}[] ~\label{lem-overlap-length}
  Let there be an AC-machine for a keyword set $R=\{t_1,...,t_m\}$ and a state $p$ represented
  by string $s$.
  There is an overlap of length $d(q)$ between $s$ and a keyword $t_i\in R$ if and only if
  for some $k \geq 0$ state $q=f^k(p)$ is such that $i\in L(q)$.
\end{lemma}
\begin{proof}
  Let us have an induction hypothesis that the lemma is true for all states whose depth is
  smaller than some depth $d'\geq 1$. For the base case of the induction we use the start state
  with depth 0. The hypothesis holds
  with $k=0$ since $L(start)$ contains every keyword index $1$ to $m$ and there also exists an empty
  overlap (length $d(start)=0$) between the empty string that the start state is represented by and all
  the keywords in $R$.

  In the induction step we assume there is a state $p, d(p) = d'$ represented by string $s$.
  Assume also that there is an overlap $u$ of length $d(q)$ between $s$ and some keyword $t_i$. The
  overlap $u$ represents a state $q$.
  There are three possibilities:
  \begin{enumerate}
  \item The overlap $u$ is the string $s$ itself. In this case clerarly $i\in L(p)$. Since
    $p=f^0(p)$, that is $p=q$ and $d(q) = d(p) = |s|$, the lemma is true. 
  \item The overlap $u$ is the longest proper suffix of $s$ that is also a prefix of $t_i$.
    By lemma \ref{lem-overlap} this is true if and only if $f(p) = q$. In this case
    $i\in L(q)$ and the lemma is true with $k=1$.
  \item In this case the overlap between $s$ and $t_i$ is shorter than the longest proper
    suffix of $s$ that is a prefix of some keyword $t_j, i\neq j$. Let $u'$ be the maximal
    overlap between $s$ and $t_j$ that is $u'$ is the longest proper suffix of $s$ that is also
    a prefix of some keyword. Let also $q'$ be represented by the string $u'$. By lemma
    \ref{lem-overlap} $f(p) = q'$. Because $u$ and $u'$ are both suffices of $s$ and $|u'|<|u|$ we
    know that there is also an overlap $u$ between $u'$ and $t_i$. Since $d(q') < d(p) = d'$ we can
    apply the induction hypothesis to the state $q'$. Therefore, there is an overlap of length
    $d(q)$ between $q'$ and a keyword $t_i$ (which we know is true) if and only if for some
    $k \geq 0$ state $q=f^k(q')$ is such that $i\in L(q)$. Since we know by assumption $q'=f(p)$ and
    by induction $q=f^k(q')$ it follows that $q=f^{k+1}(p)$. We also know that $i\in L(q)$ and
    $|u|=d(q)$

  \end{enumerate}
  
\end{proof}

The theorem \ref{thm-maximal-overlaps} directly follows from the lemmas \ref{lem-overlap} and \ref{lem-overlap-length}

\begin{theorem}[] ~\label{thm-maximal-overlaps}

  Let $p$ be a state represented by string $s_i$ and let $s_i,s_j\in R$. The maximum overlap between
  $s_i$ and $s_j$ is represented by the state $q$ such that $q$ is the first sate in the failure
  path of $p$ $f^k(p)$ and $j\in L(q)$. The length of this maximum overlap is $d(p)$.

\end{theorem}
  
By theorem \ref{thm-maximal-overlaps} the AC-machine implicitely encodes the information of the
longest pairwise overlaps between any two keywords. By lemma \ref{lem-composition} the maximal
overlaps encoded in the AC-machine are equivalent with the weights of the corresponding overlap
graph, thus we use this property to implement the greedy heuristic for the
\textsc{Longest Hamiltonian Path} problem in the overlap graph. The overlaps encoded by the
failure function have the length equivalent with the depth of the state that have an incoming
failure path hence the longest possible overlap can be found by searching the state
that has some incoming failure path(s) such that the depth of that state is maximized. This suggest
the following implementation. We process every state by follow the failure path and saving the
information of which state the failure transition is coming from. 
the states with greater depth are processed first. This can be done with a reversed breadth
first ordering. Since $d(p) > d(f(p))$ for every $p$, the reversed BFS ordering of the states
ensures that when the state $q$ is being processed, all the states $p$ s.t. $q=f^k(p)$ are already
processed and we have the information of each such state. Since we have the information of every such
state we also have the information of all the overlaps between $L(p)$ and each state $q$ s.t.
$p$ is in failure path of $q$. We can now select a corresponding edge in the overlap graph as long
as it does not violate the constraints of the heuristic, that is, considering earlier selections
the corresponding edge does not form a cycle nor its start state does already have an outcoming
edge nor its end node does already have an incoming edge. By processing all the states in this
fashion we end up with the set of edges that forms a hamiltonian cycle in the
overlap graph. 

Let us apply this method to the AC-machine described in figure \ref{fig-ac}. The processing is started
with depth 4 from state 6 or state 10. While processing those states we save the information of
the incoming failure path for state 9 and 5 respectively. Depending on the order of which we process
the states with depth 3 we end up selecting one of the two possible overlaps. Either the state 5
is processed first and the overlap between $s_3$ and $s_2$ is selected or the state 9 is processed
first and the overlap between $s_2$ and $s_3$ is selected. The information of the incoming failure
paths for states 3, 8 and 11 is also saved while processing the states of depth 3. Let us assume
that the state 9 was processed first and the second of the presented overlaps were selected.
While processing the state 3 we have an information of the incoming failure path from state 6 (via the
state 9). This indicates there exists an overlap between the state 6 and every state
in $L(3)=\{6,4\}$ The self overlap between $s_2$ and $s_2$ is of course forbidden since it would
create a cycle. However the other overlap between $s_2$ and $s_1$ cannot be selected either because
the edge from $s_2$ is selected earlier. The state 8 also has information of two incoming failure
paths. How there already exists an edge with $s_3$ as an end node thus, no edges are selected.
When processing the state 11 the only possible overlap is compatible with the rules and the edge
from $s_1$ to $s_4$ is selected. For the states of depth 1 it does not matter which one is processed
first. In any case while processing the state 7 we find only one failure path coming from a leaf state
corresponding with keyword $s_2$. Since $s_2$ already has an outgoing edge the processing continues
without any selection. While processing the state 2 we find two possible selections. We can select
either $(s_3,s_1)$ or $(s_4,s_2)$. Depending on this last selection the common superstring for
the projection of the selected Hamiltonian path is either "baababab" or "bababaab", which
both have the same length 8 an the same compression 14-8=6.

Since, when finding two equally long maximal overlaps the heuristic allows arbitrary selections,
the outcome of the algorithm may differ between implementations. For example, in the first time
we made a selection between two allowed edges, if we selected otherwise the heuristics would
have produced either "aababaa", "baababa" or "ababaab". These all are the actual shorest common
superstrings with length 7 and compression 7.

%Todo: Why define these here. We could use at least $F(index)$ all over the text.
The actual algorithm is implemented in two parts. The algorithm \ref{ukk-pre} defines the first part
which is used to calculate a set of auxiliary data structures that are needed in order to implement
the actual heuristic. The algorithm takes as an input the AC-machine consturted with
algorithms \ref{ac-goto} and \ref{ac-fail} as well as the set of keywords $R$ which is also
the same set that the goto and failure functions were constructed for. 
The following listing consists of the data structures that are calculated
in algorithm \ref{ukk-pre} and used in algorithm \ref{ukk-h}. The following listin uses $\mathbb{Q}$ as
a set of states in the AC-machine. The set of keyword indices are denoted as $\mathbb{I}$. 

\begin{enumerate}
  \item $F : \mathbb{I} \rightarrow \mathbb{Q}, F(i)=q$ Maps the index of the keyword to the corresponding state. I.e a state $q=F(i)$ is represented by the keyword $s_i$.
  \item $E : \mathbb{Q} \rightarrow \mathbb{I}, E(q)=i$ Inverse function of $F$. If the state $q$ is a leaf represented by $s_i$ then $E(q)=i$. For every non-leaf states $q'$ we define $E(q')=0$.
  \item $d : \mathbb{Q} \rightarrow \mathbb{Q}$ Depth function for each state as defined before.
  \item $L : \mathbb{Q} \rightarrow 2^{\mathbb{Q}}$ Set of supporters as defined before.
  \item $b : \mathbb{Q} \rightarrow \mathbb{Q}, b(q)=r$ Reverse breadth first search ordering of the states.
  \item $B$ The last state in the BFS ordering of the machine. I.e. $b(B)$ is the second last state in BFS ordering, $b(b(B))$ is the third last and so on.
\end{enumerate}

Since by lemma \ref{lem-reduced} the shortest common superstring for a reduced set of keywords
is same with the set where it was reduced from, the algorithm \ref{ukk-pre} also performs
the elimination of those keywords that are substrings of other keywords in the set. There are two
situations when the structure of the AC-machine indicates the occurrence of such a keyword. First,
if there is a goto transition from a a state that is represented by a keyword that keyword
must be a substring of some other keyword. I.e. If $g(F(i),\sigma)$ is defined for any pair
$(i,\sigma)$ then the keyword $s_i$ must be a substring (prefix)  of some other keyword and can be
removed. The second indication of such a keyword is when a leaf state has an incoming failure
transition from some other state. By lemma \ref{lem-overlap}, if $F(i) = f(q)$ then the keyword
$s_i$ must be a proper suffix of some other state. This means that $s_i$ is either a suffix
(if the state $q$ is itself a leaf) or a substring that is not a prefix nor suffix of some other
keyword. The removal of a substring is done using the function $F$. If some keyword
$s_i$ is a substring of another keyword, we set $F(i)=1$ to indicate that the state represented
by $s_i$ is not processed like it was a keyword.

  \begin{algorithm}[h!]

    \caption{Preprocessing algorithm for the greedy heuristic} \label{ukk-pre}
    \hspace*{\algorithmicindent} \textbf{Input:} AC-machine constructed with algorithms \ref{ac-goto} and \ref{ac-fail} and the associated set of keywords $R=\{s_1,...,s_m\}$ \\
    \hspace*{\algorithmicindent} \textbf{Output:} $F, E, d, L, b$ and $B$ as defined earlier.\\
    \hspace*{\algorithmicindent} \textbf{Notes:} initially $E(s) = 0$ for every state s.
    
    \begin{algorithmic}[1]
      \Function{calculateAuxiliaryFunctions}{$g : \mathbb{Q} \rightarrow \mathbb{Q}$, $f : \mathbb{Q} \rightarrow \mathbb{Q}, R=\{s_1,...,s_m\}$}
        \For {$i = 1,...,m$} \\
          \hspace*{1.1cm plus \algorithmicindent} let $x_i = a_0,...,a_{k-1}$
          \State $s\gets 1$
          \For {$j = 0,...,k-1$}
            \State $s\gets g(s,a_j)$
            %% ERRATA: i = j.
            \State $L(s)\gets L(s) \cdot \{i\}$
            \If {$j = k-1$}
              \State $F(i)\gets s$
              \State $E(s)\gets i$
              \If {$s$ is not a leaf of the AC machine}
                \State $F(i)\gets 1)$
              \EndIf
            \EndIf
          \EndFor
        \EndFor
        \State $queue\gets 1$
        \State $d(1)\gets 0$
        \State $B\gets 1$
        \While{$queue\neq empty$}\\
          \hspace*{1.1cm plus \algorithmicindent} let $r$ be the next state in queue
          \State $queue\gets queue \backslash \{r\}$
          %% ERRATA: Cyclic graph!          
          \For {each $s$ that is a child of $r$ and $s\neq r$}
            \State $queue\gets queue\cdot s$
            \State $d(s)\gets d(r)+1$
            \State $b(s)\gets B$
            \State $F(E(f(s)))\gets 1$
          \EndFor
        \EndWhile
      \EndFunction
        
    \end{algorithmic}
  \end{algorithm}


% algoritmin läpikäynti rivi kerrallaan.
The algorithm \ref{ukk-pre} proceeds in two independent steps that could be processed in either order.
In the first part we loop through every keyword (line 2) and every symbol in the keywords (line 5).
Before we enter the for loop in line 5 we set the state $s$ to the start state. In the loop
we traverse the AC-machine from the start state to the state represented by the current keyword $x_i$.
On the way we add the index $i$ to every state we pass through (line 7). Looping every keyword in this
way constructs the function $L$ as defined. At the end of every keyword (if line 8 evaluates to
\textit{true}) we set the values for functions $F$ and $E$ as defined. (lines 9 and 10).
Further we set $F(i) = 1$ for those keyword indices that correspond to a keyword that is a prefix
of some other keyword. (lines 11, 12).

The second part of this algorithm calculates the reversed breadth first ordering $b$ for the
AC-machine and set the depths for every state. In addition, substring keywords others than
prefixes are removed. This part is basically a BFS travel through the machine. We create a queue
and initialize it with the start state $1$ in line 17 and set the depth of the start state to
zero (line 18). The variable $B$ initialized in line 19 contains the value of the latest state
that is already processed, thus at the end of the algorithm the variable $B$ contains the last state
in the BFS ordering (which is the first state in the reversed BFS ordering). The actual BFS starts
in lines 20 to 22 when we start to process the states in the queue. When all states are processed
the while loop ends and the function terminates. In the for loop starting at line 23 we add the
children of the current state to the queue as the BFS requires. In line 25 we set the depth
of the current state to be one greater than its parent state. We also update the $B$ variable
as discussed before. Finally the line 27 sets $F(i) = 1$ for each keyword index $i$ such that
the state represented by $s_i$ has an incoming failure transition from any other state.

The lines 12 together with line 27 ensures that for all keywords $i$ such that $s_i$ is a substring
of some other keyword, the value of $F(i)=1$. I.e. The set of keywords $\{s_j : F(j) \neq 1\}$ is
reduced.

Let us next discuss the time complexity of algorithm \ref{ukk-pre}. As with the construction of
the AC-machine the time complexity of querying the goto function depends on its underlying
implementation. At first we ignore the queries for the goto function and focus on the rest.
In the first part of the algorithm we iterate over all keywords at the outer for loop. In the
inner for loop we iterate over each symbol in the keyword so the total number of iterations
in the inner loop equals the total number of symbols in all the keywords combined. In line 7
we add an element to a set of items $L(s)$. Since this set needs not to be ordered it can be
implemented for example as a linked list so the time complexity for adding an item is $O(1)$.
Also the statements in lines 8 to 12 can clearly be performed in constant time. This means that
the time complexity of the outer for loop is $O(n\cdot g')$ where $n$ is the total number of
symbols in the input and $g'$ is the time complexity of querying the goto function.

The lines 17 to 19 can clearly be executed in constant time. The while loop starting at line 20
does the breadth first search for the AC-machine. Since the number of states in the machine is
$O(n)$ the execution time for the BFS is also $O(n)$. Here we assume that the queue is also
implemented using a structure that enables insertions and deletions in a constant time (E.g.
a linked list). The lines 25 to 27 that are not related to the BFS search itself can also clearly
be executed at constant time. This results the total time complexity of algorithm \ref{ukk-pre} to be
$O(n\cdot g')$. For example if the goto function is implemented using a direct indexing the
reading and writing of $g$ is performed at constant time, thus the time complexity is $O(n)$.
On the other hand, if $g$ is implemented using for example balanced binary trees then the acces
time of $g$ is $O(log (n))$ and the total time complexity of the algorithm is $O(n\cdot log(n))$.
The former discussion leads us to the following theorem.

\begin{theorem}[]~\label{thm-pre-time}
  The time complexity of algorithm \ref{ukk-pre} equals the time complexity of
  algorithms \ref{ac-goto}.
\end{theorem}

Let us now discuss the algorithm that implements the greedy selection of the edges in the
overlap graph to form the approximate longest Hamiltonian path in more detail. Earlier we simulated
the greedy heuristic without defining the appropriate data structures. Now, with algorithm
\ref{ukk-pre} we have augmented the AC-machine with a set of auxiliary functions and one variable.
This auxiliary data is static in the following algorithm \ref{ukk-h} I.e. the data is only read
not written at any point. In addition, we need some dynamic data to store the information
related to the selection of the edges in the overlap graph. As we discussed in the simulation
of the heuristic, when the execution enters a state $q$, all other states with depth $>d(q)$ is
already processed and there is an information of every $F(i)$ that starts the failure path to $q$
such that there is no already selected edge $(s_i, s')$ for any keyword $s'$. Let us denote
this information with $p$ with the following definition. Let $p(q)$ be a set of keyword indices
such that for each $i\in p(q)$ $q$ is in the failure path of $F(i)$
I.e $q = f^k(p_i) : k>0$ and there is no edge $(s_i, s')$ already selected for any $s'\in R$.
When we encounter a state $q$ s.t. $i\in P(q)$ we know that there exists an overlap of length
$d(q)$ between $s_i$ and every keyword whose corresponding state is accessbile in the AC-machine
through the state $q$ i.e. that state is descendant of $q$. We have already defined a data structure
$L$ for this information, hence there is a maximal overlap of length $d$ between eachevery $s_{P(q)}$
and every $s_{L(q)}$. Based on this information, the next edge cannot yet be selected.
Note that although the data structure $P$ ensures that every keyword $s_{P(q)}$ can be selected
for a start node of the new edge the $L(q)$ returns a static data. If there is already selected
edges $(s',s_{L(q)})$ for each $L(q)$ no more edges can be selected while processing $q$.
This constraint is encoded in the function $forbidden(i)$ where $i$ is an index of a keyword.
If the edge with $s_i$ as an end node is already selected or if $s_i$ is a substring of another
keyword then $forbidden(i) = true$. Otherwise, when this kind of edge has not been selected
$forbidden(i) = false$.

We encode the constraints that no node can have multiple inbound or outbound edges
with the functions $L, P$ and $forbidden$. The last constraint that is to prevents the edge selections
that form a cycle is guarded with functions $FIRST$ and $LAST$. Both of these functions are defined
for all keyword indices. At the beginning of the algorithm \ref{ukk-h} $FIRST(i) = LAST(i) = i$
for every keyword index. The value of these functions is updated such that at any point of execution
if we have selected a set of edges that forms a path starting from $s_x$ and ending to $s_y$, then
$FIRST(y)=x$ and $LAST(x)=y$. Note that the values for the nodes in the middle of such paths are
not updated nor queried. Now, before the new edge $(s_i,s_j)$ is selected we check that the
selection does form a cycle by checking that $FIRST(i) \neq j$. Note that this is the same thing
than checking that $LAST(j) \neq i$. Only one of those checks is needed. If the check fails,
the edge in question must be discarded. The following pseudocode concludes the above discussion.


  \begin{algorithm}[h!]
    \caption{Selection of the edges} \label{ukk-h}
    \hspace*{\algorithmicindent} \textbf{Input:} The AC-machine including the auxiliary data from algorithm \ref{ukk-pre} \\
    \hspace*{\algorithmicindent} \textbf{Output:} The approximate longest Hamiltonian path in the overlap graph.
    \begin{algorithmic}[1]
      \Function{createPath}{input?}
        \For {$j = 1,...,m$}
          \If {$F(j)\neq 1$}
            \State $P(f(F(j)))\gets P(f(F(j)))\cdot {j}$
            \State $FIRST(j)\gets j$
            \State $LAST(j)\gets j$
          \Else
            \State $forbidden(j)\gets true$
          \EndIf
        \EndFor  
        \State $s\gets b(B)$
        \While {$s\neq 1$}
          \If {$P(s)$ is not empty}
            \For {each $j$ in $L(s)$ s.t. $forbidden(j) = false$}
              %% ERRATA: Check emptiness
              \If {$P(s)$ is empty}
                \State \textbf{break}
              \EndIf
              \State $i\gets$ the first element of $P(s)$
              \If {$FIRST(i) = j)$}
                \If {$P(s)$ has only one element}
                  \State \textbf{continue}
                \Else
                  \State $i\gets$ the second element of $P(s)$
                \EndIf
              \EndIf  
              \State $H\gets H\cdot \{(x_i,x_j)\}$
              \State $forbidden(j)\gets true$
              \State $P(s)\gets P(s) \backslash\{i\}$
              \State $FIRST(LAST(j))\gets FIRST(i)$
              \State $LAST(FIRST(i))\gets LAST(j)$
            \EndFor
          \State $P(f(s))\gets P(f(s))\cdot P(s)$
          \EndIf
          \State $s\gets b(s)$
        \EndWhile
      \EndFunction
    \end{algorithmic}
  \end{algorithm}


  

%% next algo time analysis

%% conclusion theorem.


  %Tänne bugikorjaukset sähköpostista joka lähetetty 30.4
  %Kommentoitu sanoin ERRATA:

\chapter{Relative Lempel-Ziv}

%  Lempel-Ziv dictionary construction.

%  subsections?

%%
\chapter{experiments}

  \section{Implementation}

  \section{Benchmark Data}
  %HW + instances

  \section{Results}

  \section{Discussion}

%
\chapter{Conclusions}
  

  \begin{enumerate}
  \item \citep{Aho75} describes the Aho-Corasick machine for the first time. It gives the pseudocode to creation and search.
  \item alanko dissertation, no bibtex yet. Discusses some things related to this topic.
  \item \citep{Alanko17} describes approx scs algorithm for compact space.
  \item statistics.pdf desccribes the dataset pizzachili.
  \item \citep{Ukkonen90} is the most important reference in this thesis. Describes the main scs algorithm.
  \item \citep{Tarhio88} Describes the same algorithm as ukkonen 90 but not in linear time.
  \end{enumerate}











%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\cleardoublepage                          %fixes the position of bibliography in bookmarks
\phantomsection
\addcontentsline{toc}{chapter}{\bibname}  % This lines adds the bibliography to the ToC
\printbibliography

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\backmatter

%
% TARVIINKO APPENDICES??
%
%

\begin{appendices}

%\input{instructions_english}
%% \input{instructions_finnish}

%% \appendix{Sample Appendix\label{appendix:model}}
%% usually starts on its own page, with the name and number of the appendix at the top. 
%% The appendices here are just models of the table of contents and the presentation. Each appendix
%% Each appendix is paginated separately.

%% In addition to complementing the main document, each appendix is also its own, independent entity.
%% This means that an appendix cannot be just an image or a piece of programming, but the appendix must explain its contents and meaning.

\end{appendices}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
