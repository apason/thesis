
%% TODO: todo todo todo

% texttt for every "special" word like filenames, types etc
% Should AC-machine be AC machine without the hyphen?
% SCS lyhenteen intro
% FSA lyhenteen intro
% algon method muotoon note
% esimerkit example environmenttiin
% määritelmät environmentiin
% lähteet
% proof scetch
% pseudon paikat
% lz kompressiosta 5 sivua
% number in a ball ?
% sigma ac:n määritelmään

% The implementation is n log sigma. We refer n log n everywhere!

% REDUCED SET !?!?!

% TERMIT
% HamPath: vertex, edge
% goto graph (illustration): (root)node, edge?
% ACM (start) state, goto and failure transitions

% represented by string... this is shit. Just state somewhere that states and strings
% are sometimes referred interchangeably: By slight abuse of notation...

% STRUCTURE!

% Formal definition for forbidden, first, last ?

%% Regarding motivation: be sure to cite the paper by Alanko and Norri where they specifically state that it would be great to have an efficient implementation of Ukkonen's algorithm (I think they say this toward the end of the paper).
%% Regarding RLZ dictionary construction: I have put a couple of papers below (there are more, but these will do to start). Using SSS would be a new approach to that problem.
%% https://rdcu.be/ctUES
%% https://dl.acm.org/doi/pdf/10.1145/2872427.2883042


%% SALMELAN PAPERI
%% L. Salmela, J. Tarhio, and J. Kytöjoki: Multi-pattern string matching with
%% q-grams. ACM Journal of Experimental Algorithmics, Volume 11, 2006.

% PUGLISIN ekoista soposteista.. ? ei varmaan olla tekemässä. 
%4. No requirements yet - you're in control. ASCII with one string per line sounds fine to me while you're developing the basic approach. Remember we talked about strings on large alphabets at some point? For those each symbol will be a fixed width (16 or 32 bits) and all the same length, so we can define a custom binary file format (something like: 32-bit int telling number of strings, 32-bit int telling length of each string, 32-bit int telling width of each symbol, then the strings themselves ....).

% PUGLISIN sähköpostista:
%Thanks for the results --- they make some sense to me too (at first glance). The duplicate removal method you suggest (using quicksort) is fine. By the way, it would be interesting to know the number of duplicates for each test case (to determine the amount of compression that comes from duplicates and the amount from the main SSS algorithm).


%Regarding RLZ dictionary construction: I have put a couple of papers below (there are more, but these will do to start). Using SSS would be a new approach to that problem.
%https://rdcu.be/ctUES
%https://dl.acm.org/doi/pdf/10.1145/2872427.2883042



% Modifief filename tarkottaa EI rivinvaihtoja!?


% miten RLZ tulokset saatu?
% -Sämplätään alkuperäinen file esim cere parametreilla 15_128
%     - Tämä on valmiiksi tehty löytyy ukkonen-90/data/ hakemistosta.
% -Ajetaan scs josta saadaan .superstring tiedosto ja compressio
% -Ajetaan sama file josta tehtiin scs parametreilla cere, 15_128, compressio
%    - ei taida toimia. pitää samplata alkuperäsestä tiedostosta compressio/x parametrilla?
% -Nyt on kaksi samankokoista referenssiä. Toinen on scs ja toinen ei
% HUOM! .ref file sisältää rivinvaihdot !?

% -Lasketaan indeksien määrät tiedostoa cere vasten molemmilla referensseillä



%% History:
%% December 2020 Veli Mäkinen removed obsolete options related to 40 cr theses
%% May 2019 Tomi Männistö, Antti-Pekka Tuovinen proofreading; 30 vs. 40 cr theses, etc.
%% May 2019 Tomi Männistö changed from babelbib to bibtex; Abstract page (and other pages as well) reformatting.
%% January–May 2019 several issues fixed by Niko Mäkitalo; long fields in abstract
%% March 2018 template file extended by Lea Kutvonen to exploit HYthesisML.cls.
%% Feb2018 This template file for the use of HYgraduML.cls was  modified by Veli Mäkinen from HY_fysiikka_LuKtemplate.tex
%% authored by Roope Halonen ja Tomi Vainio in 2017.
%% Some text is also inherited from engl_malli.tex versions by Kutvonen, Erkiö, Mäkelä, Verkamo, Kurhila, and
%% Nykänen, to accompany tktltiki.cls (by Puolakka 2002).


%% Follow comments to support use.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% STEP 1: Choose options for MSc / BSc / seminar layout and your bibliographic style
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%  Language: 
%%      finnish, swedish, or english
%%  Pagination (use twoside by default)  
%%      oneside or twoside,
%%  Study programme / kind of report
%%      csm  = Master's thesis in Computer Science Master's Programme;
%%      tkt = Bachelor's thesis in Computer Science Bachelor's Programme;
%%      seminar = seminar report
%%  For Master's thesis choose your line or track:
%%      (30 cr thesis, 2020 onwards, Master's Programme in Computer Science = csm)
%%      software-track-2020 = Software study track
%%      algorithms-track-2020 = Algorithms study track
%%      networking-track-2020 = Networking study track
%%
%%      (30 cr thesis, Master's Programme in Computer Science = csm)
%%      sw-track-2018 = Software Systems study track
%%      alko-track-2018 = Algorithms study track
%%      nodes-track-2018 = Networking and Services study track
%%
%%      (30 cr thesis, Master's Programme in Computer Science = csm)
%%      sw-line-2017 =  Software systems subprogramme
%%      alko-line-2017 = Algorithms, Data Analytics and Machine Learning subprogramme
%%      bio-line-2017 = Algorithmic Bioinformatics subprogramme
%%      nodes-line-2017 = Networking and Services subprogramme
%%

\documentclass[english,twoside,censored,csm,algorithms-track-2020]{HYthesisML}


% In theses, open new chapters only at right page.
% For other types of documents, may ask "openany" in document.
\PassOptionsToClass{openright,twoside,a4paper}{report}
%\PassOptionsToClass{openany,twoside,a4paper}{report}

\usepackage{csquotes}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% REFERENCES
%% Some notes on bibliography usage and options:
%% natbib -> you can use, e.g., \citep{} or \parencite{} for (Einstein, 1905); with APA \cite -> Einstein, 1905 without ()
%% maxcitenames=2 -> only 2 author names in text citations, if more -> et al. is used
%% maxbibnames=99 as no great need to suppress the biliography list in a thesis
%% for more information see biblatex package documentation, e.g., from https://ctan.org/pkg/biblatex 

%% Reference style: select one 
%% for APA = Harvard style = authoryear -> (Einstein, 1905) use:
\usepackage[style=authoryear,bibstyle=authoryear,backend=biber,natbib=true,maxnames=99,maxcitenames=2,giveninits=true,uniquename=init]{biblatex}
%% for numeric = Vancouver style -> [1] use:
%\usepackage[style=numeric,bibstyle=numeric,backend=biber,natbib=true,maxbibnames=99,giveninits=true,uniquename=init]{biblatex}
%% for alpahbetic -> [Ein05] use:
%\usepackage[style=alphabetic,bibstyle=alphabetic,backend=biber,natbib=true,maxbibnames=99,giveninits=true,uniquename=init]{biblatex}
%

\addbibresource{bibliography.bib}
% in case you want the final delimiter between authors & -> (Einstein & Zweistein, 1905) 
% \renewcommand{\finalnamedelim}{ \& }
% List the authors in the Bibilipgraphy as Lastname F, Familyname G,
\DeclareNameAlias{sortname}{family-given}
% remove the punctuation between author names in Bibliography 
%\renewcommand{\revsdnamepunct}{ }


%% Block of definitions for fonts and packages for picture management.
%% In some systems, the figure packages may not be happy together.
%% Choose the ones you need.

%\usepackage[utf8]{inputenc} % For UTF8 support, in some systems. Use UTF8 when saving your file.

\usepackage{enumerate}
\usepackage{lmodern}         % Font package, again in some systems.
\usepackage{textcomp}        % Package for special symbols
\usepackage[pdftex]{color, graphicx} % For pdf output and jpg/png graphics
\usepackage{epsfig}
%\usepackage{subfigure}
\usepackage[pdftex, plainpages=false]{hyperref} % For hyperlinks and pdf metadata
\usepackage{fancyhdr}        % For nicer page headers
\usepackage{tikz}            % For making vector graphics (hard to learn but powerful)
\usepackage{wrapfig}        % For nice text-wrapping figures (use at own discretion)
% Tikz stuff for apas graphs
\usetikzlibrary{positioning,arrows,shapes,automata}
\tikzstyle{gnode} = [circle,minimum size=1cm,fill=blue!20,draw]
\tikzstyle{line} = [line width=0.5,draw]
\tikzstyle{shaded} = [color=blue!30,line width=2mm,draw]
\tikzstyle{shadedf} = [color=blue!25,line width=1.7mm,draw]
\tikzstyle{tnode} = [minimum size=0.67cm]
\tikzstyle{enode} = [minimum size=0.01cm]
\tikzstyle{dotted} = [dashed, line width=0.4, draw]

\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
    \node[shape=circle,draw,inner sep=2pt] (char) {#1};}}

\usepackage{pgfplots}
\usepackage{filecontents}

\usepackage{amsmath, amssymb, amsthm, amsfonts, hyperref} % For better math

\usepackage[noend]{algpseudocode} % for pseudocode: provides algorithmic environment
\usepackage{algorithm}     % for pseudocode: enables captioning

\usepackage{tabularx}
\usepackage{makecell}

\usepackage{minted}

\usepackage{afterpage}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[chapter]

\theoremstyle{definition}
%% \newtheorem{definition}[equation]{Definition}

\makeatletter
\let\c@equation\c@figure
\makeatother
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{example}[theorem]{Example}
\newtheorem{definition}[theorem]{Definition}

\usepackage[pdftex]{graphicx}
%%\usepackage{subfigure}
\usepackage{float}


\singlespacing               %line spacing options; normally use single

\fussy
%\sloppy                      % sloppy and fussy commands can be used to avoid overlong text lines
% if you want to see which lines are too long or have too little stuff, comment out the following lines
% \overfullrule=1mm
% to see more info in the detailed log about under/overfull boxes...
% \showboxbreadth=50 
% \showboxdepth=50



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% STEP 2:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Set up personal information for the title page and the abstract form.
%% Replace parameters with your information.
\title{Implementation and benchmarking of Ukkonen 1990 -algorithm}

% TM: Contributors to template editors now listed in the beginning of the file in comments
\author{Arttu Kilpinen}
\date{\today}



% Set supervisors and examiners, use the titles according to the thesis language
% Prof. 
% Dr. or in Finnish toht. or tri or FT, TkT, Ph.D. or in Swedish... 
\supervisors{Assoc Prof.~Simon Puglisi}
\examiners{Prof.~Dunno yet}


\keywords{Implementation, Shortest Common Superstring}
\additionalinformation{\translate{\track}}

%% For seminar reports:
%%\additionalinformation{Name of the seminar}

%% Replace classification terms with the ones that match your work. ACM
%% ACM Digital library provides a taxonomy and a tool for classification
%% in computer science. Use 1-3 paths, and use right arrows between the
%% about three levels in the path; each path requires a new line.

\classification{\protect{\ \\
\  Theory of Computation $\rightarrow$ Design and Analysis of Algorithms  $\rightarrow$ Data Structures Design and Analysis $\rightarrow$ Data Compression  \\
\  Theory of Computation $\rightarrow$ Design and Analysis of Algorithms  $\rightarrow$ Data Structures Design and Analysis $\rightarrow$ Pattern Matching \\
\  Theory of Computation $\rightarrow$ Design and Analysis of Algorithms  $\rightarrow$ Data Structures Design and Analysis $\rightarrow$ Sorting and Searching
}}

%% if you want to quote someone special. You can comment this line out and there will be nothing on the document.
%\quoting{Bachelor's degrees make pretty good placemats if you get them laminated.}{Jeph Jacques}


%% OPTIONAL STEP: Set up properties and metadata for the pdf file that pdfLaTeX makes.
%% Your name, work title, and keywords are recommended.
\hypersetup{
    unicode=true,           % to show non-Latin characters in Acrobat’s bookmarks
    pdftoolbar=true,        % show Acrobat’s toolbar?
    pdfmenubar=true,        % show Acrobat’s menu?
    pdffitwindow=false,     % window fit to page when opened
    pdfstartview={FitH},    % fits the width of the page to the window
    pdftitle={},            % title
    pdfauthor={},           % author
    pdfsubject={},          % subject of the document
    pdfcreator={},          % creator of the document
    pdfproducer={pdfLaTeX}, % producer of the document
    pdfkeywords={something} {something else}, % list of keywords for
    pdfnewwindow=true,      % links in new window
    colorlinks=true,        % false: boxed links; true: colored links
    linkcolor=black,        % color of internal links
    citecolor=black,        % color of links to bibliography
    filecolor=magenta,      % color of file links
    urlcolor=cyan           % color of external links
}

%%-----------------------------------------------------------------------------------

\begin{document}

% Generate title page.
\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% STEP 3:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Write your abstract to be positioned here.
%% You can make several abstract pages (if you want it in different languages),
%% but you should also then redefine some of the above parameters in the proper
%% language as well, in between the abstract definitions.

\begin{abstract}

  Abstract here. Last thing to write

\end{abstract}

% Place ToC
\newpage
\mytableofcontents
\mainmatter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% STEP 4: Write the thesis.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Your actual text starts here. You shouldn't mess with the code above the line except
%% to change the parameters. Removing the abstract and ToC commands will mess up stuff.
%%
%% You may wish to include material to avoid browsing the definitions
%% above. Command \include{file} includes the file of name file.tex.
%% As a side effect, subsequent inclusions may force a page break.

% BSc instructions
%\include{bsc_finnish_contents}
%\include{bsc_english_contents}
% MSc instructions
%\include{msc_finnish_contents}
%\include{msc_english_contents}


%% Puglisin kommentteja:
%% - Experiments section saattaa paisua aika paljon. Voisi mahdollisesti jakaa useaan kappaleeseen. Esim implementation kohtaa voisi laittaa myös ukkonen kappaleeseen.

%% Intro ja Conclusions luonnollisella kielellä. Muualla teknistä kamaa.

%% ei kappalejakoa introon. sectionit pois.
\chapter{Introduction}~\label{chp-intro}
%  vitusti viitteitä ja pelkkää LUONNOLLISTA kieltä. Ei esim määritelmiä!

% tänne vois katella rlz papereista introja ja miettiä jos saisi hyviä lainauksia tai viitteitä

\section{Motivation}
  %% - scs yleisesti datan pakkaamiseen
  %% - molekyylibiologia sekvensointi vaikeaa pitkälle pätkälle
  %%   - molekyylin pilkkominen random paloiksi, sekvensointi ja superstringin ottaminen.
  %%     saadaan approksimaatio alkuperäisestä pitkästä sekvenssistä
  %%   - toimii käutännössä hyvin \citep{Peltola83}
  

  %% introssa motivaatiossa rlz mainittu mutta myöhemmin teknisesti
  
  %% alanko or someone mentionet thet it would be interesting to see an implementation

  \section{Related Work}

  %% O(nm) based algorithm based on KnuthMorrisPratt \citep{Knuth77} \citep{Tarhio88}
  %% perustuu kaikkien overlap parien laskemiseen kmp:llä.

  %% hae jostain tietokannasta approx scs algoja !

  %% related workista oma kappale jos muita vertailuja kuin alanko ja norri. muuten ehkä introon.
  
  \section{Structure of the Thesis}

%%
\chapter {Shortest Common Superstring (preli)}~\label{chp-preli}


%% preliminaries including syntax

%% what is an approximation algorithm

%% Maininta aakkostosta?

%% Tätä voi ehkä jakaa sectioneihin?

%% The algorithm described in this thesis \\[1in]


TODO:
In this chapter we define the concepts used in the thesis. These include the concepts related to
strings as well as the concept of approximation algorithms and state automata. The definition of
linear time complexity is also recalled. This chapter contains the explanations in natural
languages following a corresponding formal definitions. First we define the concepts related
to strings followed by ...  \\[1in]



Regardless of the alphabet a string is simply a finite sequence of symbols. The length of the string equals the number of symbols it contains. 

\begin{definition}[\textsc{String}]~\label{def-string}
  
  For an alphabet $\Sigma$ a string $S$ is a finite sequence of symbols  ${s_1\cdots s_n}$ where each symbol $s_i \in \Sigma$.
  The length of the string $S$ denoted by $|S| = n$ is the number of symbols in the string.
  An empty string denoted by $\epsilon$ contains zero symbols.
  
\end{definition}

If a string $S$ contains another string $S'$ we say that the string $S'$ is a substring of a string $S$ and string $S$ is a superstring of $S'$.
By containing a string we mean that there is a sequence of symbols in $S$ that spells out $S'$

\begin{definition}[\textsc{Substring and Superstring}]~\label{def-sub-super}
  Let there be two strings $S$ and $S'$.
  $S'$ is a substring of $S$ if and only if $S$ can be written as $RS'T$ where $R$ and $T$ are strings. $R$ and $T$ may be empty.
  If $R$ and $T$ are both empty then $S = S'$ which is a substring of itself.
  An empty string is trivially a substring of every other string.\\

  If $S'$ if a substring of $S$ then $S$ is a superstring of $S'$
\end{definition}

If a string contains (I.e. is a superstring) a set of strings, it is said to be a common superstring of the set.

\begin{definition}[\textsc{Common superstring}]~\label{def-cs}

  Let $R = \{S1,...,Sn\}$ be a set of strings and $T$ be a string.
  $T$ is a common superstring of $R$ if and only if $T$ is a superstring for each $Si$ such that $i\in 1,...,n$.
  
\end{definition}

Trivially a common superstring of a set of strings can be constructed by catenating all of the strings of the set together.
This kind of string has a length that equals the lengths of all the substrings added together.

In many cases there exists one or more common superstrings that are shorter than the common superstring produced by this naive methot.

(if there is no overlaps then the naive method yields the csc ... ref ??)

If a common superstring is a shortest possible it is said to be a shortest common superstring. Finding a shortest common superstring is
an \textsc{NP}-hard problem. 

\begin{definition}[\textsc{Shortest Common Superstring}]~\label{def-scs}

  Let string $T$ be a common superstring of a set of strings $K$.
  $T$ is a shortest common superstring of $K$ if and only if there does not exist a common superstring of $K$ which is shorter than $T$.
  
\end{definition}

% Tarhio88
%Since the decision version of this problem is NP-complete... \citep{Garey79}


%-Lemma: Jos x on y:n substring s.t. y kuuluu R niin scs(R) == scs(Rux)
%todistus ???

%definition: reduced set of strings = joukko stringejä joissa mikään string ei ole toisen substring.

%definition: overlap note this is not symmetric!
%maximal overlap.


%%
\chapter{Ukkonen's Algorithm}~\label{chp-ukkonen}

In this chapter we describe Ukkonen's linear time approximation algorithm \citep{Ukkonen90} for
\textsc{Shortest Common Superstring} problem in detail. First we define the heuristic that the
algorithm uses in terms of strings. Then we show that the problem can be reduced to the problem
of finding a longest Hamiltonian path in a specific overlap graph that encodes the same SCS problem.
After the problems are proven to be equivalent, we define the same heuristic for the reduced problem.
The equivalence of the two approaches are necessary since the bounds of the achieved compression
are proven using the \textsc{Longest Hamiltonian Path} problem. Conceptually, Ukkonen's algorithm 
solves the SCS problem by constructing the Hamiltonian path.

Further, we define the Aho-Corasick (AC for short) machine, as it is used in the algorithm for finding
the maximal overlaps between the set of input keywords. The time complexity of the construction of
the AC-machine is also discussed. We also prove that the AC-machine can be used to find the maximal
pairwise overlaps between every input keyword.

This chapter contains four pseudocode blocks which essentially form the algorithm as whole.
For each of those algorithms, we first discuss their principles on a higher level. Then the pseudocode
is presented and discussed in more detail. Finally, we prove their time complexities.
The algorithms represented here are slightly changed and contain few corrections from the form they
were originally presented. Each such modification is noted to emphasise the differences between
the algorithms presented here versus the original ones.

\listofalgorithms

\section{The Greedy Heuristic}

%% Introtaan overlap graph
%% demotaan että tästä selvästi saadaan superstringi
%% nyt pitää vielä näyttää että tällä tavalla (overlap graphista saatu) lyhyin superstringi on se the scs!
%%   tämän todistamiseksi tarvitaan
%%   - näytetään että decomposition on sama
%%   - parittaiset decompositionit on maksimaalisia
  
%% Note to Simon: The purpose of this chapter is to introduce the greedy heuristic using strings. Then
%% we show that the analogy between the SCS and longest Hampath problem and define the heuristics using hampath. The point of proving the problems are equivalent is that Tarhio and Ukkonen used it to prove the
%% correctness and the bound (approx scs >= 1/2 scs). 
%% I think this whole section is poorly written and it is hard to follow. Maybe there is something missing
%% and it is not even correct. Any ideas how to improve the reaability and emphasise the important things?

Let there be a set of strings $R=\{s_1,...,s_m\}$. A naive method for constructing a common superstring
is to concatenate all the strings in the set. For the set $R$, the resulting superstring
is $s_1s_2\cdots s_m$.
The following greedy heuristic can be used to construct an approximation of a shortest
common superstring.

\begin{enumerate}
\item Examine the set and remove two strings $s_i$ and $s_j$ that have a longest overlap among all pairs of strings $(s_i, s_j)$ with $i\neq j$. Note that a longest overlap may be an empty string. If there are multiple pairs of strings with a longest overlap, the decision can be made arbitrarily.
\item Merge the strings together such that they are maximally overlapped to form a shortest common superstring of the set $\{s_i, s_j\}$.
  \item Add the new string back to the set $R$ and repeat until there is only one string left.
\end{enumerate}

It turns out that the \textsc{Shortest Common Superstring} problem for a reduced set or strings is
analogous with a special case of
the \textsc{Longest Hamiltonian Path} problem, that is, every instance of the SCS problem can be encoded
as an instance of a longesta Hamiltonian path problem. The encoding works by creating a weighted
complete digraph from the set of strings as follows. 
The graph $G_R=(V_R,E_R,w)$, where $V_R$ is a set of vertices, $E_R$ is a set of edges and $w$ is a
 weight function defined for each edge in $E_R$, is constructed from the set of strings $R$.
Let there be a vertex $v_i=s_i$ for each string in $R$. Moreover, we define the start vertex
$v_{start}$ and the end vertex $v_{end}$. Each vertex $v_i$ has a directed weighted edge to each vertex
$v_j$ such that $i\neq j$. The start vertex has an edge to each $v_i$ and each $v_i$ has an edge to the
end vertex. The weight $w(v_{start}, v_i) = 0$ and the weight $w(v_i, v_{end}) = 0$ for all $i=1,...,m$.
The weight of every other edge $(v_i, v_j)$ is the length of the maximum overlap between $s_i$ and $s_j$.
More formally,

% Pitäskö tossa nyt kuitenkin olla: vi = si \in R ????
$G_R=(V_R,E_R,w)$ with
\begin{align*}
  V_R &= \{v_i~|~s_i\in R\} \cup \{v_{start},v_{end}\} \\
  E_R &= \{(v_i,v_j)~|~i=1,...,m, j=1,...,m, i\neq j\}
\end{align*}
\[   
w((x,y)) = 
     \begin{cases}
       \text{length of the longest overlap between } x \text{ and } y &\quad\text{ when } x\in R \text{ and } y\in R  \\
       0 &\quad\text{ when } x=v_{start} \text{ and } y\in R \\
       0 &\quad\text{ when } x\in R \text{ and } y=v_{end} \\
     \end{cases}
     \]




%-intro hamiltonian pathiin!
%-special case of shortest hampath in weighted directed graph.
%- graafin nodet keywordeja SEKÄ alku ja loppunode
%- graafin kaarten painot maksimaalisia overlappeja

Since the subgraph consisting of vertices $v_i, i=1,...,m$ is a complete graph, there certainly exists a
Hamiltonian path(s).\citep{asd} Given any
of the Hamiltonian paths $H$ from the graph $G_R$, we can construct a common superstring of $R$ by
overlapping the strings representing the nodes in the same order they appear on the path in the
following way.

Let us take the strings in the same order as their corresponding nodes appear in the path $H$. Put the
strings above each other such that they are maximally overlapped. The projection $p(H)$ which describes
the strings that are maximally overlapped (i.e. the overlaps are not duplicated) clearly results in a
common superstring of $R$. Depending on the Hamiltonian path $H$ the overlaps in the
projection $p(H)$ have different lengths, hence the projection changes depending on the path $H$. 

\begin{figure}
\centering
\begin{tikzpicture}
  
  \node[enode] (H1) at (0,0) {};
  \node[enode] (H2) at (14,0) {};
  \path [|-|] (H1) edge node[below] {p(H)} (H2);

  \node[enode, above = 0.5cm of H1] (T1a) {};
  \node[enode, above right = 0.5cm and 3.3cm of H1] (T1b) {};
  \path[|-|] (T1a) edge (T1b);
  \node[enode, right = 0cm of T1b] (T1txt) {$t_1$};

  \node[enode, above right = 0.5cm and 1.2cm of T1a] (T2a) {};
  \node[enode, right = 2.6cm of T2a] (T2b) {};
  \path[|-|] (T2a) edge (T2b);
  \node[enode, right = 0cm of T2b] (T2txt) {$t_2$};
  
  \node[enode, above right = 0.5cm and 2cm of T2a] (T3a) {};
  \node[enode, right = 2.6cm of T3a] (T3b) {};
  \path[|-|] (T3a) edge (T3b);
  \node[enode, right = 0cm of T3b] (T3txt) {$t_3$};

  \node[tnode, rotate = 40, above = 0.2cm of T3b] (dots1) {...};

  \node[enode, above right = 1cm and 2.2cm of T3a] (Tia) {};
  \node[enode, right = 2.6cm of Tia] (Tib) {};
  \path[|-|] (Tia) edge (Tib);
  \node[enode, right = 0cm of Tib] (Titxt) {$t_i$};

  \node[enode, above right = 0.5cm and 1.2cm of Tia] (Ti1a) {};
  \node[enode, right = 2.6cm of Ti1a] (Ti1b) {};
  \path[|-|] (Ti1a) edge (Ti1b);
  \node[enode, right = 0cm of Ti1b] (Titxt) {$t_{i+1}$};

  \node[enode, above = 0.1cm of Tia] (zc) {};
  \node[enode, below = 0.1cm of Ti1a] (za) {};
  \node[enode, above = 0.1cm of Tib] (zb) {};

  \path[|-|, shorten >= -0.3cm] (zc) edge node[right = -0.1cm,fill=white,align=center] {$u_i$} (za);
  \path[|-|] (za) edge node[fill=white,align=center] {$v_i$} (zb);

  \node[tnode, rotate = 35, above = 0.2cm of Ti1b] (dots2) {...};

  \node[enode, above right = 1cm and 2.2cm of Ti1a] (Tma) {};
  \node[enode, right = 3.455cm of Tma] (Tmb) {};
  \path[|-|] (Tma) edge (Tmb);
  \node[enode, right = 0cm of Tmb] (Titxt) {$t_m$};  
  
\end{tikzpicture}
\caption{The projection $p(H)$ of the set of strings $\{s_1,...,s_m\}$} \label{fig-projection}
\end{figure}

% INTORO, VÄITE, PERUSTELU, KONKLUSIO

% PAPERISSA TÄÄ KOKO PASKA ON REDUCED. eli käydään ensin reduced läpi ja tehdään tämä sille.. ?

Figure \ref{fig-projection} shows an example of the projection constructed from a
Hamiltonian path $H$ in an overlap graph. The corresponding set of
strings consists of $s_1$ to $s_m$.
The strings $t_1$ to $t_m$ are the same strings in the order they appear on $H$.
We can represent the string $t_i, i=1,...,m-1$ as a prefix-suffix pair
$(u_i,v_i)$ where $v_i$ is the suffix of $t_i$ that is also a prefix of $t_{i+1}$ such that the
overlap between $t_i$ and $t_{i+1}$ is maximal. The prefix of $t_i$ whose length is $|t_i|-|v_i|$ is
$u_i$. I.e. $u_i$ is the part of $t_i$ that can not be overlapped with $t_{i+1}$. Using this
partitioning we can write the projection $p(H)$ as $u_1u_2\cdots u_{m-1}t_m$.

Since the maximal overlaps $v_i,i=1,...,m-1$ appear in the projection only once, the length of the
projection is $(|s_1|+...+|s_m|) - (|v_1|+...+|v_{m-1}|)$. The length of the maximal overlaps
are equivalent to the weights of the overlap graph $G_R$, hence $v_1+\cdots +v_{m-1} = |H|$. As the size
of $|H|$ grows, the size of the projection gets shorter.

%That means the shortest common superstring of $R$ corresponds with the longest Hamiltonian path in $G_R$.
% ^ This must be proven!

With this analogy the size of $H$ is the compression of the common superstring,
and the length of $p(H)$ is the length of the common superstring.

We now know that the Hamiltonian path in the overlap graph can be used to form a common superstring
of the set of strings. Next, we show that also a shortest common superstring of $R$ has a
corresponding Hamiltonian path. To prove this we introduce the following lemma.


\begin{lemma}[]~\label{lem-composition}
The compositions of $p(H)$ and a shortest common superstring $z$ for a reduced set of strings are equivalent.
\end{lemma}
\begin{proof}[Proof (by construction)]
  Let there be a reduced set of strings $R=\{r_1,r_2,...,r_m\}$ and
  a shortest common superstring $z$ for $R$. The indices of the strings $r_1$ to $r_m$ are in the same
  order they appear in $z$. Let $y_i$ be the longest suffix of $r_i$ such that it is also a prefix
  of $r_{i+1}$, i.e there is an overlap $y_i$ between $r_i$ and $R_{i+1}$. Additionally, let us
  define $x_i$ to be the prefix of $r_i$ that does not overlap with $y_i$. By this construction we
  see that the composition of any SCS for any reduced set of strings is equivalent with the composition
  of the superstring acquired via the projection from the Hamiltonian path.

  Hence, there exists a
  Hamiltonian path and the corresponding projection for any SCS.
\end{proof}

\begin{theorem} [] ~\label{lem-similarity-between-SCS-Hamp}
  The projection $p(H)$ for the longest Hamiltonian path in the overlap graph is equivalent to
  a shortest common superstring for a reduced $R$.
\end{theorem}
\begin{proof}[Proof scketch]
By Lemma \ref{lem-composition} there exists a decomposition of the SCS $z$ that corresponds to a
Hamiltonian path in an overlap graph of a reduced set of strings.
\end{proof}

The following defines a conceptually equivalent greedy heuristic for the SCS problem in the terms of
a Hamiltonian path in the overlap graph.

To  construct an approximate longest Hamiltonian path select an edge $e$ from the set $E_R$ such that

\begin{enumerate}
\item The edge $e$ has the largest weight. If there are multiple such edges, the decision can be made arbitrarily.
\item The edge $e$ has not been selected before.
\item It is possible to construct a Hamiltonian path with $e$ and the already selected edges. 
\end{enumerate}

The constraint 3 indicates the following:
\begin{enumerate}[i]
\item The selection does not form a cycle.
\item The earlier selected edges can not have the same start node with $e$.
\item The earlier selected edges can not have the same end node with $e$.
\end{enumerate}

%  ongelma ahneessa heuristiikassa on että voi tehdä hyvän valinnan joka pakottaa myöhemmin huonoihin valintoihin.

%  CONCEPTUALLY the ukkonen algo does just that so the proofs apply.

%ennen tätä maininta approksimaatioalgosta. koska np niin käytetään approksimaatiota ja tätä heuristiikkaa

%-määrittele kompressio

%-scsn kompressio on vähintään puolet optimaalisesta \citep{Tarhio88}

\begin{theorem}[]~\label{theorem-heuristic-bound}
  Let $H'$ be the approximate longest Hamiltonian path in the overlap graph created using
  the greedy heuristic and let $H$ be the the actual longest Hamiltonian path. Now,
  
  $|H'|\geq \frac{1}{2}|H|$
  
\end{theorem}

For the proof we refer the reader to \citep{Tarhio88}.

In other words, the compression achieved by the greedy heuristic is at least half of the optimal
compression. 

%  approx factor (ei välttämättä epsilon mutta jotkut boundit on olemassa ainakin kompressiolle)



\section{Aco-Corasick Machine}

%  - mitä tää paska oli alunperin: Laajennettu kmp
%  - äärellinen automaatti poikkeuksin (failure)
%  - pattern matching machine

% Ei yhden rivin kappaleita!  
  
The Aho-Corasick (AC for short) machine is a finite state automaton like data structure that was
developed to be used
in a pattern matching program to find occurrences of the given keywords in a text string \citep{Aho75}.
The input of such a pattern matching program contains a finite set of strings called keywords and an
arbitrary string called a text string. The output of the program is a list of locations of every
occurrence of every keyword in a text. The AC-machine data structure and the associated pattern
matching program were first described in \citep{Aho75}.

todo: similarity with kmp ?

% Tulee fiilis etten tiedä mistä puhun. Kerro vähän enemmän tähän kappaleeseen  
As the string searching part of \citep{Aho75} is not used in \citep{Ukkonen90} nor is it relevant in
this thesis, we only discuss the AC-machine as a data structure and omit the string searching
functionality of the original author.

Formally, AC-machine is defined\footnote{\citep{Aho75} also defined an output function.
This is needed only when the AC-machine is used for searching strings.} as follows.

\begin{definition}
  Aho-Corasick machine is a tuple $(Q, \Sigma, g, f)$, where
  \begin{enumerate}
    \item $Q$ is a set of states which contains at least the start state, \\
    \item $\Sigma$ is the alphabet.
    \item $g : Q \times \Sigma \rightarrow Q, g(s_s,c) = s_d$ is a function that defines a state transition from a state $s_s$ to a state $s_d$ for a symbol $c$, \\
    \item $f : Q \rightarrow Q, f(s_s) = s_d$, is a function that defines a failure transition from state $s_s$ to state $s_d$,
  \end{enumerate}
\end{definition}
todo: should the following lines be included in the definition?

If there is a goto transition from $s_s$ to $s_d$ in an AC-machine we say that the state $s_s$ is parent
of the state $s_d$ and that the state $s_d$ is a child of the state $s_s$. A depth of a state $q$ is the
minimum number of state transitions that is needed to get $q$ from the start state. 
We use a term failure path for a continuous sequence of failure transitions starting from a leaf state.

When the AC-machine is used in a text search program,
the goto function defines the state transition from start state $s_s$ to the destination state
$s_d$ with the current input symbol $c\in \Sigma$ of the state automaton. If there is no such
goto transition
the failure function $f$ is queried and the failure transition from the start state $s_s$ to
the destination state $s_d$ occurs. The alphabet $\Sigma$ contains every symbol in the set of
keywords (and in case of a string search program, also every string in the processed text string)
encoded by the AC-machine.

To construct the AC-machine for the set of keywords the \textsc{FSA} like goto function is first
created. The goto function is then used to construct the failure function which completes the
construction of the AC-machine. The goto function is similar with a child function of a trie
i.e it describes a rooted tree that stores a set of keywords such that edges are labeled with the
symbols of the alphabet and the edge label is different between each sibling nodes. We say that a node
spells out a string that is formed by concatenating the edge labels from root to the node.
The failure function describes chain of state transitions in a case when the goto function is not defined
for the current input symbol. The failure function is queried as long as the goto function is not defined.
In the AC-machine the goto function is defined for every symbol in the alphabet in the start state.
Therefore the failure function is not defined for the start state. Other states, whether the
goto function is fully defined or not, always have a defined failure function value.
Algorithms \ref{ac-goto} and \ref{ac-fail} are used as complete definitons of the goto and
failure functions. 

The AC-machine can be illustrated as a graph where each state is represented as a node and each
goto transition is represented as an labeled edge. Failure transitions can also be drawn. We call
such an illustration a goto graph of an AC-machine. In a goto graph the start state is represented by
a root node.

Figure \ref{fig-ac} illustrates an example of the AC-machine constructed from the set of
keywords $K=\{$``baa'', ``baba'', ``abab'', ``aab''$\}$. The solid lines describe the goto function
and the dashed lines describe the failure function, for example, $g(8, a) = 9$ and $f(4)=11$.

%  Can produce "aababaa" (scs with compression 14-7=7) or "baababab" with compression 14-8 = 6.

\begin{algorithm}[h!]
  
  \caption{Construction of the goto function} \label{ac-goto}
  % TODO: Kirjoita omin sanoin input, output, method.
  \hspace*{\algorithmicindent} \textbf{Input:} Set of keywords $K = \{s_1,s_2...,s_k\}$.\\
  \hspace*{\algorithmicindent} \textbf{Output:} Goto function $g$ that is initially undefined for all states\\
  \hspace*{\algorithmicindent} \textbf{Method:} The index of the root node is 1. The procedure \textit{insert($s$)} inserts into the goto graph a path that spells out $s$ or does nothing if there already exists such a path.
  
      \begin{algorithmic}[1]

        \Function{calculateGotoFunction}{$K = \{s_1,s_2...,s_k\}$}
          \State $newstate\gets 1$
          \For{$i \gets 1$ until $k$}
            \State $INSERT(s_i)$
          \EndFor
          \For{$a$ s.t. $g(1,a)$ is not defined }
            \State $g(1,a)\gets 1$
          \EndFor
        \EndFunction

      \item[]

        \Function{insert}{$s = (a_0,a_1,...,a_{m-1})$}
          \State $state\gets 1$
          \State $j\gets 0$
          \While{$g(state, a_j)$ is defined}
            \State $state\gets g(state,a_j)$
            \State $j\gets j+1$
          \EndWhile
          \For{$p\gets j$ until $m-1$}
            \State $newstate\gets newstate+1$
            \State $g(state,a_p)\gets newstate$
            \State $state\gets newstate$
          \EndFor
        \EndFunction
      \end{algorithmic}
      
  \end{algorithm}

  Algorithm 1 creates a goto function. It begins by creating the root node on line 2.
  The algorithm continues by looping over the set of keywords and inserting them to the
  goto function on lines 3 to 5. At the end of the main function on lines 7 to 8
  the goto function is defined for all $g(1,\sigma)$ that were undefined creating a loop within
  the root node. This ensures that the failure function of the root node is never used when the
  AC-machine is used for searching strings.

  The procedure \textsc{INSERT} inserts a single keyword to the goto graph. The idea in the lines
  13 to 16 is to skip the prefix of $s$ that already exists in the trie. The variable $j$ initialized
  in the line 12 keeps track of this prefix. After the first symbol not already represented in the
  trie is found the new branch is created and the remaining suffix of $s$ is inserted symbol by symbol
  at lines 17-20.

  \begin{example}[]~\label{exmp-goto}
  Let us simulate this algorithm with a set of keywords $K=\{$``baa'', ``baba'', ``abab'', ``aab''$\}$.
  algorithm starts from line 2 where the root node is created. Figure \ref{fig-ac-step1}
  presents this state. Since there are four keywords the first for loop is executed four times.
  In the first round the \textit{insert} function is called for the first keyword ``baa''.
  In the \textit{insert} function the first while loop is not executed since there are no
  outgoing edges from the root node yet. The for loop is executed for every symbol and the three
  nodes (2,3,4) are added to the goto graph with edge labels 'b', 'a', and 'a'.
  Figure \ref{fig-ac-step2} presents this state of the construction.


  \begin{figure}[h]
  \centering
  \begin{tikzpicture}
%   [scale=.8]

    \node[gnode] (1) at (0,0) {$1$};
    
  \end{tikzpicture}
  \caption{A root node. TODO: bigger arrow heads.} \label{fig-ac-step1}
  \end{figure}

  \begin{figure}[h]
  \centering
  \begin{tikzpicture}
%   [scale=.8]

    \node[gnode] (1) at (0,0) {$1$};
    
    \node[gnode, right = 2cm of 1] (2) {$2$};
    \node[gnode, right = 2cm of 2] (3) {$3$};
    \node[gnode, right = 2cm of 3] (4) {$4$};
    
    \path[->]
    (1) edge node[above] {b} (2)
    (2) edge node[above] {a} (3)
    (3) edge node[above] {a} (4);

  \end{tikzpicture}
  \caption{first key added. TODO: bigger arrow heads.} \label{fig-ac-step2}
\end{figure}

  With the second round of the for loop we add the keyword ``baba''. In this case the \textit{insert}
  function executes the while loop two times since the common prefix of ``baa'' and ``baba'' has a length
  2. At this point the goto graph is branched and the remaining symbols 'b' and 'a' are inserted. The
  figure \ref{fig-ac-step3} corresponds with this state.

  \begin{figure}[h]
  \centering
  \begin{tikzpicture}
%   [scale=.8]

    \node[gnode] (1) at (0,0) {$1$};
    
    \node[gnode, right = 2cm of 1] (2) {$2$};
    \node[gnode, right = 2cm of 2] (3) {$3$};
    \node[gnode, above right = 1cm and 2.277cm of 3] (4) {$4$};
    
    \node[gnode, right = 2cm of 3] (5) {$5$};
    \node[gnode, right = 2cm of 5] (6) {$6$};

    \path[->]
    (1) edge node[above] {b} (2)
    (2) edge node[above] {a} (3)
    (3) edge node[above] {a} (4)

    (3) edge node[above] {b} (5)
    (5) edge node[above] {a} (6);

  \end{tikzpicture}
  \caption{second key added. TODO: bigger arrow heads.} \label{fig-ac-step3}
\end{figure}

  In the same way, the rest of the keywords are added. For keyword ``baba'' the insertion
  happens in the same way as with the first keyword, that is the while loop in the \textit{insert}
  function is not executed since there is no goto transition from the root with a symbol 'a'.
  For the last keyword the branching of the goto graph happens again
  similarly with the second keyword. At this point the goto function is a trie of the
  input keywords. After the first for loop in the main function is executed
  the second loop on lines 6 to 8 creates an edge from root to root for every symbol in the
  alphabet such that $g(1,\sigma)$ is not defined. With this example, we already have an edge from the
  root for both of the symbols in the binary alphabet, hence no edges are created in this step.
  However, the corresponding edge is drawn to the image for clarity. This completes 
  Algorithm \ref{ac-goto}.
  The resulting goto graph is described in Figure \ref{fig-ac-goto}

  \begin{figure}[h]
  \centering
  \begin{tikzpicture}
%   [scale=.8]

    \node[gnode] (1) at (0,0) {$1$};
    
    \node[gnode, above right = 1cm and 2.277cm of 1] (2) {$2$};
    \node[gnode, right = 2cm of 2] (3) {$3$};
    \node[gnode, above right = 1cm and 2.277cm of 3] (4) {$4$};
    
    \node[gnode, right = 2cm of 3] (5) {$5$};
    \node[gnode, right = 2cm of 5] (6) {$6$};

    \node[gnode, right =  2cm of 1] (7) {$7$};
    \node[gnode, right =  2cm of 7] (8) {$8$};
    \node[gnode, right =  2cm of 8] (9) {$9$};
    \node[gnode, right =  2cm of 9] (10) {$10$};
    
    \node[gnode, below right = 1cm and 2.277cm of 7] (11) {$11$};
    \node[gnode, right = 2cm of 11] (12) {$12$};

    \path[->]
    (1) edge[loop,in=110,out=190,distance=3cm] node[below right,align=center] {$\neg \{a,b\}$} (1)
    
    (1) edge node[above] {b} (2)
    (2) edge node[above] {a} (3)
    (3) edge node[above] {a} (4)

    (3) edge node[above] {b} (5)
    (5) edge node[above] {a} (6)

    (1) edge node[above] {a} (7)
    (7) edge node[above] {b} (8)
    (8) edge node[above] {a} (9)
    (9) edge node[above] {b} (10)

    (7) edge node[above] {a} (11)
    (11) edge node[above] {b} (12);

  \end{tikzpicture}
  \caption{The complete goto graph. TODO: bigger arrow heads.} \label{fig-ac-goto}
\end{figure}
  \end{example}
  
  Let us define a depth function for the AC-machine. At this point depth is merely a concept of the
  machine but later on the corresponding data structure is also created in Algorithm \ref{ukk-pre}.
  In an AC-machine the depth of a state $p$, denoted by $d(p)$, is a depth of the node (number of
  edges from root to that node) that corresponds
  the state in the goto graph. That is equivalent with the length of the string that represents the
  node i.e if a node $p$ in the goto graph is represented by the string $s$, then $d(p) = |s|$.
  
  Next we describe the construction of the failure function in more detail. The failure function
  for a state is calculated using the failure function of the parent of that state. Thus the computation
  starts from the start state to the states of depth 1 to the states of depth 2 and so on. That clearly
  is equal to index of a symbol in the string that the node spells out. I.e. the depth of
  the state that spells out ``baa'' is 3.

  Since the failure function is not defined for the start state (depth 0), the calculation is started
  from depth 1. We define $f(s)=1$ for all states $s$ with depth = 1. Assuming the failure function is
  calculated for all states of depth d-1, then the failure function of a state s is defined with the
  following instructions.
  
  \begin{figure}[h]
  \begin{enumerate}
  \item Let $p$ be the parent of the node $s$ such that $g(p,\sigma) = s$.
  \item Set $state = f(p)$.
  \item Set $state = f(state)$ until $g(state,\sigma)$ is defined. Since $g(start,\sigma)$ is defined
    for all symbols, at some point, the condition is met.
  \item Set $f(s) = state$
  \end{enumerate}
   \caption{Listing of the failure calculation} \label{lst-failure-calculation}    
  \end{figure}

  \begin{example}[]~\label{exmp-fail}
  Let us continue with the example from the goto function construction. By the definiton $f(1)$ is not
  defined. For the states of depth 1 ($s\in\{2,7\}$) the failure function is defined $f(s)=1$. The
  figure \ref{fig-ac-step5} represents this state.

  \begin{figure}[h]
  \centering
  \begin{tikzpicture}
%   [scale=.8]

    \node[gnode] (1) at (0,0) {$1$};
    
    \node[gnode, above right = 1cm and 2.277cm of 1] (2) {$2$};
    \node[gnode, right = 2cm of 2] (3) {$3$};
    \node[gnode, above right = 1cm and 2.277cm of 3] (4) {$4$};
    
    \node[gnode, right = 2cm of 3] (5) {$5$};
    \node[gnode, right = 2cm of 5] (6) {$6$};

    \node[gnode, right =  2cm of 1] (7) {$7$};
    \node[gnode, right =  2cm of 7] (8) {$8$};
    \node[gnode, right =  2cm of 8] (9) {$9$};
    \node[gnode, right =  2cm of 9] (10) {$10$};
    
    \node[gnode, below right = 1cm and 2.277cm of 7] (11) {$11$};
    \node[gnode, right = 2cm of 11] (12) {$12$};

    \path[->]
    (1) edge[loop,in=110,out=190,distance=3cm] node[below right,align=center] {$\neg \{a,b\}$} (1)
    
    (1) edge[bend left=15] node[above] {b} (2)
    (2) edge node[above] {a} (3)
    (3) edge node[above] {a} (4)

    (3) edge node[above] {b} (5)
    (5) edge node[above] {a} (6)

    (1) edge[bend right=15] node[above] {a} (7)
    (7) edge node[above] {b} (8)
    (8) edge node[above] {a} (9)
    (9) edge node[above] {b} (10)

    (7) edge node[above] {a} (11)
    (11) edge node[above] {b} (12);

    \path [->,dotted]

    (2) edge[bend left=15] (1)
    (7) edge [bend right=15] (1);
    
  \end{tikzpicture}
  \caption{The failure function construction when $f(s)$ is defined for all states $s$ with depth <= 1. TODO: bigger arrow heads.} \label{fig-ac-step5}
\end{figure}

  Next we calculate the failure function for the states 3, 8 and 11, that is, the states with depth 2.
  For state 3, we take the parent state 2 and follow the failure function to the root node. Since
  $g(1,$a$) = 7$ we define $f(3)=7$. For the states 8 and 11 the procedure is similar. We set 
  $f(8)=2$ and $f(11)=7$. The procedure is iterated until the failure function is defined for every
  node. Figure \ref{fig-ac} describes the final result. Because the AC-machine only contains these
  two functions, Figure \ref{fig-ac} also presents the fully constructed AC-machine.

    \begin{figure}
  \centering
  \begin{tikzpicture}
%   [scale=.8]

    \node[gnode] (1) at (0,0) {$1$};
    
    \node[gnode, above right = 1cm and 2.277cm of 1] (2) {$2$};
    \node[gnode, right = 2cm of 2] (3) {$3$};
    \node[gnode, above right = 1cm and 2.277cm of 3] (4) {$4$};
    
    \node[gnode, right = 2cm of 3] (5) {$5$};
    \node[gnode, right = 2cm of 5] (6) {$6$};

    \node[gnode, right =  2cm of 1] (7) {$7$};
    \node[gnode, right =  2cm of 7] (8) {$8$};
    \node[gnode, right =  2cm of 8] (9) {$9$};
    \node[gnode, right =  2cm of 9] (10) {$10$};
    
    \node[gnode, below right = 1cm and 2.277cm of 7] (11) {$11$};
    \node[gnode, right = 2cm of 11] (12) {$12$};

    \node[tnode, above left = 2.725cm and 2.277cm of 1] (v1) {};
    \node[tnode, below left = 1cm and 2.277 of 1] (v2) {};
    
    \path[->]
    (1) edge[loop,in=110,out=190,distance=3cm] node[below right,align=center] {$\neg \{a,b\}$} (1)
    
    (1) edge[bend left=15] node[above] {b} (2)
    (2) edge node[above] {a} (3)
    (3) edge node[above] {a} (4)

    (3) edge node[above] {b} (5)
    (5) edge node[above] {a} (6)

    (1) edge[bend right=15] node[above] {a} (7)
    (7) edge node[above] {b} (8)
    (8) edge node[above] {a} (9)
    (9) edge node[above] {b} (10)

    (7) edge[bend right=15] node[above] {a} (11)
    (11) edge node[above] {b} (12);

    \path [->,dotted]

    (2) edge[bend left=15] (1)
    (7) edge [bend right=15] (1)
    
    (3) edge (7)
    (8) edge (2)
    (11) edge[bend right=15] (7)
    
    %%(4) edge (v1) edge (v2) edge (11)
    (5) edge (8)
    (9) edge (3)
    (12) edge (8)
    
    (6) edge (9)
    (10) edge (5);

    \path[->,dotted,rounded corners, shorten <= 0.5cm, shorten >= 0.5cm] plot coordinates {(4) (v1) (v2) (11)};
    % repaint these nodes
    \node[gnode, above right = 1cm and 2.277cm of 3] (4) {$4$};
    \node[gnode, below right = 1cm and 2.277cm of 7] (11) {$11$};
    

  \end{tikzpicture}
  \caption{Fully constructed AC-machine for the keywords $R=\{$"baa", "baba", "abab", "aab"$\}$. TODO: bigger arrow heads.} \label{fig-ac}
\end{figure}

  \end{example}
  
  In the Example \ref{exmp-fail} the value for the failure function was always found in a single
  iteration of the step 3 of the above definition. Next we go through the pseudocode for the failure
  function calculation in Algorithm \ref{ac-fail}.


  \begin{algorithm}[h!]

    \caption{Construction of the failure function} \label{ac-fail}
    \hspace*{\algorithmicindent} \textbf{Input:} Goto function $g$ from algorithm \ref{ac-goto}\\
    \hspace*{\algorithmicindent} \textbf{Output:} Failure function $f$

    \begin{algorithmic}[1]
      \Function{calculateFailureFunction}{$g : \mathbb{N} \rightarrow \mathbb{N}$}
        \State $queue\gets \textit{empty}$
        \For {each $a$ s.t. $g(1,a) = s \neq 1$}
          \State $queue\gets queue \cup \{s\}$
          \State $f(s)\gets 1$
        \EndFor
        \While{$queue \neq empty$}\\
          \hspace*{\algorithmicindent}let $r$ be the next state in $queue$
          \State $queue\gets queue \backslash \{r\}$
          \For{each $a$ s.t. $g(r,a) = s$ is defined}
            \State $queue\gets queue \cup \{s\}$
            \State $state\gets f(r)$
            \While{$g(state,a)$ is undefined}
              \State $state\gets f(state)$
            \EndWhile
            \State $f(s)\gets g(state,a)$              
          \EndFor
        \EndWhile
          
      \EndFunction

    \end{algorithmic}
  \end{algorithm}


  Since the failure values must be calculated from lowest depth states to the highest depth states
  we go through the goto path using a breadth-first search. This is provided by the queue that is
  appended in every non-leaf state, i.e. the children of the currently processed state are put into the
  queue and the next state to be processed is popped. The queue is initialized to be empty in the first
  line of the algorithm. The for loop on lines 3 to 6 adds all the children of the root to the queue.
  Because the start state is special as it contains goto transitions to itself the additional constraint
  on line 3 is added to prevent infinite looping. In the same for loop, we define the
  $f(s)=1$ for all states with depth 1. The while loop from line 7 to 18 implements the instructions
  in Listing \ref{lst-failure-calculation}. Since the AC-machine does not save parent information
  this is done a bit differently, although the result is exactly the same as Listing
  \ref{lst-failure-calculation} describes. On line 8 and 9 we pop a
  state from the queue. At this point the failure function for that state is already defined. Then we
  process each child of the state separately on lines 10 to 17 by \circled{1} adding the child to the
  queue  on line 11  \circled{2} travelling the failure function of the parent as long as we get a state
  that have goto   transition with the same symbol that appears between the parent and the currently
  processed child on lines 12 to 15 and  \circled{3} set the failure function for the child on line 16.
  The execution of the function halts when the BFS of the  AC-machine is ready and the queue is
  finally empty.

  For now, as the AC-machine is fully defined we focus on its usage for finding the maximal pairwise
  overlaps of every input keyword, as the lemma \ref{lem-overlap} suggests.


\begin{lemma}[]~\label{lem-overlap}
  Let there be a state $p$ and state $q, p\neq q$ in an AC-machine that are represented by strings
  $s$ and $t$ respectively. The failure value $f(p) = q$ if and only if $t$ is the longest proper suffix
  of $s$ that is also a prefix of some keyword.
\end{lemma}
\begin{proof}[Proof by induction]
  Let us have an induction hypothesis that the lemma is true for all states whose depth is smaller
  than some depth $d > 1$. The hypothesis trivially holds for the base case when
  depth of the state is 1. That is when $f(p) = q$ is the start state represented by the empty
  string and the string represented by the state $p$ has a length 1.
  
  Let us assume there is a state $p$ of depth $d$ that is represented by a string
  $s=x_1x_2\cdots x_d$. Let $r$ be the parent state of $p$. Therefore $r$ is represented by a
  string $x_1x_2\cdots x_{d-1}$. Let $r_1,\cdots ,r_n$ be a sequence of states such that:

  \begin{enumerate}
  \item $r_1 = f(r)$,
  \item $r_i = f(r_{i-1})$ for all $ 1 \leq i < n$,
  \item $g(r_i, x_d)$ is undefined for all $i < n$,
  \item $g(r_n, x_d) = q$.
  \end{enumerate}

  Since Algorithm \ref{ac-fail} encodes the above rules on lines 13 to 16 the state $q$ is defined
  for $f(p)$. Let the strings $u_1,...,u_n$ represent the states $r_1,...,r_n$ respectively.
  According to the induction hypothesis $u_1$ is a longest proper suffix of $x_1x_2\cdots x_{d-1}$
  that is also a prefix of some keyword. Also, $u_2$ is a longest proper suffix of $u_1$ that is also
  a prefix of some keyword and so on. Therefore $u_n$ is a longest proper suffix of $s$ such that
  $u_nx_d$ is also a prefix of some keyword. Since Algorithm \ref{ac-fail} sets $f(p) = q$,
  the lemma is proven.
  
\end{proof}  

Lemma \ref{lem-overlap} is used later on in the description of Ukkonen's algorithm. Next we look
over the time complexities of Algorithms \ref{ac-goto} and \ref{ac-fail}. First we shall investigate
the time complexity of the insert and read operations of the functions $g$ and $f$. The failure
function maps each state (except the root) to some other state. It is most effective and practical
to implement this as an array which stores the value $f(s)=d$ such that the array index $s$ has
a value $d$. Therefore the both inserting and reading to/from the failure function is performed
in constant time. The goto function maps a pair of values to a single value.
In the same way, if we use a two dimensional array which stores
a $|\Sigma|$ sized row for each state in the AC-machine, the read and write operations of $g$ would
perform in a constant time. This of course results in very high memory usage since the table would be
sparsely used especially with large alphabets and long keywords. The goto function can also
be implemented in many other ways. One solution which improves the memory efficiency is to save
a balanced binary tree for each state in the AC-machine. With balanced binary trees the insert and read
operations are performed in logarithmic time.



\begin{theorem}[]~\label{thm-time-goto}
  The goto function for the AC-machine can be constructed in time $O(n \cdot t(g))$,
  where $t(g)$ is the time complexity of a single read / write query of the
  goto function.
\end{theorem}
\begin{proof}[Proof sketch]
The main function of Algorithm \ref{ac-goto} consists of two for loops.
The first loop calls a procedure once for each keyword of the input. The function calls are
executed in a constant time so the time complexity of the first for loop is linear with respect
to the number of input keywords. The second for loop starting on line 6 is used to search all
children of the start state. Since the start state has only $|\Sigma|$ goto transitions the loop
is executed at most $|\Sigma|$ times. In the worst case every symbol
in every input keyword is different. In that case $|\Sigma| = n$, where $n$ is the sum of the input
symbols. Therefore the second for loop is executed at most $n$ times resulting in the time complexity of
the main function to be linear with respect to the size of the input.

Let us next examine the insert function. The first two lines 11 and 12 can clearly be executed in
constant time. The first while loop is used to find the symbol of the string that is not yet defined
in the goto function. Let $s=vu$ such that $v$ is the part of $s$ that is already represented in the
goto function. The index variable $j$ is used so it contains the value of the first symbol of $u$ after
the while loop is executed. The for loop continues from $j$ so the lines 18 to 20 are executed
only for $u$. In the other words the \textit{insert} function behaves in a way that every symbol of
the input keyword is processed either in one of the two loops but not both. Since the time complexities
of the loop bodies depend on the implementation of $g$, the the total time complexity of the function
is $O(n)\cdot O(1) = O(n)$ or $O(n)\cdot O(log(n)) = O(n\cdot log(n))$ when the goto function
can be used in linear or logarithmic time respectively.
\end{proof}


\begin{theorem}[]~\label{thm-time-failure}
    The time complexity of Algorithm \ref{ac-fail} is the same as the time complexity of
  Algorithm \ref{ac-goto}.
\end{theorem}
\begin{proof}[Proof sketch]
In the discussion of Theorem
\ref{thm-time-goto} we reasoned that finding all children of the state can be done in
linear time. In the failure calculation we have the same kind of instruction in the for loop
on line 10. Since this loop is executed for every state of the machine this kind of approach leads
us to $O(n|\Sigma|)$ time complexity for finding every child of every state. Instead of scanning
the goto function for each symbol in the alphabet we can use an auxiliary data structure to
explicitly save the children of each state. One possibility is to use a linked list ($O(1)$ time
insertions) and save the information of the children while creating the goto function. This adds
$O(1)$ time instruction to the for loop in the \textit{insert} function. The body of this for loop
is executed $O(n)$ times so the total time complexity is not changed. This allows us to get the
children of a state in linear time with respect to the number of the children (and not the number of
symbols in the alphabet). Since every state
has at most one parent the body of the for loop is executed $O(n)$ times total nevertheless it
is nested with another while loop (line 7). Thus the while loop on line 7 as well as the for loop in
line 10 are executed once for every state, that is $O(n)$ times. The lines 11 and 12 are clearly constant
time operations and the time complexity of line 16 depends on the goto function implementation and
can also be a constant time operation. Lastly, let us examine the inner while loop on line 13.
The idea is to travel the failure path until a state is found such that the goto function
for that state is defined with the same symbol as the goto transition is defined for state we are
defining the failure function for. According to the definition of the failure function the state
$y=f(x)$ has depth at most one less than the depth of the state $x$. This means that the while
loop is always executed at most d times where d is the depth of the state \textit{state}.
... Got stuck here..? How to prove that construction of $f$ is in linear time?

\end{proof}

%  note that this works for reduced or nonreduced set.

\section{The approximation algorithm}

In this section we discuss the algorithms that use the AC-machine to find the pairwise overlaps
of a set of input keywords to implement the previously defined greedy heuristic. Conceptually,
we create an overlap graph of the keywords and select the edges according the rules of
Figure \ref{lst-failure-calculation}. In practice, the computation works in two steps. First we
preprocess the AC-machine and create the needed auxiliary data structures in Algorithm \ref{ukk-pre}
followed by Algorithm \ref{ukk-h} which implements the greedy selection of the edges in the
overlap graph.

By Lemma \ref{lem-overlap}, we can find the longest proper suffix of any state that is also
a prefix of some keyword by following the failure path of the state. In particular let the start
state $p$ itself be represented by some keyword (i.e. $p$ is a leaf). Now the failure value $f(p)$
defines the state $q$ that is represented by the longest proper suffix of $p$ that is also a prefix
of every keyword $r_i$ such that $q$ is an ancestor of $r_i$.
%is that name "supporters" ever used?
Let us denote this kind of descendant relationship with a function $L(p)$ and call it a set of
supporters.

\begin{definition}
$L(p)$ is a set of indices of the keyword set $R$ such that states represented by
$r_i\in R, i\in L(p)$ are descendants of the state $p$.
\end{definition}

In other words, if a keyword index
$j$ is in $L(p)$, then the state represented by $r_j$ is a descendant of the state $p$.
For example, the set $L(3) = 1,2$ for the example AC-machine in Figure \ref{fig-ac}.
Since the prefix relation is transitive we can iterate over the failure function to find every
pairwise overlap between $p$ and the keywords. Algorithm \ref{ukk-h} does exactly this.
Lemma \ref{lem-overlap-length} formalizes this property.

%% nyt näytetään miten aiemmin mainittua heuristiikkaa pisimmälle hampathille voidaan käyttää
%% yhdessä ac-koneen kanssa muodostamaan scs approksimaatio

%% edellisestä lemmasta f(x) = y iff y suffix of x ...
%% seuraa se että iteroimalla jonkin keywordin failure funktiota löydetään kaikki suffixit
%% jotka ovat toisten prefiksejä. viite seruaavaan lemmaan..

%% Määritellään supporters set L(s) jolle pätee että kaikki keyt jotka vastaavat lehtiä
%% siten että s on niiden edeltäjä, kuuluvat tähän settiin. annetaan esimerkki goto graphissä.
%% määritellään d (tämän vois määritellä jo aiemmin) ja annetaan esimerkki.

%% L:n ja d:n avulla määritellään seuraava lemma

\begin{lemma}[] ~\label{lem-overlap-length}
  Let there be an AC-machine for a keyword set $R=\{t_1,...,t_m\}$ and a state $p$ represented
  by string $s$.
  There is an overlap of length $d(q)$ between $s$ and a keyword $t_i\in R$ if and only if
  for some $k \geq 0$ state $q=f^k(p)$ is such that $i\in L(q)$.
\end{lemma}
\begin{proof}[Proof by induction]
  Let us have an induction hypothesis that the lemma is true for all states whose depth is
  smaller than some depth $d'\geq 1$. For the base case of the induction we use the start state
  with depth 0. The hypothesis holds
  with $k=0$ since $L(start)$ contains every keyword index $1$ to $m$ and there also exists an empty
  overlap (length $d(start)=0$) between the empty string that the start state is represented by and all
  the keywords in $R$.

  In the induction step we assume there is a state $p, d(p) = d'$ represented by string $s$.
  Assume also that there is an overlap $u$ of length $d(q)$ between $s$ and some keyword $t_i$. The
  overlap $u$ represents a state $q$.
  There are three possibilities:
  \begin{enumerate}
  \item The overlap $u$ is the string $s$ itself. In this case clearly $i\in L(p)$. Since
    $p=f^0(p)$, that is $p=q$ and $d(q) = d(p) = |s|$, the lemma is true. 
  \item The overlap $u$ is the longest proper suffix of $s$ that is also a prefix of $t_i$.
    By Lemma \ref{lem-overlap} this is true if and only if $f(p) = q$. In this case
    $i\in L(q)$ and the lemma is true with $k=1$.
  \item In this case the overlap between $s$ and $t_i$ is shorter than the longest proper
    suffix of $s$ that is a prefix of some keyword $t_j, i\neq j$. Let $u'$ be the maximal
    overlap between $s$ and $t_j$ that is $u'$ is the longest proper suffix of $s$ that is also
    a prefix of some keyword. Let also $q'$ be represented by the string $u'$. By Lemma
    \ref{lem-overlap} $f(p) = q'$. Because $u$ and $u'$ are both suffices of $s$ and $|u'|<|u|$ we
    know that there is also an overlap $u$ between $u'$ and $t_i$. Since $d(q') < d(p) = d'$ we can
    apply the induction hypothesis to the state $q'$. Therefore, there is an overlap of length
    $d(q)$ between $q'$ and a keyword $t_i$ (which we know is true) if and only if for some
    $k \geq 0$ state $q=f^k(q')$ is such that $i\in L(q)$. Since we know by assumption $q'=f(p)$ and
    by induction $q=f^k(q')$ it follows that $q=f^{k+1}(p)$. We also know that $i\in L(q)$ and
    $|u|=d(q)$. This concludes the proof.
    
  \end{enumerate}
\end{proof}

\begin{theorem}[] ~\label{thm-maximal-overlaps}
  Let $p$ be a state represented by string $s_i$ and let $s_i,s_j\in R$. The maximum overlap between
  $s_i$ and $s_j$ is represented by the state $q$ such that $q$ is the first sate in the failure
  path of $p$ $f^k(p)$ and $j\in L(q)$. The length of this maximum overlap is $d(q)$.
\end{theorem}
\begin{proof}[Proof sketch]
  The proof directly follows from Lemmas \ref{lem-overlap} and \ref{lem-overlap-length}
\end{proof}

\paragraph{Explanation of Algorithm \ref{ukk-pre}.}
By Theorem \ref{thm-maximal-overlaps} the AC-machine implicitly encodes the information of the
longest pairwise overlaps between any two keywords. By Lemma \ref{lem-composition} the lengths of the
maximal overlaps encoded in the AC-machine are equivalent to the weights of the corresponding edges
in the overlap graph. We use the property of Theorem \ref{thm-maximal-overlaps} to implement
the greedy heuristic for the \textsc{Longest Hamiltonian Path} problem in the overlap graph. The
overlaps encoded by the failure function have length equivalent to the depth of the state that
have an incoming failure path. Hence the longest possible overlap can be found by searching the state
that has some incoming failure path(s) such that the depth of that state is maximized. This suggests
the following implementation.

\paragraph{Implementation of the heuristic using the AC-machine.}
We process every state by following the failure path and saving 
which state the failure transition is coming from. 
The states with greater depth are processed first. This can be done with a reversed breadth-first
search (BFS) ordering. Since $d(p) > d(f(p))$ for every $p$, the reversed BFS ordering of the states
ensures that when the state $q$ is being processed, all the states $p$ with $q=f^k(p)$ are already
processed and we have the information of each such state. Since we have the information of every such
state we also have the information of all the overlaps between $L(q)$ and each state $q$, s.t.
$q$ is in failure path from $p$. We can now select a corresponding edge in the overlap graph as long
as it does not violate the constraints of the heuristic, that is, considering earlier selections
the corresponding edge does not form a cycle, nor does its start state already have an outcoming
edge, nor does its end node already have an incoming edge. By processing all the states in this
fashion we end up with the set of edges that forms a Hamiltonian path in the
overlap graph. 

\begin{example}
Let us apply this method to the AC-machine described in Figure \ref{fig-ac}. The processing is started
with depth 4 from state 6 or state 10. While processing those states we save the information of
the incoming failure path for state 9 and 5 respectively. Depending on the order in which we process
the states with depth 3 we end up selecting one of the two possible overlaps. Either the state 5
is processed first and the overlap between $s_3$ and $s_2$ is selected, or the state 9 is processed
first and the overlap between $s_2$ and $s_3$ is selected. The information of the incoming failure
paths for states 3, 8 and 11 is also saved while processing the states of depth 3. Let us assume
that the state 9 was processed first and the second of the presented overlaps were selected.
While processing the state 3 we have an information of the incoming failure path from state 6 (via the
state 9). This indicates that there exists an overlap between the state 6 and every state
in $L(3)=\{6,4\}$ The self-overlap between $s_2$ and $s_2$ is forbidden since it would
create a cycle. However the other overlap between $s_2$ and $s_1$ cannot be selected either because
the edge from $s_2$ is selected earlier. The state 8 also has information of two incoming failure
paths. Now there already exists an edge with $s_3$ as an end node. Thus no edges are selected.
When processing the state 11 the only possible overlap is compatible with the rules and the edge
from $s_1$ to $s_4$ is selected. For the states of depth 1 it does not matter which one is processed
first. In any case, while processing the state 7 we find only one failure path coming from a leaf state
corresponding to keyword $s_2$. Since $s_2$ already has an outgoing edge the process continues
without any selection. While processing the state 2 we find two possible selections. We can select
either $(s_3,s_1)$ or $(s_4,s_2)$. Depending on this last selection the common superstring for
the projection of the selected Hamiltonian path is either ``baababab'' or ``bababaab'', which
both have the same length 8 and the same compression 14-8=6.

When finding two equally long maximal overlaps the heuristic allows arbitrary selections.
Because of this, the outcome of the algorithm may differ between implementations. For example,
the first time
we made a selection between two allowed edges, if we selected otherwise the heuristics would
have produced either ``aababaa'', ``baababa'' or ``ababaab''. These all are the actual shortest common
superstrings with length 7 and compression 7.
\end{example}

%Todo: Why define these here. We could use at least $F(index)$ all over the text.
The actual algorithm is implemented in two parts. Algorithm \ref{ukk-pre} defines the first part
which is used to calculate a set of auxiliary data structures that are needed in order to implement
the actual heuristic. The algorithm takes as an input the AC-machine constructed with
Algorithms \ref{ac-goto} and \ref{ac-fail} as well as the set of keywords $R$ which is also
the same set that the goto and failure functions were constructed for. 
The listing in Figure \ref{lst-auxiliary-data} consists of the data structures that are calculated
in algorithm \ref{ukk-pre} and used in algorithm \ref{ukk-h}. It uses $I$ as
a set of keyword indices in $R$.

\begin{figure}[h!]
\begin{enumerate}
  \item $F : I \rightarrow Q, F(i)=q$ Maps the index of the keyword to the corresponding state. I.e a state $q=F(i)$ is represented by the keyword $s_i$.
  \item $E : Q \rightarrow I, E(q)=i$ Inverse function of $F$. If the state $q$ is a leaf represented by $s_i$ then $E(q)=i$. For every non-leaf states $q'$ we define $E(q')=0$.
  \item $d : Q \rightarrow Q$ Depth function for each state as defined before.
  \item $L : Q \rightarrow 2^{I}$ Set of supporters as defined before.
  \item $b : Q \rightarrow Q, b(q)=r$ Reverse breadth first search ordering of the states.
  \item $B$ The last state in the BFS ordering of the machine. I.e. $b(B)$ is the second last state in BFS ordering, $b(b(B))$ is the third last and so on.
\end{enumerate}
\caption{Static data structures created in Algorithm \ref{ukk-pre}} \label{lst-auxiliary-data}
\end{figure}

Since by Lemma \ref{lem-reduced} a shortest common superstring for a reduced set of keywords
is same as the set where it was reduced from, Algorithm \ref{ukk-pre} also performs
the elimination of those keywords that are substrings of other keywords in the set. There are two
situations when the structure of the AC-machine indicates the occurrence of such a keyword. First,
if there is a goto transition from a state that is represented by a keyword that keyword
must be a substring of some other keyword. I.e. If $g(F(i),\sigma)$ is defined for any pair
$(i,\sigma)$ then the keyword $s_i$ must be a substring (prefix)  of some other keyword and can be
removed. The second indication of such a keyword is when a leaf state has an incoming failure
transition from some other state. By Lemma \ref{lem-overlap}, if $F(i) = f(q)$ then the keyword
$s_i$ must be a proper suffix of some other state. This means that $s_i$ is either a suffix
(if the state $q$ is itself a leaf) or a substring that is not a prefix nor suffix of some other
keyword. The removal of a substring is done using the function $F$. If some keyword
$s_i$ is a substring of another keyword, we set $F(i)=1$ to indicate that the state represented
by $s_i$ is not processed like it was a keyword.

% Näihin voi myös raittaa labelit per rivi ja viitata siten.
  \begin{algorithm}[h!]

    \caption{Preprocessing algorithm for the greedy heuristic} \label{ukk-pre}
    \hspace*{\algorithmicindent} \textbf{Input:} AC-machine constructed with algorithms \ref{ac-goto} and \ref{ac-fail} and the associated set of keywords $R=\{s_1,...,s_m\}$ \\
    \hspace*{\algorithmicindent} \textbf{Output:} $F, E, d, L, b$ and $B$ as defined earlier.\\
    \hspace*{\algorithmicindent} \textbf{Preconditions:} Initially $E(s) = 0$ for every state s.
    
    \begin{algorithmic}[1]
      \Function{calculateAuxiliaryFunctions}{$g, f, R$}
        \For {$i = 1,...,m$} \\
          \hspace*{1.1cm plus \algorithmicindent} let $x_i = a_0,...,a_{k-1}$
          \State $s\gets 1$
          \For {$j = 0,...,k-1$}
            \State $s\gets g(s,a_j)$
            %% ERRATA: i = j.
            \State $L(s)\gets L(s) \cdot \{i\}$
            \If {$j = k-1$}
              \State $F(i)\gets s$
              \State $E(s)\gets i$
              \If {$s$ is not a leaf of the AC machine}
                \State $F(i)\gets 1$
              \EndIf
            \EndIf
          \EndFor
        \EndFor
        \State $queue\gets 1$
        \State $d(1)\gets 0$
        \State $B\gets 1$
        \While{$queue\neq empty$}\\
          \hspace*{1.1cm plus \algorithmicindent} let $r$ be the next state in queue
          \State $queue\gets queue \backslash \{r\}$
          %% ERRATA: Cyclic graph!          
          \For {each $s$ that is a child of $r$ and $s\neq r$}
            \State $queue\gets queue\cdot s$
            \State $d(s)\gets d(r)+1$
            \State $b(s)\gets B$
            \State $F(E(f(s)))\gets 1$
          \EndFor
        \EndWhile
      \EndFunction
        
    \end{algorithmic}
  \end{algorithm}


  % algoritmin läpikäynti rivi kerrallaan.
\paragraph{Explanation of Algorithm \ref{ukk-pre}.}  
Algorithm \ref{ukk-pre} proceeds in two independent steps that could be processed in either order.
In the first part we loop through every keyword (line 2) and every symbol in the keywords (line 5).
Before we enter the for loop on line 5 we set the state $s$ to the start state. In the loop
we traverse the AC-machine from the start state to the state represented by the current keyword $x_i$.
On the way we add the index $i$ to every state we pass through (line 7). Looping every keyword in this
way constructs the function $L$ as defined. At the end of every keyword (if line 8 evaluates to
\textit{true}) we set the values for functions $F$ and $E$ as defined. (lines 9 and 10).
Further we set $F(i) = 1$ for those keyword indices that correspond to a keyword that is a prefix
of some other keyword. (lines 11, 12).

The second part of this algorithm calculates the reversed breadth-first ordering $b$ for the
AC-machine and set the depths for every state. In addition, substring keywords other than
prefixes are removed. This part is basically a BFS travel through the machine. We create a queue
and initialize it with the start state $1$ on line 17 and set the depth of the start state to
zero (line 18). The variable $B$ initialized on line 19 contains the value of the latest state
that is already processed, thus at the end of the algorithm the variable $B$ contains the last state
in the BFS ordering (which is the first state in the reversed BFS ordering). The actual BFS starts
on lines 20 to 22 when we start to process the states in the queue. When all states are processed
the while loop ends and the function terminates. In the for loop starting at line 23 we add the
children of the current state to the queue as the BFS requires. On line 25 we set the depth
of the current state to be one greater than its parent state. We also update the $B$ variable
as discussed before. Finally the line 27 sets $F(i) = 1$ for each keyword index $i$ such that
the state represented by $s_i$ has an incoming failure transition from any other state.

The lines 12 together with line 27 ensure that for all keywords $i$ such that $s_i$ is a substring
of some other keyword, the value of $F(i)=1$, i.e., the set of keywords $\{s_j : F(j) \neq 1\}$ is
reduced.

% flags indicating leaves ?
\begin{theorem}[]~\label{thm-time-pre}
  The time complexity of Algorithm \ref{ukk-pre} is the same as time complexity of
  Algorithm \ref{ac-goto}.
\end{theorem}
\begin{proof}[Proof sketch]
As with the construction of
the AC-machine the time complexity of querying the goto function depends on its underlying
implementation. At first we ignore the queries for the goto function and focus on the rest.
In the first part of the algorithm we iterate over all keywords at the outer for loop. In the
inner for loop we iterate over each symbol in the keyword so the total number of iterations
in the inner loop equals the total number of symbols in all the keywords combined. On line 7
we add an element to a set of items $L(s)$. Since this set does not need to be ordered it can be
implemented for example as a linked list so the time complexity for adding an item is $O(1)$.
Also the statements on lines 8 to 12 can clearly be performed in constant time. This means that
the time complexity of the outer for loop is $O(n\cdot t(x))$ where $n$ is the total number of
symbols in the input and $t(x)$ is the time complexity of querying the goto function.

The lines 17 to 19 can clearly be executed in constant time. The while loop starting at line 20
does the breadth first search for the AC-machine. Since the number of states in the machine is
$O(n)$, the execution time for the BFS is also $O(n)$. Here we assume that the queue is also
implemented using a structure that enables insertions and deletions in a constant time (e.g.
a linked list). The lines 25 to 27 that are not related to the BFS search itself can also clearly
be executed at constant time. This results in the total time complexity of algorithm \ref{ukk-pre} to be
$O(n\cdot t(x))$. For example, if the goto function is implemented using a direct indexing, the
reading and writing of $g$ is performed at constant time, thus the time complexity is $O(n)$.
On the other hand, if $g$ is implemented using for example balanced binary trees, then the access
time of $g$ is $O(log (n))$ and the total time complexity of the algorithm is $O(n\cdot log(n))$.
\end{proof}

Let us now discuss the algorithm that implements the greedy selection of the edges in the
overlap graph to form an approximate longest Hamiltonian path in more detail. Earlier we simulated
the greedy heuristic without defining the appropriate data structures. Now, with Algorithm
\ref{ukk-pre} we have augmented the AC-machine with a set of auxiliary functions and one variable,
listed in Figure \ref{lst-auxiliary-data}.
This auxiliary data is static in the following algorithm \ref{ukk-h}, i.e. the data is only read
not written at any point. In addition, we need some dynamic data to store the information
related to the selection of the edges in the overlap graph. As we discussed in the simulation
of the heuristic, when the execution enters a state $q$, all other states with depth $>d(q)$ are
already processed and there is an information of every $F(i)$ that is a starting point of any failure
path to $q$,
such that there is no already selected edge $(s_i, s')$ for any keyword $s'$. Let us denote
this information with $p$.

\begin{definition}~\label{def-p}
  At the point of the execution of Algorithm \ref{ukk-h} when a state $q$ is being processed,
  $p(q)$ is a set of keyword indices $i$ such that each state $ F(i), i\in p(q)$ is
  a starting point of a failure path to $q$, i.e $q = f^k(F(p(q))) : k>0$ and there is no
  edge $(s_i, s')$ already selected for any $s'\in R$.
\end{definition}

When we encounter a state $q$ with $i\in P(q)$ we know that there exists an overlap of length
$d(q)$ between $s_i$ and every keyword whose corresponding state is accessible in the AC-machine
through the state $q$, i.e. that state is descendant of $q$. We have already defined a data structure
$L$ for this information, hence there is a maximal overlap of length $d$ between every $s_{P(q)}$
and every $s_{L(q)}$. Based on this information, the next edge cannot yet be selected.
Note that although the data structure $P$ ensures that every keyword $s_{P(q)}$ can be selected
for a start node of the new edge the $L(q)$ returns static data. If there are already selected
edges $(s',s_{L(q)})$ for each $L(q)$, no more edges can be selected while processing $q$.
This constraint, that is to prevent a node having multiple ingoing edges, is encoded in the
function $forbidden(i)$ where $i$ is an index of a keyword.

\begin{definition}~\label{def-forbidden}
  At any point of execution of Algorithm \ref{ukk-h}
  \[
  forbidden(i) =
  \begin{cases}
    true, &\text{ if }  (s',s_i)\in H \text{ for any } s'\in R; \\
    true, &\text{ if }  s_i \text{ is a substring of any other string in } R; \\
    false, &\text{ otherwise}. \\
  \end{cases}
  \]
\end{definition}

We encode the constraints that no node can have multiple inbound or outbound edges
with the functions $L, P$ and $forbidden$. The last constraint, which prevents the edge selections
that form a cycle, is guarded with functions $FIRST$ and $LAST$. Both of these functions are defined
for all keyword indices. At the beginning of Algorithm \ref{ukk-h}, $FIRST(i) = LAST(i) = i$
for every keyword index $i$. The value of these functions is updated such that at any point of execution
if we have selected a set of edges that forms a path starting from $s_x$ and ending to $s_y$, then
$FIRST(y)=x$ and $LAST(x)=y$. Note that the values for the nodes in the middle of such paths are
not updated nor queried. Now, before the new edge $(s_i,s_j)$ is selected we check that the
selection does form a cycle by checking that $FIRST(i) \neq j$. Note that this is the same thing
than checking that $LAST(j) \neq i$. Only one of those checks are necessary. If the check fails,
the edge in question must be discarded. 

  \begin{algorithm}[h!]
    \caption{Selection of the edges} \label{ukk-h}
    \hspace*{\algorithmicindent} \textbf{Input:} The AC-machine including the auxiliary data from algorithm \ref{ukk-pre} \\
    \hspace*{\algorithmicindent} \textbf{Output:} The approximate longest Hamiltonian path in the overlap graph.
    \begin{algorithmic}[1]
      \Function{createPath}{$g,f,F,E,d,L,b,B$}
        \For {$j = 1,...,m$}
          \If {$F(j)\neq 1$}
            \State $P(f(F(j)))\gets P(f(F(j)))\cdot {j}$
            \State $FIRST(j)\gets j$
            \State $LAST(j)\gets j$
          \Else
            \State $forbidden(j)\gets true$
          \EndIf
        \EndFor  
        \State $s\gets b(B)$
        \While {$s\neq 1$}
          \If {$P(s)$ is not empty}
            \For {each $j$ in $L(s)$ s.t. $forbidden(j) = false$}
              %% ERRATA: Check emptiness
              \If {$P(s)$ is empty}
                \State \textbf{break}
              \EndIf
              \State $i\gets$ the first element of $P(s)$
              \If {$FIRST(i) = j)$}
                \If {$P(s)$ has only one element}
                  \State \textbf{continue}
                \Else
                  \State $i\gets$ the second element of $P(s)$
                \EndIf
              \EndIf  
              \State $H\gets H\cdot \{(x_i,x_j)\}$
              \State $forbidden(j)\gets true$
              \State $P(s)\gets P(s) \backslash\{i\}$
              \State $FIRST(LAST(j))\gets FIRST(i)$
              \State $LAST(FIRST(i))\gets LAST(j)$
            \EndFor
          \State $P(f(s))\gets P(f(s))\cdot P(s)$
          \EndIf
          \State $s\gets b(s)$
        \EndWhile
      \EndFunction
    \end{algorithmic}
  \end{algorithm}

\paragraph{Explanation of Algorithm \ref{ukk-h}.}
Algorithm \ref{ukk-h} starts by setting the initial values for the dynamic functions
$P, FIRST, LAST$ and $forbidden$ in the for loop starting at lines 2. On lines 4 to 6 the algorithm
initializes the data that is related to the used keywords, that is the keywords that forms a reduced set.
The line 4 sets the $P$ function for each state that has a direct failure link from some of
those keywords. For those same indices the functions $FIRST(i)$ and $LAST(i)$ are initialized
to $i$, indicating there is no edges yet selected. The else clause on lines 7 to 8 is
to ensure that there will be no incoming edges to the keywords that are substrings of other
keywords. (Note: setting $F(j) = 1$ for the same keywords in algorithm \ref{ukk-pre} ensures
the prevention of outcoming edges from those strings.) After the initialization is done the
first state to process is set on line 11. Note that setting $s=B$ would cause one more unnecessary
iteration of the following while loop since the last state in the BFS ordering can not have
incoming failure transitions, hence the first state is set to the second last node in that
ordering.

The while loop starting at line 12 implements the actual selection of the edges. Since
deepest states are processed first and nonemptiness of $P(s)$ and $L(s)$ indicates an existing
overlap of depth $d(s)$ the maximal overlaps are selected first, if allowed by the constraints.
On line 13 we ensure that $P(s)$ is not empty since otherwise there are no allowed overlaps.
After that each keyword index in $L(s)$ that is not forbidden are processed (line 14). Each
iteration (except the last one if no allowed edge is found) of that for loop decreases the
size of $P(s)$ and $\{i\in L(s) : forbidden(i) = false\}$. On lines 15 to 17 we check that
the set $P(s)$ to ensure there is at least one possible startpoint to the new edge. after line 18
there is a potential new edge that needs to be verified against the cyclicality constraint
(n line 19). If the edge $(s_i,s_j)$ would create a cycle we check on line 20 if there exists
another index in $P(s)$ and if so, the edge is created. Otherwise the execution is jumped to the
next iteration. Note that if the edge $(s_i,s_j)$ would create a cycle then no other edge $(s',s_j)$
will for any $s'$. Therefore it is only needed to query $P(s)$ at most twice with each iteration.

On line 26 the edge is selected. The lines 27 ensures the definition for $forbidden$ function
remains valid and lines 29 and 30 updates the $FIRST$ and $LAST$ functions accordinly. On line
28 the selected keyword index is removed from $P(s)$. After all possible edges starting from
$s_{P(s)}$ are examined the process can move on to the next state in the $b$ link chain (line 34).
Before that on line 32 the $P(s)$ is concatenated to $P(f(s))$ so the possibly remaining unused
keyword indices are passed to the less deep states.




\begin{theorem}[]~\label{thm-time-h}
  The time complexity of Algorithm \ref{ukk-h} equals the time complexity of
  Algorithm \ref{ac-goto}
\end{theorem}
\begin{proof}[Proof scketch]

As with $L$ in Algorithm \ref{ukk-pre}
we assume that the set returned by function $P$ is implemented using a linked list, allowing a constant
time insertions, deletions, and concatenations. The first for loop starting at line 2 can clearly be
executed in linear time since all the statements take constant time and $m \leq n $. The while loop
starting at line 12 iterates once for each state that is $O(n)$ times. However, the for loop starting
at line 14 is also executed at most $m = O(n)$ times.

How is that condition on line 14 resolved in linear time ????

\end{proof}

Theorems \ref{thm-time-goto}, \ref{thm-time-failure}, \ref{thm-time-pre} and \ref{thm-time-h}
show that every algorithm presented in this chapter have the same time complexity that depends
on the implementation of the goto function for the AC-machine. As discussed earlier, the goto function
can be implemented to allow read and write queries in  $O(1)$ using direct indexing over a
two-dimensional array. Another feasible possibility, especially for the larger alphabets,
is to use balanced
binary trees with $O(log(n))$ time complexity. This conclusion leads us to the following theorem.

\begin{theorem}[]~\label{thm-overall-time}
  With Algorithms \ref{ac-goto}, \ref{ac-fail}, \ref{ukk-pre} and \ref{ukk-h} the greedy heuristic
  for the approximate longest Hamiltonian path in the overlap graph can be implemented in time
  $O(n \cdot t(x))$ where $t(x)$ is the time complexity of a single read / write query of the
  goto function.
\end{theorem}


  %Tänne bugikorjaukset sähköpostista joka lähetetty 30.4
  %Kommentoitu sanoin ERRATA:

\chapter{Relative Lempel-Ziv}~\label{chp-rlz}

Dictionary compression algorithms \citep{Storer82} process the input as a sequence of symbols.
Using the structure and repetitiveness of the input, longer input sequences are encoded as a shorter
tokens. This reduces the space needed to save the original input data. In this approach a so called
dictionary is maintained. The dictionary may be static or it can be updated during the (en)compression.
The tokens (also called phrases) contain a pointers to the dictionary such that each token
unambigiously corresponds to a sequence of symbols. \citep{Pu05} 

In this chapter we discuss the Lempel-Ziv family \citep{Ziv77}, \citep{Ziv78}, \citep{Welch84}
 of dictionary compression methods.
first we discuss Lempel-Ziv77 (LZ77 for short) algorithm
\citep{Ziv77} as an introduction to the relatively new  algorithm called
Relative Lempel-Ziv \citep{Kurppu10} (RLZ for short). We show few examples of these algorithms and
discuss the possible improvement for RLZ dictionary construction with use of a shortest common
superstring approximation algorithms.

\section{Lempel-Ziv77}

Lempel-Ziv77 is an adaptive dictionay compression method. By adaptive, we mean that during the
encompression the dictionary is updated depending on the input. At first, the dictionary is empty. 
The original LZ77 algorithm uses a part of the input, called a sliding window, as a dictionary.
However, we introduce a version of LZ77 that has an unbounded windows size, allowing the phrases
to point any position of the input that is already processed. I.e. the dictionary consists of the
already processed part of the input.

Each phrase in LZ77 are either 
\begin{enumerate}
\item a literal phrase or
\item a repeat phrase.
\end{enumerate}

Literal phrases are used when the current symbol in the input file is first encountered. The repeat
phrases consists of a pointer and a length, where the pointer defines the point in the already
processed text where the repeating sequence of symbols starts. To use a repeat phrase, the text up
to the pointer must be already tokenized.
The length encodes the size of the original text
corresponding to the repeat phrase. The more repetitive the input text is the bigger the length
values in a repeat phrases are. That means a single phrase encodes a longer portion of the original
text, resulting a better compression. On the other hand, if the length parameter is small enough,
to save a single repeat phrase we need more space than saving the original text as is. For example,
if the repeat phrase is encoded using a two 4-byte integers and the input symbols are encoded in a
single byte, the repeat phrase with a length parameter less than 8 would actually increase the
space requirement of the compression relative to the original text.

\begin{example}
  Let us simulate the LZ77 algorithm for a text string $T=$ ``abbabaabbaba''. We denote literal phrases
  with $[\sigma]$, where $\sigma$ is the symbol of the phrase. Repeate phrases are denoted
  with (pos,len). The processing starts
  at the first symbol of $T$. Since the symbol ``a'' is encountered first time we create a
  literal phrase $[$``a''$]$. Same thing happens with the next symbol ``b''. The third symbol ``b'' is
  encoded with a repat phrase $(2,1)$ which means that the phrase encodes a string of length 1 starting
  at position 2. The next repeat phase $(1,2)$ encodes the string ``ab''. At this point we have encoded
  the string ``abbab''. The next phrase encodes the following ``a'' with a phrase $(1,1)$. Since
  the text string encoded to this point contains a string ``abbab'' the last phrase is a pointer
  to the start of this string with a length of 6. The final LZ77 factorization is therefore
  $[$ ``a''$][$ ``b''$](2,1)(1,2)(1,1)(1,6)$.
\end{example}

% decompression O(n)
To decompress the LZ77 parse we process the phrases from beginning to the end. For each
phrase, using the already decompressed data, we augment the dictionary with the text corresponging
to the phrase. After the decompression, the generated dictionary is same as the dictionary used with the
compression. In other words, the resulting decompression dictionary is the same sequence of symbols
as the original uncompressed data. 

\section{Relative Lempel-Ziv}

Relative Lempel-Ziv is a simple and efficient \citep{Deorowicz11} general purpose \citep{Hoobin11}
compression method. It combines a static dictionary with a LZ77 parsing providing a semi random
access to the compressed data. It was developed to compress a set of genomes, however, further
research has shown that RLZ scales well with any repetitive data providing
one of the best (de)compression time and compression ratio \citep{Gagie16}.

To compress a collection of genomes of a same species, RLZ works as follows. One genome is selected to a
base sequence. This genome acts as a static dictionary. Further, LZ77 parse of every other sequence
is generated relative to the base. The LZ77 parses of the other sequences are restricted to have
references only to the base sequence acting as a dictionary. As opposed to the adaptive dictionary
of the LZ77 algorithm, where the dictionary is constructed during the (de)compression, RLZ uses
a static dictionary, providing a random access for any phrase whether or not the decompression
of the previous part of the data are already performed. Definition \ref{def-rlz} formalizes the
Relative Lempel-Ziv parsing (or factorization).

%alemmat setit orkkis paperista
%- kehitetty ratkaisemaan ongelman jossa kollektiosta dna sekvenssejä saadaan random accessilla
%nopeasti dataa purettua.

%- menetelmässä käytetään yhtä sekvenssiä ``base sequence'' staattisena dictionarynä muiden sekvenssien
%pakkaamiseen.
%- muut sekvenssit pakataan LZ77 parsena tätä basea vastaan siten että phraset saavat osoittaa vain
%tähän sekvenssiin.

%random access tulee siitä että toisin kuin muissa lz varianteissa dictionary on staattinen ja
%kokoajan saatavilla. ei ole tarvetta purkaa koko tiedostoa että pääsee haluttuun kohtaan.

%% todo: kysy puglisilta mikä on w_0 idea?
\begin{definition}~\label{def-rlz}
  Let $T$ and $R$ be strings. Relative Lempel-Ziv factorization of $T$ relative to $R$, denoted by
  RLZ$(T|R)$ is a factoriszation $T=w_0w_1w_2\cdots w_n$. The first factor $w_0$ is an empty string
  and for each $1\leq i \leq n$, $w_i$ is either
  \begin{enumerate}
  \item a symbol $\sigma$ that does not occur in $R$, or
  \item the longest prefix of $T[j...|T|]$, where $j = |w_0\cdots w_{i-1}|$ that is a substring of $R$.
  \end{enumerate}
  Each factor $w_i$ is encoded as a pair $(p_i,l_i)$ where $p_i$ is the index of $R$ where $w_i$
  starts from and $l_i$ is the length of $w_i$.
\end{definition}

\begin{example}
  ~\\
  \vspace{-1cm}
  \begin{center}
  \begin{tabular}[H]{r c c c c c c c}
    &1&2&3&4&5&6&7 \\
    $R=$&a&b&b&a&b&a&a \\
  \end{tabular}
  \end{center}
  Let $T=$ ``abbababaaaaabbabaa'' and $R=$``abbabaa'' be strings. The RLZ factorization of $T$
  relative to $R$ is RLZ$(T|R)=w_0(1,6)(5,3)(6,2)(1,7)$.
\end{example}

As discussed before, the RLZ can be used as a general purpose compressor. With a collection of
genomes of a same specie, one genome is a natural choice for a dictionary that other genomes are
factorized against to. Choosing the dictionary for arbitrary data (not necessarily a collection of the
same kind), is however more difficult since there is no natural reference. As the number of factors
highly depends on the dictionary, the decision what to use as a reference must be examined. 

% alemmat setit sebastian deorowich
- yksittäisen dna sekvenssin uskottu olevan lähes pakkaamaton.
%- rlz on simple and efficient

% hoobin
- jopa 0.1 prosentin dictionary tuottaa hyvän kompression ja purkuajan.

%- myöhemmin sovellettu myös ``general data:n'' pakkaamiseen. täss

- merkittävä tekijä rlz:ssa on se mitä dictionaryä käyttää. 

- hoobin et al näyttää että hyvä dictionary voidaan saatuvvaa keinotekoisesti konkatenoimalla random
sämplejä datasta.

% gagie

-rlz on suosittu pakkausmenetelmä

- voidaan käyttää vaikka ei olisikaan luonnollista referenssiä

%- scales well

%- pidetään yhenä parhaista general purpose pakkauksista.

- näytetään teoriassa miksi keinotekoinen dictionary toimii

% puglisi and zhukova
- voidaan käyttää tehokkaasti suffix arrayn pakkaamiseen

- pohditaan miten scs voisi parantaa tätä paskaa. ts. sama määrä tokeneita mutta pienemmällä
dictionaryllä. 


% random access
% can not be better than LZ77 full window.
% what does it mean that the original text is GREEDILY factored ? 
%%
\chapter{Empirical Evaluation}

% todo: This section.
In this chapter we introduce and describe an open-source implementation of Ukkonen's shortest
common superstring
approximation algorithm discussed in Chapter \ref{chp-ukkonen}. We go through the most significant
aspects of the implementation and usage of the code. We also describe the data that was used to
benchmark the efficiency of the implementation as well as the experiments related to
relative Lempel-Ziv compression using the SCS of the sampled original data as the reference string.

Finally, we show multiple tables of data that compare the implementation with
another implementation of the same heuristic. We also show how the number of factors in the
RLZ compression is reduced when the SCS approach is used.

\section{Implementation}~\label{sec-our}
% our implementation and reference implementation

% std and LINUX libraries ? time measurements?
\paragraph{Overview:}
%Our implementation of Ukkonen's shortest common superstring approximation algorithm were written
%for this thesis.
The implementation is written in the C programming language without any external
libraries. Only the C standard library and POSIX libraries were used.
Although algorithms presented in Chapter
\ref{chp-ukkonen} are relatively short, the actual implementation also contains other code
that binds Algorithms \ref{ac-goto}, \ref{ac-fail}, \ref{ukk-pre} and \ref{ukk-h} together. 
Various data structures were also implemented for the algorithm. In total, the implementation consists
of 2400 lines of code, including the comments and empty lines. The implementation also includes some
test scripts for building and testing the code. 

% Explaion array goto and rb tree goto
As discussed earlier, the goto function of the AC-machine can be implemented in many different
ways. In our implementation the underlying data structure for the goto function is defined at compile
time to be either a two dimensional array or a red-black tree\ref{asd}. Hence, by Theorem \ref{thm-overall-time}
the time complexity of our implementation of Ukkonen's algorithm is either $O(n)$ or $O(n\cdot log(n))$,
respectively.

% Explain string quicksort
Since the definition of the input of the algorithm is a set of keywords, we have to guard against that
constraint too. In practice, that means we need to examine the input keywords and remove any possible
duplicate values. The duplicate removal is implemented by sorting the input keywords and removing any
consecutive duplicate strings. The keywords are sorted using string quicksort \ref{asd}.
Therefore the overall time complexity of our software is $O(n\cdot log(n))$. At the time of writing, the
duplicates are forced to be searched. There is no command line parameter that would omit the 
$O(n\cdot log(n))$ time string sorting. However, as we see later even with the $O(n)$ implementation
of Ukkonen's algorithm the $O(n\cdot log(n))$ time preprocessing is insignificant even with relatively
large inputs.

% Explanation of the limitations: char
Our implementation uses the \texttt{char} datatype of the C programming language as the
type of the element
in an integer alphabet that is used in the algorithm. The \texttt{char} datatype is an obvious
choice since the C starndard defines many string processing functions for \texttt{char} arrays.
However, since \texttt{char} is only 8 bits long, this choice limits our implementation for alphabets
$\Sigma$ with $|\Sigma| \leq 256$. The code is however designed so that the alphabet data type
is bound to a macro. Thus, the change in the
data type would be as easy as possible if the software is further developed to work with
bigger alphabets. 

The git repository of our implementation can be found from GitHub:\\ \url{https://github.com/apason/ukkonen-90}

% Explain links optimization flag
\paragraph{Compile-time parameters:}
To optimize the memory consumption and the execution time depending on the input data,
various alternative approaches were implemented.
The purpose was to include various compile or runtime flags that would change the behaviour
of the application. All of those modifications except one were discarded. The current version includes
a compile-time macro OPTIMIZE\_LINKS that changes the data structure that is used to the the children of each
state. Later on we refer to this parameter as links optimization. The optimization works as follows.
Each state must be able to refer to all of its children in time linear with respect to the number
of children.
Each child of a state is saved in a list. When links optimization is turned on, this list is
implemented as an array and allocated
to have as many elements as there are symbols in the alphabet. Since the list has a fixed size, there
will be only one memory allocation for each state. The drawback of this feature is the increased memory
consumption with large alphabets. When links optimization is turned off, the list containing the
children of a state is implemented using a linked list. A new node to the list is allocated for each
child, thus the overhead from the memory management is relatively larger. In practice, the links
optimization is viable only with small alphabets and can decrease the execution time and the memory
consumption with larger alphabets. As we will see later, in general the best performance for big
alphabets
is achieved with the combination of red-black trees and disabled links optimization. On the contrary,
when the alphabet is small enough, direct indexing over two dimensional arrays combined with
enabled links
optimization results in the best execution time.

% Explain other compile time flags: MAX_LINE LONG_KEYS and SCS INFO MAX_STATE 
The input keywords are given to the program as an input file that contains one keyword per line. The
line separator is an ASCII line feed character (code 10). This adds an additional limitation to the
alphabet, since in practice the decimal value 10 cannot be a part of any given keyword. In Section
\ref{usage} we introduce another input system that allows any character code (except 0) to be used. In
that input method the keywords are merely sampled from the input file and the individual keywords
are not defined. When the input keywords are read, there is a symbol buffer where the individual lines
are first read to. No input keyword can be longer than this buffer. The size of the buffer defaults
to 2048 symbols but can be modified in compile time by setting the desired value for the macro
\texttt{MAX\_LINE}.
Note that the \texttt{MAX\_LINE} macro defines the size of the buffer, not the size of a maximum
keyword, and
that the keywords are read to the buffer with the newline character (that is not part of the keyword)
and that the char arrays must be terminated with the \texttt{NULL} character, the actual maximum
keyword size is \texttt{MAX\_LINE} -2.

Since the value of the depth function $d$ can not exceed the length of the longest keyword(s) that the
AC-machine encodes, we can define the datatype for the codomain of $d$ depending on the maximum
keyword length. The default data type is an 8-bit positive integer, hence by default,
up to 255-length keywords are supported.
When a keyword longer than 255 is used, the data type for the codomain of the depth function
must be larger. By defining the compile-time macro \texttt{LONG\_KEYS} the depth function
uses a 16-bit datatype instead. This allows the maximum keyword length to be $2^{16}-1$ which
should be enough for the RLZ dictionary construction. \ref{joku}

The underlying data type that encodes the numbers of states defaults to a 4-byte unsigned integer. That
means the maximum number of states that the AC-machine can encode is $2^{32}$ which is approximately
4 billion. Since the number of states cannot be known beforehand, the fact that the number of states
is at most the number of symbols in the input is useful. As we use a one-byte alphabet, the size
of the input file should not exceed 4GiB when using the default configuration. The data type
for the state numbers can be changed as follows. There is a macro \texttt{MAX\_STATE} that automatically
defines the data type to be as small as possible. Possible data types are 8, 16, 32 and 64-bit
unsigned integers. Note that it is not necessary to set the \texttt{MAX\_STATES} macro to a smaller value
even if the input size were less than $2^{16}$ bytes. This only affects the memory consumption of
the program which is anyway quite small with input sizes less than 16KiB.

There are also two compile-time options that affects the execution. If the macro \texttt{SCS}
is not defined,
the program only executes Algorithms \ref{ac-goto} and \ref{ac-fail} creating a plain AC-machine
for the input keywords. This option was added in order to compare to the construction of the
AC-machine with another similar implementation \citep{Salmela}. When using the program for approximate
shortest common superstring calculation, the \texttt{SCS} macro must always be defined. The last
compile-time option is the INFO macro. When \texttt{INFO} is defined, additional information
of the calculation
is printed along with the approximate shortest common supertsing. The additional information contains
some used data types, number of removed duplicates in the input keywords, number of different symbols
in the input (real alphabet size), the number of states and a table that lists the most important
phases of the execution and their time usages. Without the INFO macro the program only outputs
the approximate SCS for the input.

The following table summarizes the available compile-time options.\\

%\begin{center}
\hspace{-1.7cm}
  \begin{tabular}{ |l|c|c|l| }
    \hline
    \textbf{Macro name} & \textbf{Possible values} & \textbf{Default value} & \textbf{Notes} \\
 \hline
 ARRAY\_GOTO & defined / undefined & N/A & Either one but not both of this macro \\
 RB\_TREE\_GOTO & defined / undefined & N/A & or this macro must be defined. ~~~ \\
 OPTIMIZE\_LINKS & defined / undefined  & undefined & Enables the links optimization.\\
 MAX\_LINE & Any positive integer & 2048 & The size of the keyword buffer. \\
 LONG\_KEYS & defined / undefined & undefined & Sets max depth to 65536. \\
 MAX\_STATES & Any positive integer & 4294967296 & Should be as small as possible. \\
 SCS & defined / undefined & undefined & Enables a common superstring calculation. \\
 INFO & defined / undefined & undefined & Prints additional information if defined.\\
 \hline
  \end{tabular}
%\end{center}



% details about the leaf calculation ?

\paragraph{Compilation:}~\label{usage}
The compilation and the usage of our implementation requires a C compiler and the GNU Make software.
Since the program uses \texttt{<sys/resource.h>} and \texttt{<sys/time.h>} POSIX header files,
the program can only
be used in a POSIX compliant (e.g. Linux, BSD, MacOs) operating system providing these libraries.
There are no other
requirements for building and running the binary. The default C compiler is GCC and it is defined in the
file \texttt{common.mk}.

There is a top-level Makefile in the root of the repository and other Makefiles in the \texttt{tests/}
and \texttt{src/} folders. Every action can be executed in the repository root since the top level
Makefile further calls the Makefiles in the subfolders. The default action triggers the project build.
When the build is done, the \texttt{target/scs} binary is created and ready to be executed. To build
the binary, either the \texttt{ARRAY\_GOTO} or \texttt{RB\_TREE\_GOTO} macro must be defined.
There is no default value
for the goto function's data structure. Other macros can be defined if needed.
The Makefile in the \texttt{src/} folder, that builds the binary is configured to append every
compilation command with an environment variable named DEFS. This is a convinient way of passing
macro definitions to the compiler. For example, to compile our implementation to calculate the
shortest common superstring approximation that uses the red-black trees, one may issue the following
command.

\begin{minted}{Text}
user@host:.../ukkonen-90/$ make DEFS="-DRB_TREE_GOTO -DSCS"
\end{minted}

To compile the binary with support of keywords of length 1000 and direct indexing with links
optimization enabled and additional information printed one may issue the following command.

\begin{minted}{Text}
user@host:.../ukkonen-90/$ make DEFS="-DARRAY_GOTO -DOPTIMIZE_LINKS -DSCS \
                           -DLONG_KEYS -DMAX_LINE=1002 -DINFO"
\end{minted}

The Makefile also contains jobs called \texttt{clean}, \texttt{clobber} and \texttt{delete\_tests}.
The \texttt{clean} job removes all
intermediate object files. The \texttt{clobber} does the same and also removes the executable in the
\texttt{target/} folder. The \texttt{delete\_tests} target removes the test instances in the
\texttt{tests/}
folder. After that, the next time the tests are executed the new random test instances are
also generated.

\paragraph{Usage:}
When the binary is ready to use there are two methods for executing it. The simplest method is to
execute the binary with a single input file. The file should contain single byte encoded text with
line feed (keycode 10) delimited keywords. The file should end with a line feed. Note that the input
can be any single byte encoded text with the following constraints:

\begin{enumerate}
\item The input keywords must not contain a byte of value 10, since it is reserved for delimiting the
  keywords. Any keyword that contains line feed values are treated as multiple
  keywords.
\item The input file must not contain zero values.
\end{enumerate}

It does not matter whether the text is ASCII, extended ASCII, some single byte ISO- encoding or any
custom made single byte encoding. The following example demonstrates the usage with single input
file as a parameter. In the example the binary is compiled with \texttt{SCS} macro enabled and
\texttt{INFO} macro disabled.

\begin{minted}{Text}
user@host:.../ukkonen-90/$ cat input
baa
baba
abab
aab
user@host:.../ukkonen-90/$ target/scs input
aababaa
\end{minted}

The other way of using our implementation is with command line options that are associated with
flags. The flag \texttt{-f} defines the input file. The flag \texttt{-a} and \texttt{-b} defines
the input types. Only one of the
input types can be specified at a time. The \texttt{-a} flag defines the same kind of input method
we discussed earlier, thus the following command is equivalent to the earlier example:

\begin{minted}{Text}
user@host:.../ukkonen-90/$ target/scs -a -f input
\end{minted}

The input type specified with the \texttt{-b} flag requires two more options and the behavior of
the program is
different. In this case the input file is considered as a source file where the keywords are randomly
sampled as defined by \texttt{-l} (line length) and \texttt{-c} (cut) parameters.
The sampling of the keywords is
performed as follows. The \texttt{-l} parameter defines the length of a single keyword (all keywords have
the same length). The \texttt{-c} parameter defines how many lines are sampled. The cut parameter takes a
floating point value between 0 and 1. The total number of sampled input keywords multiplied by
the length of a single keyword must equal (the value can be rounded) the original input file size
multiplied by the cut parameter. For example, if the input file is 10KB and the \texttt{-l}
parameter has a value
of 100 and the cut parameter has a value of 0.50, then the keywords are sampled such that there
are 50 keywords of length 100 (that is 10KB$\cdot 0.50 = 5$KB). Note that when the
keywords are sampled from rather than delimited in the input file, they can also contain the line
feed characters. After the keywords are sampled it is possible (especially with highly repetitive
input files and big cuts) that there are multiple identical keywords. Before the SCS algorithm
is executed, the duplicates are first removed. The following example
demonstrates the usage with an input file that is sampled to get a set of keywords of lengths 128
such that the size of the sampled keywods equals 10\% of the size of the given input file

\begin{minted}{Text}
user@host:.../ukkonen-90/$ target/scs -b -f input -l 128 -c 0.1
\end{minted}

\paragraph{Tests:}
TODO: should I just ignore the tests in the thesis?

Our implementation has been tested to be correct with three different sets of tests. The tests are
located in the same Git repository as the actual implementation. There is also a helper script for
generating random test instances and scripts that run the actual tests.
All test-related files are in the \texttt{tests/} folder of the
root of the git repository. To run the tests just issue

\begin{minted}{Text}
user@host:.../ukkonen-90/$ make tests
\end{minted}

in the root folder (of the repository). This invokes the Makefile in the \texttt{tests/} folder. If the
test instances have not yet been created, the script \texttt{create\_instance.sh} is executed and two
test folders are generated. Note that depending on your hardware, the generation of the test instances
can be quite slow (up to 30 min).
The first test folder contains random instances for alphanumeric, binary,
DNA and hexadecimal alphabets, with different number and length of keywords. This test set is used
to test that the generated common superstring actually contains every keyword in the test file. This
test is executed with the \texttt{test1.sh} shell script file. The grep utility is used to ensure
that each keyword is present in the output of our implementation. 

The same set of tests is also used with the \texttt{test3.sh} file. In this test we compile two version
of our implementation, one which uses the direct indexing and another that uses the red-black tree
implementation. The tests instances are then ran with each version of the implementation and for each
test, the test is passed if the generated superstrings are equal. In othre words the idea of this
test is to ensure that the data structure that is used with the goto function does not affect
the outcome.

The second set of test instances is rather small. There are only few very small instances with binary
alphabet. The test script \texttt{test2.sh} runs these tests as follows. First each test instance is
executed with a python script \texttt{scs.py} that uses the \texttt{pysat} python library to calculate
the length of a longest common superstring of the instance. The same instance is then ran with our
implementation and the compression values for both are calculated. Finally the compressions is compared
and the test is considered to be passed if the compression of acquired with our implementation is
at least half of the optimal compression. The idea of this test set is to make sure that the proven
quality of the compression holds.

Note that in order to run all the tests the python environment with the pysat library must be configured.
Note also that the make format uses GNU specific features that requires GNU Make or or GNU Make compliant
makefile software. 


%% - Differences !!!!

\section{Benchmark Data}
Here we describe the two datasets that are use in Section \ref{sec-results}. We discuss the origin
of both of those datasets and inspect their basic properties. Since the data is slightly manipulated
to fit our experiments we also describe the preprocessing that had been done.

\paragraph{PizzaChili datasets:}
PizzaChili\ref{PizzaChili} is a deprecated project of University of Pisa and University of Chile,
whose purpose is to
share publicly available corpuses and related full-text indices for others to experiment with.
The dataset defined here consists of some of the repetitive text collections from PizzaChili project.
We refer to this dataset as the PizzaChili dataset.
There are seven files of repetitive text from real (as opposed to artificial) origin in the
PizzaChili dataset. The following listing gives a short description of each of those files. The table
under the listing includes some basic information of the files. The inverse match probability is defined
as the inverse of the probability that two randomly chosen characters from the file are the same. This
is also referred to the effective alphabet size. With uniformly distributed data, the effective and
actual alphabet sizes are exactly the same.

\begin{enumerate}
\item The \textbf{cere} file consists of 17 concatenated sequences of Saccharomyces Cerevisiae. The origin of this file is the Saccharomyces Genome Resequencing Project.
\item The \textbf{coreutils} file consists of 9 versions of the coreutils 5.x source code files.
\item The \textbf{einstein.de.txt} file consists of all German wikipedia versions of Albert Einstein up to 2010-01-12.
\item  The \textbf{einstein.en.txt} file consists of all English wikipedia versions of Albert Einstein up to 2006-10-10.
\item The \textbf{influenza} file consists of 78,041 sequences of Haemophilus Influenzae. The origin of this file is the National Center for Biotechnology Information.
\item The \textbf{kernel} file consists of 36 versions of the Linux kernel source files of versions 1.0.x and 1.1.x.
\item The \textbf{para} file consists of 36 concatenated sequences of DNA of Saccharyomyces Paradoxus. The origin of this file is the Saccharomyces Genome Resequencing Project.
\end{enumerate}


\begin{figure}
\begin{center}
  \begin{tabular}{|l|c|c|c|}
    \hline
    \textbf{File} & \textbf{Size(MiB)} & \textbf{Alphabet size} & \textbf{Inverse match probability} \\
    \hline
    cere & 440 & 5 & 4.301 \\
    coreutils & 196 & 236 & 19.553 \\
    einstein.de.txt & 89 & 117 & 19.264 \\
    einstein.en.txt & 446 & 139 & 19.501 \\
    influenza & 148 & 15 & 3.845 \\
    kernel & 247 & 160 & 23.078 \\
    para & 410 & 5 & 4.096\\
    \hline
  \end{tabular}
  \caption{Basic information of the PizzaChili files. \ref{PizzaChili}}
\end{center}
\end{figure}


\paragraph{Preprocessing of PizzaChili:}
Each file in the dataset is preprocessed as follows. First, the newline characters are substituted
with tab characters (ASCII key code 9). After that a number of new files are sampled from the files
such that the new file consists of multiple lines delimited by a newline character. Each line has a
length $l$ and the total (rounded) number of characters, excluding the newlines, are $0.01c\cdot s$,
where $s$ is the size of the original file. In other words, the $c$ parameter describes the percentage
of the size of the original file. The parameters $c$ and $l$ have the following value
combinations: (15, 128), (15, 254), (25, 254), (50, 254), (50, 512), (75, 512), thus at total six
sampled files are generated. The sampling is performed by selecting a random position between
the beginning of the file and the end of the file subtracted by $l$ and copying a string of length
$l$ from that point to the sample file. This process is then repeated until the wanted file size is
achieved. The possible duplicate lines are removed and the process is repeated until the wanted
number of lines is reached. Finally we have a sample file that contains unique keywords of length
$l$ such that the size of all keywords combined is $c\%$ of the original size. The files are
named \texttt{s\_c\_l}. For example the file \texttt{cere\_50\_128} has size 220MB
(plus the number of newlines) and contains unique keywords of length 128.

\paragraph{The Dna dataset:}
The second dataset is called the Dna dataset. The origin of this dataset is xxxx xxxx xxxx.
It contains xxxx lines of 100 xxxxx. The alphabet $\Sigma=\{A,C,G,T,N\}$ size of the original file is 5.
The 'N' characters were relatively rare in this file (3506 / 3849544 lines).
Since the PizzaChili dataset does not have a
file with alphabet size less than 5, the Dna dataset was modified by removing all lines containing
the 'N' character, resulting in a file with alphabet of size 4.
Further, the duplicates were removed from the data and the data was randomly sampled to 11 different
files as follows. The first file was from the main file such that the number of lines was 3125, thus
the first has a size of $3125+3125\cdot 100 = 315625$ bytes. The first file was named
\texttt{1\_dna\_3125.acgt}. The process was repeated for the last ten files such that the number
of lines for each file was two times the number of lines in the previous file.
The same naming convention
was used with the exception that each consecutive set of three zeros was substituted with the letter k.
The first character in the file name is a hexadecimal digit of the number of the file.
For example, the third file was named to \texttt{3\_dna\_12500.acgt}, the fourth file was named
to \texttt{4\_dna\_25k.acgt} and the last file was named to \texttt{B\_dna\_3200k.acgt}

\section{Benchmark Setups}

The experiments were done in two different environments. In this section we describe the relevant
aspects of the used software and hardware for both of them. We also name the environments so they
can be referred to in Section \ref{sec-results}. The first environment is called Haapa. It is the more
powerful of the environments providing, due to its larger amount of memory, a possibility to run
much bigger problem instances. The second
environment is referred to as Env2. For running
our experiments, Haapa is however the slower one of the two environments. Although Haapa has many times
more CPUs, the single CPU is significantly slower. Since the code used in the experiments
are all single-threaded, the performance of a single core is all that matters. The memory
clock speed in Haapa is also configured to be significantly slower, thus limiting the performance
of our implementation with huge memory requirements. Less experiments are run with Env2 since the
memory of the machine provided a limitation to the size of the problem instances.
Table \ref{tbl-sys} describes the main features of both environments.

\begin{table}
\begin{tabular}{l|r|r}
  & \textbf{Haapa} & \textbf{Env2} \\
  \hline
  Operating system & Ubuntu Linux 20.04 LTS & Gentoo Linux\footnote{Gentoo Linux is a so called rolling release distribution. That means it does not have any version numbers.} \\
  Linux kernel version & 5.11.0 & 5.4.80 \\
  GCC version & 9.4.0 & 10.2.0 \\
  Glibc version & 2.31 & 2.32 \\
  GNU Time version & 1.7 & 1.9 \\
  \hline
  Processor model & Intel Xeon E7-4830 v3 & Intel Core i7-6700K \\
  Processor clock speed & 2.1 GHz & 4.0 GHz \\
  Processor cache size & 30 MiB & 8 MiB \\
  Memory density & 1.5 TiB & 32 GiB \\
  Configured memory clock speed & 1333 MHz & 2133 MHz \\
\end{tabular}
\caption{Software and hardware specifications of the environments Haapa and Env2.}
\label{tbl-sys}
\end{table}


%- gnu time

%sed -i 's/\x01/\x02/g' coreutils-modified_0.15_128
% sed -i 's/</[/g' data/real_15_128/coreutils-modified_0.15_128
\section{Results}~\label{sec-results}
In this section we examine the performance of our implementation. As a reference we use another
implementation of the same greedy heuristic of the approximate shortest common superstring problem.
The reference implementation \citep{Alanko19} performs the computation in a space efficient way using
only $O(n \cdot log(|\Sigma|))$ bytes of memory. The time complexity of the reference implementation
is also $O(n \cdot log(|\Sigma|))$. In contrast, our implementation of Ukkonen's algorithm
works in linear time when the code is compiled to use direct indexing for the operations with
the goto function. Our implementation also has a possibility to use red-black trees for the goto
operations, thus both of these approaches were tested.
%This difference in the time complexities can be seen especially with a large input sizes.
In addition, as discussed in Section
\ref{sec-our}, our implementation has an additional compile-time parameter that defines the data
structure used for the children list of the states. As it is, there are four different versions of
our implementation, at total:

\begin{enumerate}
\item Direct indexing ($O(n)$) with links optimization enabled.
\item Dicect indexing ($O(n)$) with links optimization disabled.
\item Red-black tree ($O(n\cdot log(n))$) with links optimization enabled.
\item Red-black tree ($O(n\cdot log(n))$) with links optimization disabled.
\end{enumerate}

For convinience we refer these versions as DIE, DID, RBE and RBD, respectively. All of the results,
from our as well as the reference implementation,
represented in this section are from binaries that have been compiled with the same C
compiler (gcc) with the same optimization settings. The predefined optimization set \texttt{-O3} have
been used as well as the linking time optimization \texttt{-flto}. The following process was executed
with all presented time benchmarks. Each experiment was repeated five times. For
the five execution times we picked the median value. After that, if there were any values with 
at least 5 percent difference, those values were removed and the new median value was picked.
If there was an even number of values left, the bigger one of the two middle values were chosen.
The idea behind this procedure was to eliminate the possible outliers from the results. Most often
there were no outliers and the presented value was acquired immediately. For the PizzaChili files
with keyword lengths $\leq 256$ there was $31/700$ discarded values when time usages were
measured in Haapa.

\paragraph{Preprocessing time:}
As discussed in Section \ref{sec-our}, our program uses the $O(n\cdot log(n))$ string quicksort
algorithm to remove the possible duplicate input keywords. By compiling our program with the
INFO macro defined, we get, among other things, an analysis of the time spent in different phases
of the program. Referring to Tables \ref{runtimes-15-128}, \ref{runtimes-15-254},
\ref{runtimes-25-254} and \ref{runtimes-50-254}, the RBD implementation was the fastest one
to process any sampled \texttt{einstein.en.txt}
file. Correspondingly the DIE version was the fastest one for any \texttt{cere} file. In Table
\ref{table-duplicate-time}
we show the time portion of our implementations (DIE for \texttt{cere} and RBD for
\texttt{einstein.en.txt}) that were
used for the duplicate removal as a percentage of the total runtime. Since the used  implementation
versions are the fastest ones for the particular instances, the time portions for the duplicate
removals are maximal. Table \ref{table-duplicate-time} shows the worst case time percentages
for the preprocessing phase.

\begin{table}
\begin{centering}
\begin{tabular}{l | c c}
  & \textbf{cere} & \textbf{einstein.en.txt} \\
  75\_512 & 0.36\% & 3.54\% \\
  50\_512 & 0.33\% & 3.06\% \\
  50\_254 & 0.71\% & 4.87\% \\
  25\_254 & 0.59\% & 4.04\% \\
  15\_254 & 0.54\% & 3.25\% \\
  15\_128 & 1.06\% & 4.77\% \\
\end{tabular}
  \caption{Worst case time consumptions in percentage for the input preprocessing.}
  \label{table-duplicate-time}
\end{centering}
\end{table}
Investigating Table \ref{table-duplicate-time}, a few observations can be made.

\begin{enumerate}
\item With the same line length, increasing the input size increases the time portion required to preprocess the input. This is expected since the preprocessing part of the program works in $O(n\cdot log(n))$ time, which is more than the implementation of the greedy heuristic.
\item With the same input size, an increase in the keyword length causes the relative preprocessing time to decrease. 
\end{enumerate}

As we see, the maximum relative preprocessing time for the cere files is 1.06\% of the total runtime.
The results were similar for all instances with a small ($<12$) alphabets. Based on this data,
it is reasonable to say that the preprocessing time of the program is insignificant for instances
with a small alphabet while the instance sizes are in the ranges of our experiment. For bigger
alphabets ($|\Sigma|>12$) the relative preprocessing time is greater. However, in our experiments
the time required for the preprocessing with large alphabets was at most around 5 per cent. One may
interpret this value to be significant, however while comparing our implementation with large
inputs to the reference implementation, the differences with the relative time usages are usually
significantly more than 5\%.


\paragraph{Performance comparisons:}
The performance of our implementation and the reference implementation was compared. The acquired
time usages were measured using the GNU time software. For the PizzaChili dataset, all files with line
lengths of 128, 254 and 512 were investigated. This experiment was run in Haapa. For the Dna dataset,
the comparisons were run for each file. This experiment was also run in Env2 for all but the
biggest problem instance, since the memory of Env2 was not sufficient for that.

The runtimes for the PizzaChili dataset files with line length $\leq$ 254 are specified in
Tables \ref{runtimes-15-128},
\ref{runtimes-15-254}, \ref{runtimes-25-254} and \ref{runtimes-50-254}. Each table shows the
runtimes for the set of each 7 files sampled with the same parameters. The File column is the
prefix of the used filename. For examples the files in Table \ref{runtimes-15-128} are
\texttt{cere\_15\_128}, \texttt{coreutils\_15\_128} and so on. The column labeled Reference shows
the runtimes with the reference implementation. Colums labeled with DIE, DID, RBE and RBD shows the
runtimes of our implementation versions. The best runtimes for each input file are bolded.
The last two columns are discussed later.

\begin{center}
  \begin{table}
  \begin{tabular} {| l |r r r r r|l l|}
    \hline
    \textbf{File} & \textbf{Reference} & ~~~~\textbf{DIE} & ~~~~\textbf{DID} & ~~~~\textbf{RBE} & ~~~~\textbf{RBD} & \textbf{-Comp.} & \textbf{Rsize} \\
    \hline
    cere & 146.00 & \textbf{91.65} & 100.44 & 115.76 & 120.89            & 0.319  & 0.0478  \\
    coreutils & 73.23 & 105.04 & 74.24 & 104.80 & \textbf{51.27}         & 0.463  & 0.0694  \\
    einstein.de.txt & 37.76 & 29.11 & 23.95 & 29.79 & \textbf{18.13}     & 0.0463 & 0.00694 \\
    einstein.en.txt & 246.17 & 134.09 & 110.58 & 135.21 & \textbf{85.12} & 0.0269 & 0.00403 \\
    influenza & 40.62 & 29.97 & \textbf{29.79} & 34.35 & 32.92           & 0.338  & 0.0507  \\
    kernel & 114.56 & 109.74 & 85.58 & 111.53 & \textbf{65.13}           & 0.244  & 0.0366  \\
    para & 122.66 & \textbf{86.50} & 94.77 & 110.11 & 116.21             & 0.387  & 0.0580  \\
    \hline
  \end{tabular}
  \caption{Runtimes in seconds for PizzaChili dataset with cut 0.15 and keyword length 128, compression, relative size of the dictionary. The fastest runtimes are bolded.}
  \label{runtimes-15-128}
  \end{table}
\end{center}

\begin{center}
  \begin{table}
  \begin{tabular} {| l |r r r r r|l l|}
    \hline
    \textbf{File} & \textbf{Reference} & ~~~~\textbf{DIE} & ~~~~\textbf{DID} & ~~~~\textbf{RBE} & ~~~~\textbf{RBD} & \textbf{-Comp.} & \textbf{Rsize} \\
    \hline
    cere & 144.06 & \textbf{102.97} & 113.97 & 129.63 & 136.11            & 0.409  & 0.0614  \\
    coreutils & 69.29 & 112.36 & 78.83 & 107.45 & \textbf{55.21}          & 0.527  & 0.0791  \\
    einstein.de.txt & 39.44 & 30.27 & 24.44 & 30.05 & \textbf{18.54}      & 0.0652 & 0.0098  \\
    einstein.en.txt & 278.75 & 154.52 & 129.48 & 154.96 & \textbf{102.88} & 0.0268 & 0.00402 \\
    influenza & 38.75 & 33.03 & \textbf{32.76} & 38.23 & 37.28            & 0.463  & 0.0695  \\
    kernel & 113.62 & 117.58 & 91.70 & 117.96 & \textbf{71.77}            & 0.269  & 0.0403  \\
    para & 119.23 & \textbf{97.25} & 107.02 & 123.52 & 129.05             & 0.445  & 0.0668  \\
    \hline
  \end{tabular}
  \caption{Runtimes in seconds for PizzaChili dataset with cut 0.15 and keyword length 254, compression, relative size of the dictionary. The fastest runtimes are bolded.}
  \label{runtimes-15-254}
  \end{table}
\end{center}

\begin{center}
  \begin{table}
  \begin{tabular} {| l |r r r r r|l l|}
    \hline
    \textbf{File} & \textbf{Reference} & ~~~~\textbf{DIE} & ~~~~\textbf{DID} & ~~~~\textbf{RBE} & ~~~~\textbf{RBD} & \textbf{-Comp.} & \textbf{Rsize} \\
    \hline
    cere & 294.24 & \textbf{175.33} & 191.33 & 216.54 & 221.55            & 0.316  & 0.0790  \\
    coreutils & 145.94 & 187.77 & 131.09 & 180.38 & \textbf{95.82}        & 0.426  & 0.106   \\
    einstein.de.txt & 73.96 & 52.18 & 42.56 & 52.80 & \textbf{33.40}      & 0.0442 & 0.0110  \\
    einstein.en.txt & 483.52 & 244.33 & 202.77 & 246.41 & \textbf{159.36} & 0.0210 & 0.00524 \\
    influenza & 80.12 & \textbf{57.89} & 59.75 & 65.41 & 63.83            & 0.410  & 0.103   \\
    kernel & 235.27 & 194.60 & 154.91 & 198.07 & \textbf{118.92}          & 0.182  & 0.0455  \\
    para & 253.59 & \textbf{163.89} & 179.08 & 208.02 & 214.48            & 0.348  & 0.0870  \\
    \hline
  \end{tabular}
  \caption{Runtimes in seconds for PizzaChili dataset with cut 0.25 and keyword length 254, compression, relative size of the dictionary. The fastest runtimes are bolded.}
  \label{runtimes-25-254}
  \end{table}
\end{center}

\begin{center}
  \begin{table}
  \begin{tabular} {| l |r r r r r|l l|}
    \hline
    \textbf{File} & \textbf{Reference} & ~~~~\textbf{DIE} & ~~~~\textbf{DID} & ~~~~\textbf{RBE} & ~~~~\textbf{RBD} & \textbf{-Comp.} & \textbf{Rsize} \\
    \hline
    cere & 715.62 & \textbf{350.59} & 384.95 & 431.81 & 439.19            & 0.213  & 0.106   \\
    coreutils & 377.27 & 370.95 & 261.72 & 358.27 & \textbf{188.88}       & 0.289  & 0.145   \\
    einstein.de.txt & 171.43 & 99.20 & 83.85 & 101.56 & \textbf{67.74}    & 0.0255 & 0.0127  \\
    einstein.en.txt & 993.62 & 463.47 & 371.48 & 456.26 & \textbf{294.33} & 0.0157 & 0.00786 \\
    influenza & 197.47 & \textbf{118.44} & 120.73 & 129.16 & 128.45       & 0.335  & 0.167   \\
    kernel & 569.85 & 392.67 & 306.08 & 392.72 & \textbf{241.98}          & 0.104  & 0.0519  \\
    para & 629.26 & \textbf{336.03} & 361.32 & 403.99 & 424.42            & 0.237  & 0.119   \\
    \hline
  \end{tabular}
  \caption{Runtimes in seconds for PizzaChili dataset with cut 0.50 and keyword length 254, compression, relative size of the dictionary. The fastest runtimes are bolded.}
  \label{runtimes-50-254}
  \end{table}
\end{center}

As discussed in Section \ref{sec-our}, the links optimization was designed to improve the performance
of our implementation with small alphabets. In general, also the direct indexing over two
dimensional array is only reasonable with small alphabets, since it significantly increases
the memory consumption of the program. Also, in practice, our implementations with red-black trees
were faster with big alphabets. Investigating the runtimes from Tables \ref{runtimes-15-128},
\ref{runtimes-15-254}, \ref{runtimes-25-254} and \ref{runtimes-50-254} we make the following observation.

\begin{enumerate}
\item The DIE version was the fastest for all instances with small ($<12$) alphabets.
\item The RBD version was the fastest for all instances with big ($>12$) alphabets.
\item The DID version was slightly ($<1\%$) faster than the DIE version of for the files \texttt{influensa\_128\_15} and \texttt{influensa\_128\_25} ($|\Sigma| = 12$).
\item The RBE version was the fastest for none of the instances.
\end{enumerate}

As expected, the DID and RBE versions are generally not competitive with the DIE
and RBD versions. In practice, we can limit the number of our implementations to two.
Later on, we only present the results for either DIE (when alphabet size $\leq$ 12)
or RBD (when alphabet size $>12$) versions of our implementation.

% refinetty tähän asti.
Let us now examine the runtimes more carefully for the instances with keyword lengths 254.
Table \ref{tbl-graphs} illustrates the runtimes for each instance in PizzaChili dataset
with keyword length 254.
Thus, each plot uses the data from Tables
\ref{runtimes-15-254}, \ref{runtimes-25-254} and \ref{runtimes-50-254}.
Note that since the data points are not uniformly distributed in the x axis,
these graph does not illustrate the time complexities of the programs. 
The x marked blue lines and cirle marked black lines show the time usages of the reference
implementation and our implementation, respectively. Runtimes in seconds are presented in the
left vertical axis. As we see, the time usage of the reference implementation grows
more rapidly in every case. This relative change in runtimes is illustrated with
the orange triangel marked lines. The presented values are the percentages of the
time usage of our implementation compared to the time usages with the reference
implementation. The percentage values are shown in the right vertical axis.
This data suggest that our implementation outperforms the reference implementation
with big enough inputs. However, investigating Tables \ref{runtimes-15-128} and
\ref{runtimes-15-254} we also see
that increasing the line length while keeping the instance size constant affects
the runtimes contrarily. For every file, the time usage of our implementation
increases when the line length is increased from 128 to 254 while keeping the
cut in 15 percent. With the reference implementation, the time usage grows only
with the einstein files. Moreover, the growth is relatively smaller than the growth with
our implementation. By this data, we assume that there are instances similarly sampled
from PizzaChili data for which the reference implementation is faster.

Tables \ref{runtimes-50-512} and \ref{runtimes-75-512} show similar data for the PizzaChili
dataset with line lengths 512. These results are consistent with the runtimes of the smaller instances.
That is, our implementation compared to the reference implementation gets faster when the size of
the instances grows while the keyword length remains constant. Also, the relative speed of our
versus the reference implementation decreases while the input size remains constant and the
line length increases. 

\vspace{-1cm}
\begin{center}
  \begin{table}[h!]
  \begin{tabular} {| l |r r|l l|}
    \hline
    \textbf{File} & \textbf{Reference}~ & ~~\textbf{Our impl.} & \textbf{Compression}~~~ & \textbf{Rsize}~~~ \\
    \hline
    cere            & 707.99  & \textbf{436.36}  & 0.299  & 0.149   \\
    coreutils       & 0.00    & \textbf{0.00}    & 0.000  & 0.000   \\
    einstein.de.txt & 184.57  & \textbf{80.48}   & 0.0385 & 0.0192  \\
    einstein.en.txt & 1147.92 & \textbf{392.66}  & 0.0181 & 0.00903 \\
    influenza       & 185.32  & \textbf{147.45}  & 0.457  & 0.228   \\
    kernel          & 567.09  & \textbf{281.81}  & 0.123  & 0.0613  \\
    para            & 616.93  & \textbf{417.35}  & 0.296  & 0.148   \\
    \hline
  \end{tabular}
  \caption{Runtimes in seconds for PizzaChili dataset with cut 0.50 and keyword length 512, compression, relative size of the dictionary. The fastest runtimes are bolded.}
  \label{runtimes-50-512}
  \end{table}
\end{center}

%\afterpage{%
%\thispagestyle{empty}
\begin{table}[H]
  \vspace{-1.1cm}
\hskip-1.2cm\resizebox{1.1\columnwidth}{!}{%
  \begin{tabular}{c@{\hskip 0.5in}c}


    \vspace{0.5cm}
    
    \begin{minipage}{.5\textwidth}
\begin{tikzpicture}
	\begin{axis}[
		ylabel=\texttt{a) cere},
		xlabel=,
                xmax=100,
                xmin=0,
                xtick={5, 50, 95},
                xticklabels={$15$, $25$, $50$}
          ]
        % cere
	\addplot[color=blue,mark=x] coordinates {
          (5, 144)
          (50, 294)
          (95, 716)
	};
        \addplot[color=black,mark=*] coordinates {
	  (5, 103)
	  (50, 175)
	  (95, 351)
        };

	\end{axis}
        \begin{axis}[
            xlabel=,
            ylabel=,
            xmax=100,
            xmin=0,
            xtick={5, 50, 95},
            xticklabels={,,},
            xtick style={draw=none},
            ytick={71, 60, 49},
            ylabel near ticks, yticklabel pos=right
          ]
          % cere
          \addplot[color=orange, mark=triangle] coordinates {
            (5, 71)
            (50, 60)
            (95, 49)
          };
          
        \end{axis}
\end{tikzpicture}
    \end{minipage} &

\begin{minipage}{.5\textwidth}
\begin{tikzpicture}
	\begin{axis}[
		ylabel=,
		xlabel=,
                xmax=100,
                xmin=0,
                xtick={5, 50, 95},
                xticklabels={$15$, $25$, $50$}
          ]
          % coreutils
	\addplot[color=blue,mark=x] coordinates {
	  (5,69)
	  (50,146)
	  (95,377)
	};
        \addplot[color=black,mark=*] coordinates {
          (5, 55)
          (50, 96)
          (95, 189)
        };
	\end{axis}
        \begin{axis}[
            xlabel=,
            ylabel=\texttt{b) coreutils},
            xmax=100,
            xmin=0,
            xtick={5, 50, 95},
            xticklabels={,,},
            xtick style={draw=none},
            ytick={78, 66, 50},
            ylabel near ticks, yticklabel pos=right
          ]
          % coreutils
          \addplot[color=orange, mark=triangle] coordinates {
            (5, 78)
            (50, 66)
            (95, 50)
          };
        \end{axis}
\end{tikzpicture}
\end{minipage} \\

\vspace{0.5cm}
    
\begin{minipage}{.5\textwidth}
  \begin{tikzpicture}
    \begin{axis}[
		ylabel=\texttt{c) einstein.de.txt},
		xlabel=,
                xmax=100,
                xmin=0,
                xtick={5, 50, 95},
                xticklabels={$15$, $25$, $50$}
          ]
        % einstein de
	\addplot[color=blue,mark=x] coordinates {
          (5, 39)
          (50, 74)
          (95, 171)
	};
        \addplot[color=black,mark=*] coordinates {
	  (5,19)
	  (50,33)
	  (95,68)
        };        

	\end{axis}
        \begin{axis}[
            xlabel=,
            ylabel=,
            xmax=100,
            xmin=0,
            xtick={5, 50, 95},
            xticklabels={,,},
            xtick style={draw=none},
            ytick={80, 66, 50},
            ylabel near ticks, yticklabel pos=right
          ]
          % einstein de
          \addplot[color=orange, mark=triangle] coordinates {
            (5, 80)
            (50, 66)
            (95, 50)
          };
          
        \end{axis}
        
\end{tikzpicture}
\end{minipage} &

    
\begin{minipage}{.5\textwidth}
\begin{tikzpicture}
	\begin{axis}[
		ylabel=,
		xlabel=,
                xmax=100,
                xmin=0,
                xtick={5, 50, 95},
                xticklabels={$15$, $25$, $50$},
                ytick={200, 400, 600, 800, 999}
          ]

        % einstein en
	\addplot[color=blue,mark=x] coordinates {
	  (5, 246)
	  (50, 484)
	  (95, 994)
	};
        \addplot[color=black,mark=*] coordinates {
          (5, 103)
          (50, 159)
          (95, 294)
        };

	\end{axis}
        \begin{axis}[
            xlabel=,
            ylabel=\texttt{d) einstein.en.txt},
            xmax=100,
            xmin=0,
            xtick={5, 50, 95},
            xticklabels={,,},
            xtick style={draw=none},
            ytick={42, 33, 30},
            ylabel near ticks, yticklabel pos=right
          ]
          % einstein en
          \addplot[color=orange, mark=triangle] coordinates {
            (5, 42)
            (50, 33)
            (95, 30)
          };

        \end{axis}
        
\end{tikzpicture}
\end{minipage} \\

\vspace{0.5cm}

\begin{minipage}{.5\textwidth}
\begin{tikzpicture}
	\begin{axis}[
		ylabel=\texttt{e) influenza},
		xlabel=,
                xmax=100,
                xmin=0,
                xtick={5, 50, 95},
                xticklabels={$15$, $25$, $50$}
          ]

        % influenza
	\addplot[color=blue,mark=x] coordinates {
          (5, 39)
          (50, 80)
          (95, 197)
	};
        \addplot[color=black,mark=*] coordinates {
	  (5, 33)
	  (50, 59)
	  (95, 118)
        };        

	\end{axis}
        \begin{axis}[
            xlabel=,
            ylabel=,
            xmax=100,
            xmin=0,
            xtick={5, 50, 95},
            xticklabels={,,},
            xtick style={draw=none},
            ytick={85, 74, 60},
            ylabel near ticks, yticklabel pos=right
          ]

          % influenza
          \addplot[color=orange, mark=triangle] coordinates {
            (5, 85)
            (50, 74)
            (95, 60)
          };                    
          
        \end{axis}
\end{tikzpicture}
\end{minipage} &

\begin{minipage}{.5\textwidth}
\begin{tikzpicture}
	\begin{axis}[
		ylabel=,
		xlabel=,
                xmax=100,
                xmin=0,
                xtick={5, 50, 95},
                xticklabels={$15$, $25$, $50$}
          ]

        % kernel
	\addplot[color=blue,mark=x] coordinates {
          (5, 114)
          (50, 235)
          (95, 570)
	};
        \addplot[color=black,mark=*] coordinates {
	  (5, 72)
	  (50, 119)
	  (95, 242)
        };

	\end{axis}
        \begin{axis}[
            xlabel=,
            ylabel=\texttt{f) kernel},
            xmax=100,
            xmin=0,
            xtick={5, 50, 95},
            xticklabels={,,},
            xtick style={draw=none},
            ytick={63, 51, 42},
            ylabel near ticks, yticklabel pos=right
          ]

          % kernel
          \addplot[color=orange, mark=triangle] coordinates {
            (5, 63)
            (50, 51)
            (95, 42)
          };          

        \end{axis}
        
\end{tikzpicture}
\end{minipage} \\

\vspace{0.5cm}

\begin{minipage}{.5\textwidth}
\begin{tikzpicture}
	\begin{axis}[
		ylabel=\texttt{g) para},
		xlabel=,
                xmax=100,
                xmin=0,
                xtick={5, 50, 95},
                xticklabels={$15$, $25$, $50$}
          ]

        % para
        \addplot[color=blue,mark=x] coordinates {
          (5, 119)
          (50, 254)
          (95, 629)
	};
        \addplot[color=black,mark=*] coordinates {
	  (5, 97)
	  (50, 164)
	  (95, 336)
        };
	\end{axis}
        \begin{axis}[
            xlabel=,
            ylabel=,
            xmax=100,
            xmin=0,
            xtick={5, 50, 95},
            xticklabels={,,},
            xtick style={draw=none},
            ytick={82, 65, 53},
            ylabel near ticks, yticklabel pos=right
          ]

          % para
          \addplot[color=orange, mark=triangle] coordinates {
            (5, 82)
            (50, 65)
            (95, 53)
          };
        \end{axis}
\end{tikzpicture}
\end{minipage} &

\hspace{0.6cm}
\begin{minipage}{.5\textwidth}
  \caption{Illustration of the runtimes of the PizzaChili files with 254 line length. The blue lines
    marked with x show the runtimes of the reference implementation. Black lines market with * show the
    runtimes of our implementation. Runtimes are shown in the left vertical axis. Orange lines marked
    with triangles show the percentage of the runtime of our implementation compared to the reference
    implementation. Percentage values are shown in the right vertical axis. The horizontal axis
    shows the cut parameter of the sampled input file.}
  \label{tbl-graphs}    
\end{minipage}

  \end{tabular}%
}
  %\pagenumbering{gobble}
  %\pagenumbering{arabic}
\end{table}%
%}


\begin{center}
  \begin{table}
  \begin{tabular} {| l |r r|l l|}
    \hline
    \textbf{File} & \textbf{Reference}~ & ~~\textbf{Our impl.} & \textbf{Compression}~~~ & \textbf{Rsize}~~~ \\
    \hline
    cere            & 1162.88 & \textbf{665.73} & 0.237  & 0.178 \\
    coreutils       & 0.00    & \textbf{0.00}   & 0.256  & 0.192 \\
    einstein.de.txt & 303.83  & \textbf{126.08} & 0.0277 & 0.021 \\
    einstein.en.txt & 1744.57 & \textbf{560.52} & 0.0146 & 0.011 \\
    influenza       & 324.12  & \textbf{221.69} & 0.399  & 0.299 \\
    kernel          & 946.69  & \textbf{432.82} & 0.0881 & 0.066 \\
    para            & 1033.05 & \textbf{634.22} & 0.237  & 0.178 \\
    \hline
  \end{tabular}
  \caption{Runtimes in seconds for PizzaChili dataset with cut 0.75 and keyword length 512, compression, relative size of the dictionary. The fastest runtimes are bolded.}
  \label{runtimes-75-512}
  \end{table}
\end{center}


% esitellään vastaavat tulokset dna datalle. Todedaan että lineaarisuus on hyvin näkyvissä
  Let us now examine the runtime benchmarks for the Dna dataset. This experiment was run in both
  environments. The results are shown in Table \ref{runtimes-dna}. The first column of the table,
  labeled with \# defines the prefix of the filename. E.g. the row starting with 5 corresponds the
  file \texttt{5\_dna\_5k.acgt}, which consists of 5000 lines of DNA sequences, 100 bases in each
  line.
  
  The columns labeled Reference and DIE show the runtimes in seconds.
  Due to the high memory usage of our implementation, the last file could not be run in Env2.
  For both environment, there is
  also a Percentage column, which shows the runtimes of our implementation in percentages compared
  to the runtimes of the reference implementation. The last row contains the compression ratio of the
  generated superstring, that is the size of the superstring divided by the size of the input instance.
  Note that the achieved compression is a result of the greedy heuristic, which is used by both
  of the implementations. Hence, the compression is the same to the number of significant digits
  represented in the table.

  From the data, we can observe that the runtimes are significantly higher in Haapa. This result is
  expected, since Haapa has an older CPU with lower clock speed. The memory speed in Haapa is
  also lower. With the smaller instances, the reference implementation has a lower runtimes.
  In Haapa, the reference implementation is faster with instance sizes $\leq$ 200,000 lines.
  In Env2 this treshold is 100,000 lines. The corresponding instance sizes in mebibytes are
  20 and 10, respectively. After that, the performance of our implementation
  overcomes the performance of the reference implementation.

\begin{center}
  \begin{table}
  \hspace{-1cm}
  \begin{tabular} {|l|r r r|r r r|l|}
    \hline
     & \multicolumn{3}{c}{Haapa} & \multicolumn{3}{c}{Env2} & \\
     \textbf{\text{\#}} & \textbf{Reference} & \textbf{DIE} & \textbf{Percentage}  & \textbf{Reference} & \textbf{DIE} & \textbf{Percentage} & \textbf{Comp.} \\
     \hline
     1 & 0.24            & \textbf{0.21}   & 87.5   & \textbf{0.09}  & \textbf{0.09}   & 100.0           & 0.927 \\
     2 & \textbf{0.45}   & \textbf{0.45}   & 100.0  & \textbf{0.19}  & 0.21            & 110.5           & 0.909 \\
     3 & \textbf{0.88}   & 0.97            & 119.2  & \textbf{0.39}  & 0.5             & 128.2           & 0.887 \\
     4 & \textbf{1.83}   & 1.94            & 106.0  & \textbf{0.88}  & 1.22            & 138.6           & 0.840 \\
     5 & \textbf{4.06}   & 4.65            & 114.5  & \textbf{2.03}  & 2.85            & 140.4           & 0.769 \\
     6 & \textbf{9.56}   & 10.98           & 114.9  & \textbf{5.46}  & 6.25            & 114.5           & 0.660 \\
     7 & \textbf{23.45}  & 24.53           & 104.6  & 16.12          & \textbf{13.14}  & 81.51           & 0.512 \\
     8 & 62.02           & \textbf{51.07}  & 82.3   & 44.63          & \textbf{26.47}  & 59.4            & 0.362 \\
     9 & 167.34          & \textbf{102.38} & 61.6   & 107.72         & \textbf{51.92}  & 48.2            & 0.254 \\
     A & 412.7           & \textbf{202.41} & 49.0   & 247.37         & \textbf{104.78} & 42.4            & 0.191 \\
     B & 953.86          & \textbf{406.2}  & 42.6   &                &                 &                 & 0.155 \\
\hline

  \end{tabular}
  \caption{Runtimes in seconds for the Dna dataset, the runtime of our implementation compared to the reference implementation (in percentages) in both environments, compression. The fastest runtimes for each environment are bolded.}
  \label{runtimes-dna}
  \end{table}
\end{center}

To illustrate the runtimes of our and the reference implementations we also sampled a set of
instances with linearly increasing size from 100,000 lines to 2,000,000 lines with 100,000 line
interval. This set was then run in Env2. Every instance was run only once and no precise values
are shown. The results are shown in Figure \ref{fig-dna}. The image description is same
with the images in Table \ref{tbl-graphs}. Since this image has a linear scale, the time complexities
of the implementations can be seen. 

\begin{figure}[h!]
\begin{tikzpicture}
  \begin{axis}[
      width=16cm,
      height=8cm,
		ylabel=Runtime (Sec),
		xlabel=Instance size,
          ]

        \addplot[color=black,mark=x] coordinates {
          (100, 6.22)
          (200, 12.95)
          (300, 19.43)
          (400, 26.54)
          (500, 32.36)
          (600, 38.22)
          (700, 45.01)
          (800, 50.81)
          (900, 57.06)
          (1000, 63.69)
          (1100, 69.82)
          (1200, 76.12)
          (1300, 82.41)
          (1400, 89.03)
          (1500, 95.01)
          (1600, 101.38)
          (1700, 104.32)
          (1800, 110.25)
          (1900, 115.74)
          (2000, 121.28)
	};
        
        \addplot[color=blue,mark=*] coordinates {
          (100, 5.37)
          (200, 15.58)
          (300, 28.28)
          (400, 42.04)
          (500, 57.36)
          (600, 72.97)
          (700, 89.07)
          (800, 105.87)
          (900, 122.77)
          (1000, 141.08)
          (1100, 157.61)
          (1200, 175.02)
          (1300, 193.45)
          (1400, 209.05)
          (1500, 227.13)
          (1600, 245.64)
          (1700, 262.94)
          (1800, 280.43)
          (1900, 295.97)
          (2000, 315.56)
        };
	\end{axis}
  \begin{axis}[
      width=16cm,
      height=8cm,
            xlabel=,
            ylabel=Runtime (\%),
            xtick={5, 50, 95},
            xticklabels={,,},
            xtick style={draw=none},
            ylabel near ticks, yticklabel pos=right
          ]

          \addplot[color=orange, mark=triangle] coordinates {
            (100, 115.83)
            (200, 83.12)
            (300, 68.71)
            (400, 63.13)
            (500, 56.42)
            (600, 52.38)
            (700, 50.53)
            (800, 47.99)
            (900, 46.48)
            (1000, 45.14)
            (1100, 44.30)
            (1200, 43.49)
            (1300, 42.60)
            (1400, 42.59)
            (1500, 41.83)
            (1600, 41.27)
            (1700, 39.67)
            (1800, 39.31)
            (1900, 39.11)
            (2000, 38.43)
          };
        \end{axis}
\end{tikzpicture}
  \caption{Comparison of the runtimes between our and reference implementation with DNA data consisting of keywords of length 100. The x axis specifies the size of the instance in thousands of lines. }
  \label{fig-dna}
\end{figure}

  By the comparison of the runtimes with the PizzaChili and the Dna datasets we can conclude that
  our solution implements the greedy heuristic faster:

  \begin{enumerate}
  \item When the instance size is big enough,
  \item Especially with shorter input keywords and
  \item With smaller alphabet sizes.
  \end{enumerate}

% Tästä tekstistä menee ehkä osa discussioniin ? 
\paragraph{RLZ factorization}
As discussed in Chapter \ref{chp-rlz} the size of the RLZ compressed file is the size of the reference
string used as a dictionary plus the size of the factors referencing to this dictionary. Let us now
discuss the compression of the RLZ algorithm with two different methods for generating the dictionary.
The first method that we use for the dictionay construction is a well known method of
sampling substrings from a
random position of the file that is to be compressed. The samples are then concatenated and the
LZ factorization is run against that dictionary. The second approach has not been studied before. The
idea is to sample a number of substrings, similarly with the first approach but instead of
concatenation, the SCS approximation algorithm is run and the result is used as a reference
dictionary.

The PizzaChili dataset sampled with line lengths from 128 to 512 and cut parameters from 15\% to
75\% were used. The Compression column in Tables \ref{runtimes-15-128}, \ref{runtimes-15-254},
\ref{runtimes-25-254}, \ref{runtimes-50-254}, \ref{runtimes-50-512} and \ref{runtimes-75-512} show the
size of the generated superstring relative to the instance size. This compression value depends
on the size of the instance (The cut parameter). The Rsize value is calculated from the superstring
compression value to get the size of the superstring relative to the original file. In other words,
when the common superstring is used as a dictionary in the RLZ compression the Rsize value multiplied
with the size of the original file, that is to be compressed, is the size of the dictionary. As we
see, the compressions are better with bigger instances. However, increasing the keyword length while
keeping the instance size constant, the compression ratio gets worse. 

We ran the RLZ factorization for every PizzaChili file using the common superstrings from the instances
sampled with various parameters as a dictionary. The upper part of Table \ref{tbl-factors} shows the
number of factors needed to compress the original files using the approximate SCS as a dictionary.
For reference, the lower part of the table shows the number of factors when the dictionary
is sampled by concatenating the samples so that the lengths of the dictionaries are the same. We see
that the SCS method for the dictionary construction results significantly smaller number of factors.
The number of factors with the SCS method ranges from 63.5\% (\texttt{influenza\_25\_254}) to
4.1\% (\texttt{kernel\_75\_512}) compared to the numbers with conventional RLZ compression.

This suggests that the acquired RLZ compression is significantly better when the SCS approach is
used. However, the number of factors is only a part of the compression. The final compression ratio
also depens on the size of the dictionaries, which are same in both cases. Let us assume that each
factor in the compressed file consists of two 4-byte integers. Note that in practice, the factors
could be saved with less space. For example, in many cases the generated superstring is shorter than
24Mi symbols. Therefore 3 byte encodings could be used for factor length and position index.
For simplicity, we now use 4 byte integers.

\begin{center}
  \begin{table}
  \begin{tabular} {|l|r r r r r r|}
    \hline
     \textbf{File} & \textbf{15\_128} & \textbf{15\_254} & \textbf{25\_254}  &\textbf{50\_254} & \textbf{50\_512} & \textbf{75\_512} \\
     \hline

     cere            & 2,730,732  & 2,135,129  & 1,574,261  & 1,233,553  & 816,328  & 686,684 \\
     coreutils       & 3,348,186  & 3,052,924  & 1,677,551  & 641,487    & 511,471  & 289,703 \\
     einstein.de.txt & 300,857    & 168,361    & 163,409    & 154,105    & 83,080   & 85,206  \\
     einstein.en.txt & 1,546,254  & 780,202    & 779,287    & 844,213    & 418,429  & 403,249 \\
     influenza       & 1,443,648  & 1,169,337  & 964,633    & 740,696    & 580,713  & 467,416 \\
     kernel          & 948,673    & 736,718    & 391,851    & 235,094    & 168,852  & 131,755 \\
     para            & 3,829,215  & 3,205,234  & 2,080,952  & 1,220,678  & 961,559  & 700,598 \\

     \hline

     cere            & 10,986,309  & 6,900,933  & 5,264,601  & 3,930,879  & 2,148,565  & 1,904,918  \\
     coreutils       & 6,591,442   & 5,580,636  & 4,525,247  & 3,548,432  & 2,812,876  & 2,387,512  \\
     einstein.de.txt & 1,456,740   & 800,626    & 737,333    & 683,388    & 379,491    & 361,252    \\
     einstein.en.txt & 5,191,299   & 3,443,373  & 2,949,429  & 2,592,922  & 1,557,116  & 1,445,208  \\
     influenza       & 2,349,774   & 1,701,867  & 1,519,536  & 1,309,105  & 942,168    & 844,213    \\
     kernel          & 8,001,635   & 6,478,714  & 5,708,406  & 4,938,226  & 3,530,819  & 3,217,852  \\
     para            & 11,264,145  & 8,445,457  & 6,628,010  & 5,086,768  & 3,403,923  & 2,940,037  \\
     
     \hline

  \end{tabular}
  \caption{The number of factors in relative Lempel-Ziv compression. The upper numbers result when
    the RLZ dictionary is a common superstring of the corresponding file. The common superstring
    is generated from the files randomly sampled with parameters specified in the column title.
    The lower numbers result
    when the dictionary is a string obtained by concatenating random samples together such that both
    dictionaries have the same length.}

  \label{tbl-factors}
  \end{table}
\end{center}

Table \ref{tbl-total-sizes} shows
the final sizes of the RLZ compressions for these two methods. The value in each cell is a sum of
the corresponding dictionary and the number of factors presented in Table \ref{tbl-factors} multiplied
with 8 bytes. The dictionary size can be calculated by multiplying the corresponding Rsize value with
the size of the original file.

For example, Let us consider the RLZ compression of the file \texttt{para} with dictionary
generated with sample
parameters cut $= 0.50$ and line length $= 254$.
By Table \ref{runtimes-50-254} the compression ratio of the
file \texttt{para\_50\_254} is 0.237. That means the size of the superstring compared to the
original file is $50\% \cdot 0.237 = 0.119$, which is the corresponding Rsize value.
Let us also assume that the dictionary is encoded using a naive approach where each symbol is
encoded in one byte.
The absolute
size of the dictionary is therefore 410Mib $\cdot 0.119 = $ 48.79Mib. The absolute size of the
factors is $1,220,678\cdot 8$ bytes $= 9.31$ MiB. The total size of the compressed \texttt{para}
file is therefore $48.79 + 9.31 = 58.10$ mebibytes.

With conventionally generated dictionary of the same size, the number of factors was 5,086,768,
with the size of $5,086,768\cdot 8$ bytes $= 38.8$ mebibytes. The total size of the compressed
files is therefore $37.80 + 48.79 = 87.59$ MiB.

Table \ref{tbl-total-sizes} shows the total compressed sizes for each PizzaChili files with different
cut and line length parameters. Note that the values presented in Tables \ref{runtimes-15-128},
\ref{runtimes-15-254}, \ref{runtimes-25-254}, \ref{runtimes-50-254}, \ref{runtimes-50-512} and
\ref{runtimes-75-512} are rounded. The values presented in Table \ref{tbl-total-sizes} are
calculated with more precision causing a small difference compared to values calculated from the
earlier presented data. For the same reason the 75/512 version of the \texttt{einstein.de.tx}
in the lower section of the table is bolded. The compression with more precise values is better
in this case. Note that in real applications, the dictionaries could be encoded using a
$log_2(\Sigma)$ bit encodings. Also, the factors could be encoded in less space. The values
presented in table \ref{tbl-total-sizes} are examples with sizes of 8 byte factors and a single
byte encoding with the dictionary.


\begin{center}
  \begin{table}
  \begin{tabular} {|l|r r r r r r|}
    \hline
     \textbf{File} & \textbf{15\_128} & \textbf{15\_254} & \textbf{25\_254}  &\textbf{50\_254} & \textbf{50\_512} & \textbf{75\_512} \\
     \hline

     cere            & \textbf{41.86} & 43.31          & 46.78          & 56.26          & 72.01          & 83.45 \\
     coreutils       & 39.16          & 38.80          & 33.66          & \textbf{33.22} & 0.00           & 39.84 \\
     einstein.de.txt & 2.91           & \textbf{2.16}  & 2.23           & 2.31           & 2.35           & 2.50  \\
     einstein.en.txt & 13.59          & 7.75           & 8.28           & 9.95           & \textbf{7.23}  & 7.96  \\
     influenza       & \textbf{18.51} & 19.21          & 22.53          & 30.41          & 38.25          & 47.86 \\
     kernel          & 16.28          & 15.57          & \textbf{14.23} & 14.60          & 16.48          & 17.33 \\
     para            & 52.99          & 51.83          & \textbf{51.54} & 57.93          & 68.02          & 78.22 \\

     \hline

     cere            & 104.84 & 79.67          & \textbf{74.94} & 76.84 & 82.17          & 92.74          \\
     coreutils       & 63.90  & 58.08          & \textbf{55.39} & 55.40 & 0.00           & 55.85          \\
     einstein.de.txt & 11.73  & 6.98           & 6.61           & 6.35  & 4.61           & \textbf{4.61}  \\
     einstein.en.txt & 41.40  & 28.06          & 24.84          & 23.29 & 15.92          & \textbf{15.91} \\
     influenza       & 25.43  & \textbf{23.27} & 26.77          & 34.74 & 41.01          & 50.73          \\
     kernel          & 70.09  & 59.38          & 54.79          & 50.48 & 42.13          & \textbf{40.87} \\
     para            & 109.71 & 91.81          & \textbf{86.23} & 87.43 & 86.65          & 95.31          \\
     
     \hline

  \end{tabular}
  \caption{RLZ Compression sizes in mebibytes of the original PizzaChili files with different dictionaries.
  The best compressions are bolded.}
  \label{tbl-total-sizes}
  \end{table}
\end{center}

Comparing the values in Tables \ref{tbl-factors} and \ref{tbl-total-sizes} we see the significance
of the dictionary size with the compression. The bigger the dictionary is, the less significant the
number of RLZ factors becomes. We also see that the numbers of factors tends to decrease faster
with the conventional dictionary construction when the dictionary size or the line length parameter
is increased. This may be explained with the fact that the probability to find a long substring from
the dictionary is lower than to find a short substring. If the sampled substrings are large enough,
there is no significant benefit from using the SCS approach.

Regardless of the decrease in the number of factors, the resulting compression is not necessarily
better with larger lesser number of factors. This is because the size of the dictionary tends to grow more
rapidly than the number of factors decreases. This is especially true with the SCS dictionary
construction, when no compression with the parameters 75\%, 512, resulted the best compression.
Contrarily, with these parameters the conventional dictionary construction resulted the best
performance with three files.

Investigating Table \ref{tbl-total-sizes} even more, we can observe that for most of the lines
in both sections of the table, the compressions behaves similarly. That is, the compression
increases to some point which after it starts to decrease. In other words, it seems like
there is (a local) minimum for the compressio with different sampling parameters.
It would be very significant if the optimal parameters could be predicted before the full compression.


\chapter{Conclusions}
  

  \begin{enumerate}
  \item \citep{Aho75} describes the Aho-Corasick machine for the first time. It gives the pseudocode to creation and search.
  \item alanko dissertation, no bibtex yet. Discusses some things related to this topic.
  \item \citep{Alanko17} describes approx scs algorithm for compact space.
  \item statistics.pdf desccribes the dataset pizzachili.
  \item \citep{Ukkonen90} is the most important reference in this thesis. Describes the main scs algorithm.
  \item \citep{Tarhio88} Describes the same algorithm as ukkonen 90 but not in linear time.
  \end{enumerate}











%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\cleardoublepage                          %fixes the position of bibliography in bookmarks
\phantomsection
\addcontentsline{toc}{chapter}{\bibname}  % This lines adds the bibliography to the ToC
\printbibliography

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\backmatter

%
% TARVIINKO APPENDICES??
%
%

\begin{appendices}

%\input{instructions_english}
%% \input{instructions_finnish}

%% \appendix{Sample Appendix\label{appendix:model}}
%% usually starts on its own page, with the name and number of the appendix at the top. 
%% The appendices here are just models of the table of contents and the presentation. Each appendix
%% Each appendix is paginated separately.

%% In addition to complementing the main document, each appendix is also its own, independent entity.
%% This means that an appendix cannot be just an image or a piece of programming, but the appendix must explain its contents and meaning.

\end{appendices}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
